import streamlit as st
import os
import time
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import plotly.express as px
import plotly.graph_objects as go
from gensim.models import LdaModel, CoherenceModel
from datetime import datetime
import pickle
from pathlib import Path
from utils.session_state import get_session_state, log_message, update_progress

class LDAAnalyzer:
    """LDAä¸»é¢˜æ¨¡å‹åˆ†æç±»"""
    
    def __init__(self, texts, dictionary, corpus):
        self.texts = texts
        self.dictionary = dictionary
        self.corpus = corpus
        self.model = None
        self.coherence_score = None
        self.perplexity = None
        self.topic_keywords = {}
        self.doc_topic_dist = None
    
    def train_model(self, num_topics=5, iterations=50, passes=10, chunksize=None, 
                    alpha='auto', eta='auto', eval_every=10, random_state=42,
                    minimum_probability=0.01, decay=0.5, offset=1.0, 
                    gamma_threshold=0.001, callbacks=None):
        """
        è®­ç»ƒLDAæ¨¡å‹
        
        å‚æ•°è¯´æ˜ï¼š
        ---------
        num_topics : int
            ä¸»é¢˜æ•°é‡
        iterations : int
            Gibbsé‡‡æ ·è¿­ä»£æ¬¡æ•°
        passes : int
            è¯­æ–™åº“éå†æ¬¡æ•°
        chunksize : int
            åœ¨çº¿å­¦ä¹ æ‰¹é‡å¤§å°
        alpha : str or list
            æ–‡æ¡£-ä¸»é¢˜åˆ†å¸ƒçš„Dirichletå…ˆéªŒ ('auto', 'symmetric', 'asymmetric')
        eta : str or list
            ä¸»é¢˜-è¯è¯­åˆ†å¸ƒçš„Dirichletå…ˆéªŒ ('auto', 'symmetric')
        eval_every : int
            å›°æƒ‘åº¦è¯„ä¼°é—´éš”
        random_state : int
            éšæœºç§å­ï¼Œç¡®ä¿ç»“æœå¯é‡ç°
        minimum_probability : float
            ä¸»é¢˜æ¦‚ç‡é˜ˆå€¼
        decay : float
            å­¦ä¹ ç‡è¡°å‡å‚æ•° (0.5-1.0)
        offset : float
            å­¦ä¹ ç‡åç§»å‚æ•°
        gamma_threshold : float
            Eæ­¥æ”¶æ•›é˜ˆå€¼
        callbacks : dict
            å›è°ƒå‡½æ•°å­—å…¸
        """
        # ç¡®å®šåˆé€‚çš„chunksize
        if chunksize is None:
            chunksize = max(len(self.corpus) // 10, 100)
        
        # è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿ç»“æœå¯é‡ç°
        np.random.seed(random_state)
        
        # è¿›åº¦æ¡å’ŒçŠ¶æ€æ–‡æœ¬
        progress_bar = None
        status_text = None
        if callbacks:
            progress_bar = callbacks.get('progress_bar')
            status_text = callbacks.get('status_text')
        
        # è®­ç»ƒLDAæ¨¡å‹
        self.model = LdaModel(
            corpus=self.corpus,
            id2word=self.dictionary,
            num_topics=num_topics,
            iterations=iterations,
            passes=passes,
            chunksize=chunksize,
            alpha=alpha,
            eta=eta,
            eval_every=eval_every,
            random_state=random_state,
            minimum_probability=minimum_probability,
            decay=decay,
            offset=offset,
            gamma_threshold=gamma_threshold,
            callbacks=None  # ä¸ä½¿ç”¨Gensimå†…éƒ¨å›è°ƒ
        )
        
        # è®¡ç®—å›°æƒ‘åº¦
        self.perplexity = self.model.log_perplexity(self.corpus)
        
        # è®¡ç®—è¿è´¯æ€§åˆ†æ•°
        self.coherence_score = self._calculate_coherence()
        
        # æå–ä¸»é¢˜å…³é”®è¯
        self.topic_keywords = {i: [word for word, prob in self.model.show_topic(i, topn=20)]
                             for i in range(num_topics)}
        
        # è®¡ç®—æ–‡æ¡£-ä¸»é¢˜åˆ†å¸ƒ
        self.doc_topic_dist = self._get_document_topics()
        
        return self.model
    
    def _calculate_coherence(self):
        """
        è®¡ç®—è¿è´¯æ€§åˆ†æ•°
        æ³¨æ„ï¼šä½¿ç”¨u_massæ–¹æ³•è®¡ç®—çš„è¿è´¯æ€§åˆ†æ•°é€šå¸¸ä¸ºè´Ÿå€¼ï¼Œè¶Šæ¥è¿‘0è¡¨ç¤ºä¸»é¢˜ä¸€è‡´æ€§è¶Šå¥½
        """
        try:
            # æå–ä¸»é¢˜å…³é”®è¯
            topics = []
            for i in range(self.model.num_topics):
                top_words = [word for word, _ in self.model.show_topic(i, topn=10)]
                topics.append(top_words)
            
            # ä½¿ç”¨u_massè¿è´¯æ€§æµ‹é‡ï¼ˆæ¯”c_væ›´ç¨³å®šï¼Œä¸éœ€è¦åŸå§‹æ–‡æœ¬ï¼‰
            coherence_model = CoherenceModel(
                topics=topics,
                corpus=self.corpus,
                dictionary=self.dictionary,
                coherence='u_mass'  # ä½¿ç”¨æ›´ç¨³å®šçš„u_massæ–¹æ³•
            )
            return coherence_model.get_coherence()
        except Exception as e:
            log_message(f"è®¡ç®—è¿è´¯æ€§åˆ†æ•°å¤±è´¥: {str(e)}", level="error")
            return None
    
    def _get_document_topics(self):
        """è·å–æ‰€æœ‰æ–‡æ¡£çš„ä¸»é¢˜åˆ†å¸ƒ"""
        doc_topics = []
        for i, doc in enumerate(self.corpus):
            # è·å–æ–‡æ¡£çš„ä¸»é¢˜åˆ†å¸ƒ
            topics = self.model.get_document_topics(doc, minimum_probability=0.0)
            doc_topics.append([prob for _, prob in sorted(topics)])
        
        return np.array(doc_topics)
    
    def find_optimal_topics(self, start=2, end=15, step=1, random_state=42, callbacks=None):
        """
        å¯»æ‰¾æœ€ä¼˜ä¸»é¢˜æ•°é‡
        
        ç®—æ³•è¯´æ˜ï¼š
        ---------
        é€šè¿‡éå†ä¸åŒçš„ä¸»é¢˜æ•°é‡ï¼Œè®­ç»ƒå¤šä¸ªLDAæ¨¡å‹ï¼Œå¹¶è®¡ç®—æ¯ä¸ªæ¨¡å‹çš„è¿è´¯æ€§åˆ†æ•°å’Œå›°æƒ‘åº¦ï¼Œ
        æœ€ç»ˆé€‰æ‹©è¿è´¯æ€§æœ€ä¼˜çš„ä¸»é¢˜æ•°é‡ã€‚
        
        æ³¨æ„äº‹é¡¹ï¼š
        ---------
        1. å¯¹äºu_massä¸€è‡´æ€§æµ‹é‡ï¼Œå€¼é€šå¸¸ä¸ºè´Ÿï¼Œè¶Šæ¥è¿‘0è¶Šå¥½
        2. ä¸ºç¡®ä¿ç»“æœå¯é‡ç°ï¼Œæ¯æ¬¡è®­ç»ƒéƒ½ä½¿ç”¨å›ºå®šçš„éšæœºç§å­
        3. æœç´¢è¿‡ç¨‹ä½¿ç”¨è¾ƒå°‘çš„è¿­ä»£æ¬¡æ•°(iterations=50, passes=5)ä»¥åŠ å¿«é€Ÿåº¦
        
        å‚æ•°ï¼š
        -----
        start : int
            èµ·å§‹ä¸»é¢˜æ•°é‡
        end : int
            ç»“æŸä¸»é¢˜æ•°é‡
        step : int
            æ­¥é•¿
        random_state : int
            éšæœºç§å­ï¼Œç¡®ä¿ç»“æœå¯é‡ç°
        callbacks : dict
            åŒ…å«è¿›åº¦æ¡å’ŒçŠ¶æ€æ–‡æœ¬çš„å›è°ƒå­—å…¸
            
        è¿”å›ï¼š
        -----
        dict : åŒ…å«æœ€ä¼˜ä¸»é¢˜æ•°ã€è¿è´¯æ€§å€¼åˆ—è¡¨ã€å›°æƒ‘åº¦å€¼åˆ—è¡¨ã€ä¸»é¢˜èŒƒå›´å’Œæ¨¡å‹åˆ—è¡¨
        """
        coherence_values = []
        perplexity_values = []
        model_list = []
        topics_range = range(start, end+1, step)
        
        total_iterations = len(topics_range)
        
        # è¿›åº¦æ¡å’ŒçŠ¶æ€æ–‡æœ¬
        progress_bar = None
        status_text = None
        if callbacks:
            progress_bar = callbacks.get('progress_bar')
            status_text = callbacks.get('status_text')
        
        for idx, num_topics in enumerate(topics_range):
            # æ›´æ–°è¿›åº¦
            if progress_bar:
                progress = (idx + 1) / total_iterations
                progress_bar.progress(progress)
            
            # æ›´æ–°çŠ¶æ€
            if status_text:
                status_text.text(f"è®­ç»ƒæ¨¡å‹ {num_topics} ä¸»é¢˜ ({idx+1}/{total_iterations})")
            
            # è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿ç»“æœå¯é‡ç°
            np.random.seed(random_state)
            
            # è®­ç»ƒæ¨¡å‹ï¼ˆæœç´¢æ—¶ä½¿ç”¨è¾ƒå°‘çš„è¿­ä»£ä»¥åŠ å¿«é€Ÿåº¦ï¼‰
            model = LdaModel(
                corpus=self.corpus,
                id2word=self.dictionary,
                num_topics=num_topics,
                iterations=50,  # æœç´¢æ—¶ä½¿ç”¨è¾ƒå°‘çš„è¿­ä»£æ¬¡æ•°
                passes=5,       # æœç´¢æ—¶ä½¿ç”¨è¾ƒå°‘çš„passes
                alpha='auto',
                eta='auto',
                random_state=random_state,
                callbacks=None
            )
            
            # æ·»åŠ åˆ°æ¨¡å‹åˆ—è¡¨
            model_list.append(model)
            
            # è®¡ç®—å›°æƒ‘åº¦
            perplexity = model.log_perplexity(self.corpus)
            perplexity_values.append(perplexity)
            
            # è®¡ç®—è¿è´¯æ€§
            try:
                # æå–ä¸»é¢˜å…³é”®è¯
                topics = []
                for topic_idx in range(num_topics):
                    top_words = [word for word, _ in model.show_topic(topic_idx, topn=10)]
                    topics.append(top_words)
                
                # ä½¿ç”¨u_massè¿è´¯æ€§æµ‹é‡
                coherence_model = CoherenceModel(
                    topics=topics,
                    corpus=self.corpus,
                    dictionary=self.dictionary, 
                    coherence='u_mass'
                )
                coherence = coherence_model.get_coherence()
                coherence_values.append(coherence)
            except Exception as e:
                log_message(f"è®¡ç®—ä¸»é¢˜æ•°é‡={num_topics}çš„è¿è´¯æ€§å¤±è´¥: {str(e)}", level="error")
                coherence_values.append(0)
            
            # è®°å½•æ—¥å¿—
            log_message(f"ä¸»é¢˜æ•°é‡={num_topics}, è¿è´¯æ€§={coherence_values[-1]:.4f}, å›°æƒ‘åº¦={perplexity_values[-1]:.4f}")
        
        # æ‰¾åˆ°æœ€ä¼˜ä¸»é¢˜æ•°é‡ (å¯¹äºu_massï¼Œè¶Šæ¥è¿‘0è¶Šå¥½)
        if all(cv < 0 for cv in coherence_values if cv != 0):
            # æ‰€æœ‰å€¼éƒ½æ˜¯è´Ÿå€¼ï¼Œæ‰¾ç»å¯¹å€¼æœ€å°çš„
            optimal_idx = np.argmin([abs(cv) for cv in coherence_values if cv != 0] or [0])
        else:
            # æœ‰æ­£å€¼æˆ–0ï¼Œæ‰¾æœ€å¤§å€¼
            optimal_idx = np.argmax(coherence_values)
            
        optimal_topics = topics_range[optimal_idx]
        
        return {
            'optimal_topics': optimal_topics,
            'coherence_values': coherence_values,
            'perplexity_values': perplexity_values,
            'topics_range': list(topics_range),
            'model_list': model_list
        }
    
    def save_model(self, filepath):
        """ä¿å­˜æ¨¡å‹åˆ°æ–‡ä»¶"""
        if self.model:
            # ä¿å­˜LDAæ¨¡å‹
            self.model.save(filepath + ".gensim")
            
            # ä¿å­˜åˆ†æå™¨çŠ¶æ€
            analyzer_state = {
                'coherence_score': self.coherence_score,
                'perplexity': self.perplexity,
                'topic_keywords': self.topic_keywords,
                'doc_topic_dist': self.doc_topic_dist.tolist() if self.doc_topic_dist is not None else None
            }
            
            with open(filepath + ".pkl", 'wb') as f:
                pickle.dump(analyzer_state, f)
            
            return True
        return False
    
    @classmethod
    def load_model(cls, filepath, texts, dictionary, corpus):
        """ä»æ–‡ä»¶åŠ è½½æ¨¡å‹"""
        try:
            # åˆ›å»ºåˆ†æå™¨å®ä¾‹
            analyzer = cls(texts, dictionary, corpus)
            
            # åŠ è½½LDAæ¨¡å‹
            analyzer.model = LdaModel.load(filepath + ".gensim")
            
            # åŠ è½½åˆ†æå™¨çŠ¶æ€
            with open(filepath + ".pkl", 'rb') as f:
                analyzer_state = pickle.load(f)
            
            analyzer.coherence_score = analyzer_state.get('coherence_score')
            analyzer.perplexity = analyzer_state.get('perplexity')
            analyzer.topic_keywords = analyzer_state.get('topic_keywords', {})
            
            doc_topic_dist = analyzer_state.get('doc_topic_dist')
            if doc_topic_dist:
                analyzer.doc_topic_dist = np.array(doc_topic_dist)
            
            return analyzer
        except Exception as e:
            log_message(f"åŠ è½½æ¨¡å‹å¤±è´¥: {str(e)}", level="error")
            return None

def plot_coherence_perplexity(results):
    """
    ç»˜åˆ¶è¿è´¯æ€§å’Œå›°æƒ‘åº¦å›¾è¡¨
    æ³¨æ„ï¼š
    - å¯¹äºu_massè¿è´¯æ€§æµ‹é‡ï¼Œå€¼é€šå¸¸ä¸ºè´Ÿï¼Œè¶Šæ¥è¿‘0è¶Šå¥½
    - å›°æƒ‘åº¦(logå€¼)é€šå¸¸ä¸ºè´Ÿï¼Œå€¼è¶Šå¤§(è¶Šæ¥è¿‘0)è¡¨ç¤ºæ¨¡å‹è¶Šå¥½
    """
    # è®¾ç½®ä¸­æ–‡å­—ä½“ - å…¼å®¹ä¸åŒæ“ä½œç³»ç»Ÿ
    import matplotlib.font_manager as fm
    import platform
    
    # å°è¯•æŸ¥æ‰¾å¯ç”¨çš„ä¸­æ–‡å­—ä½“
    chinese_fonts = []
    system = platform.system()
    
    if system == 'Windows':
        chinese_fonts = ['SimHei', 'Microsoft YaHei', 'SimSun', 'KaiTi']
    elif system == 'Darwin':  # macOS
        chinese_fonts = ['PingFang SC', 'Heiti SC', 'STHeiti', 'Arial Unicode MS']
    else:  # Linux
        chinese_fonts = ['WenQuanYi Micro Hei', 'WenQuanYi Zen Hei', 'Noto Sans CJK SC', 
                        'Noto Sans SC', 'Source Han Sans SC', 'Droid Sans Fallback',
                        'AR PL UMing CN', 'AR PL UKai CN']
    
    # æŸ¥æ‰¾ç³»ç»Ÿä¸­å¯ç”¨çš„å­—ä½“
    available_fonts = [f.name for f in fm.fontManager.ttflist]
    font_found = None
    for font in chinese_fonts:
        if font in available_fonts:
            font_found = font
            break
    
    if font_found:
        plt.rcParams['font.sans-serif'] = [font_found] + list(plt.rcParams['font.sans-serif'])
    else:
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä¸­æ–‡å­—ä½“ï¼Œä½¿ç”¨è‹±æ–‡æ ‡ç­¾
        plt.rcParams['font.sans-serif'] = ['DejaVu Sans', 'Arial']
    
    plt.rcParams['axes.unicode_minus'] = False  # æ­£ç¡®æ˜¾ç¤ºè´Ÿå·
    
    fig = plt.figure(figsize=(12, 5))
    
    # åˆ›å»ºä¸¤ä¸ªå­å›¾
    ax1 = fig.add_subplot(121)
    ax2 = fig.add_subplot(122)
    
    # æ ¹æ®æ˜¯å¦æœ‰ä¸­æ–‡å­—ä½“å†³å®šæ ‡ç­¾è¯­è¨€
    if font_found:
        title1 = 'ä¸»é¢˜è¿è´¯æ€§è¯„åˆ† (u_mass)'
        xlabel = 'ä¸»é¢˜æ•°é‡'
        ylabel1 = 'è¿è´¯æ€§åˆ†æ•° (è¶Šæ¥è¿‘0è¶Šå¥½)'
        title2 = 'æ¨¡å‹å›°æƒ‘åº¦ (logå€¼)'
        ylabel2 = 'å›°æƒ‘åº¦ (logå€¼ï¼Œè¶Šæ¥è¿‘0è¶Šå¥½)'
        optimal_label = 'æœ€ä¼˜'
    else:
        title1 = 'Topic Coherence (u_mass)'
        xlabel = 'Number of Topics'
        ylabel1 = 'Coherence Score (closer to 0 is better)'
        title2 = 'Model Perplexity (log)'
        ylabel2 = 'Perplexity (log, closer to 0 is better)'
        optimal_label = 'Optimal'
    
    # ç»˜åˆ¶è¿è´¯æ€§å¾—åˆ†
    ax1.plot(results['topics_range'], results['coherence_values'], marker='o')
    ax1.set_title(title1)
    ax1.set_xlabel(xlabel)
    ax1.set_ylabel(ylabel1)
    ax1.grid(True, alpha=0.3)
    
    # åœ¨æœ€ä¼˜ç‚¹æ·»åŠ æ ‡è®°
    if all(cv < 0 for cv in results['coherence_values'] if cv != 0):
        # æ‰€æœ‰å€¼éƒ½æ˜¯è´Ÿå€¼ï¼Œæ‰¾ç»å¯¹å€¼æœ€å°çš„
        abs_values = [abs(cv) for cv in results['coherence_values']]
        optimal_idx = np.argmin(abs_values)
    else:
        optimal_idx = np.argmax(results['coherence_values'])
        
    optimal_topics = results['topics_range'][optimal_idx]
    optimal_coherence = results['coherence_values'][optimal_idx]
    ax1.scatter(optimal_topics, optimal_coherence, color='red', s=100, zorder=5)
    ax1.annotate(f'{optimal_label}: {optimal_topics}',
                xy=(optimal_topics, optimal_coherence),
                xytext=(optimal_topics+1, optimal_coherence),
                arrowprops=dict(arrowstyle='->'))
    
    # ç»˜åˆ¶å›°æƒ‘åº¦
    ax2.plot(results['topics_range'], results['perplexity_values'], marker='o', color='orange')
    ax2.set_title(title2)
    ax2.set_xlabel(xlabel)
    ax2.set_ylabel(ylabel2)
    ax2.grid(True, alpha=0.3)
    
    # åœ¨å›°æƒ‘åº¦æœ€ä¼˜ç‚¹æ·»åŠ æ ‡è®° (å¯¹äºlogå›°æƒ‘åº¦ï¼Œå€¼è¶Šå¤§è¶Šå¥½ï¼Œå› ä¸ºé€šå¸¸ä¸ºè´Ÿå€¼)
    perp_idx = np.argmax(results['perplexity_values'])
    perp_topics = results['topics_range'][perp_idx]
    perp_value = results['perplexity_values'][perp_idx]
    ax2.scatter(perp_topics, perp_value, color='red', s=100, zorder=5)
    ax2.annotate(f'{optimal_label}: {perp_topics}',
                xy=(perp_topics, perp_value),
                xytext=(perp_topics+1, perp_value),
                arrowprops=dict(arrowstyle='->'))
    
    plt.tight_layout()
    
    return fig

def render_model_trainer():
    """æ¸²æŸ“æ¨¡å‹è®­ç»ƒæ¨¡å—"""
    st.header("LDAä¸»é¢˜æ¨¡å‹è®­ç»ƒ")
    
    # åŠŸèƒ½ä»‹ç»å’Œæ“ä½œæ‰‹å†Œ
    with st.expander("ğŸ“– åŠŸèƒ½ä»‹ç»ä¸æ“ä½œæ‰‹å†Œ", expanded=False):
        st.markdown("""
        ## LDAä¸»é¢˜æ¨¡å‹è®­ç»ƒæ¨¡å—
        
        æœ¬æ¨¡å—ä½¿ç”¨**LDAï¼ˆLatent Dirichlet Allocationï¼Œæ½œåœ¨ç‹„åˆ©å…‹é›·åˆ†é…ï¼‰**ç®—æ³•å¯¹æ–‡æœ¬è¿›è¡Œä¸»é¢˜å»ºæ¨¡ï¼Œ
        æ˜¯æ–‡æœ¬æŒ–æ˜å’Œè‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸæœ€ç»å…¸çš„ä¸»é¢˜æ¨¡å‹ä¹‹ä¸€ã€‚
        
        ---
        
        ### ğŸ“š ç®—æ³•åŸç†
        
        LDAæ˜¯ä¸€ç§**ç”Ÿæˆå¼æ¦‚ç‡æ¨¡å‹**ï¼Œå‡è®¾æ–‡æ¡£ç”±å¤šä¸ªä¸»é¢˜æ··åˆè€Œæˆï¼Œæ¯ä¸ªä¸»é¢˜ç”±å¤šä¸ªè¯è¯­ç»„æˆï¼š
        
        1. **æ–‡æ¡£-ä¸»é¢˜åˆ†å¸ƒ (Î¸)**ï¼šæ¯ä¸ªæ–‡æ¡£åŒ…å«å¤šä¸ªä¸»é¢˜çš„æ¦‚ç‡åˆ†å¸ƒ
        2. **ä¸»é¢˜-è¯è¯­åˆ†å¸ƒ (Ï†)**ï¼šæ¯ä¸ªä¸»é¢˜åŒ…å«å¤šä¸ªè¯è¯­çš„æ¦‚ç‡åˆ†å¸ƒ
        3. **ç”Ÿæˆè¿‡ç¨‹**ï¼š
           - å¯¹æ¯ä¸ªæ–‡æ¡£ï¼Œä»Dirichletåˆ†å¸ƒé‡‡æ ·ä¸»é¢˜åˆ†å¸ƒ Î¸ ~ Dir(Î±)
           - å¯¹æ¯ä¸ªä¸»é¢˜ï¼Œä»Dirichletåˆ†å¸ƒé‡‡æ ·è¯è¯­åˆ†å¸ƒ Ï† ~ Dir(Î²)
           - å¯¹æ–‡æ¡£ä¸­çš„æ¯ä¸ªè¯ï¼Œå…ˆé‡‡æ ·ä¸»é¢˜ z ~ Multinomial(Î¸)ï¼Œå†é‡‡æ ·è¯è¯­ w ~ Multinomial(Ï†_z)
        
        ---
        
        ### ğŸ¯ ä¸»è¦åŠŸèƒ½
        
        | åŠŸèƒ½ | è¯´æ˜ |
        |------|------|
        | **æ¨¡å‹è®­ç»ƒ** | æ ¹æ®è®¾å®šå‚æ•°è®­ç»ƒLDAä¸»é¢˜æ¨¡å‹ |
        | **æœ€ä¼˜ä¸»é¢˜æ•°æœç´¢** | è‡ªåŠ¨éå†ä¸åŒä¸»é¢˜æ•°ï¼Œæ‰¾åˆ°æœ€ä½³å€¼ |
        | **æ¨¡å‹ä¿å­˜/åŠ è½½** | ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œæ”¯æŒåç»­å¤ç”¨ |
        
        ---
        
        ### âš™ï¸ å‚æ•°è¯¦è§£
        
        #### åŸºç¡€å‚æ•°
        
        | å‚æ•° | è¯´æ˜ | å»ºè®®å€¼ | å­¦æœ¯ç ”ç©¶å»ºè®® |
        |------|------|--------|--------------|
        | **ä¸»é¢˜æ•°é‡ (K)** | æ¨¡å‹è¦è¯†åˆ«çš„ä¸»é¢˜ä¸ªæ•° | 3-15 | å…ˆç”¨è‡ªåŠ¨æœç´¢ç¡®å®šï¼Œæˆ–æ ¹æ®é¢†åŸŸçŸ¥è¯†è®¾å®š |
        | **è¿­ä»£æ¬¡æ•° (iterations)** | Gibbsé‡‡æ ·çš„è¿­ä»£è½®æ•° | 50-200 | å­¦æœ¯ç ”ç©¶å»ºè®®â‰¥100ï¼Œç¡®ä¿æ”¶æ•› |
        | **passes** | æ•´ä¸ªè¯­æ–™åº“çš„éå†æ¬¡æ•° | 5-20 | å­¦æœ¯ç ”ç©¶å»ºè®®â‰¥10ï¼Œå°è¯­æ–™åº“å¯å¢åŠ  |
        
        #### é«˜çº§å‚æ•°
        
        | å‚æ•° | è¯´æ˜ | é€‰é¡¹ | å­¦æœ¯ç ”ç©¶å»ºè®® |
        |------|------|------|--------------|
        | **Alpha (Î±)** | æ–‡æ¡£-ä¸»é¢˜åˆ†å¸ƒçš„Dirichletå…ˆéªŒ | auto/symmetric/asymmetric | **auto**ï¼šè‡ªåŠ¨å­¦ä¹ æœ€ä¼˜å€¼ï¼ˆæ¨èï¼‰<br>**symmetric**ï¼šå¯¹ç§°å…ˆéªŒï¼Œæ‰€æœ‰ä¸»é¢˜æƒé‡ç›¸åŒ<br>**asymmetric**ï¼šéå¯¹ç§°å…ˆéªŒï¼Œå…è®¸æŸäº›ä¸»é¢˜æ›´å¸¸è§ |
        | **Eta (Î²)** | ä¸»é¢˜-è¯è¯­åˆ†å¸ƒçš„Dirichletå…ˆéªŒ | auto/symmetric | **auto**ï¼šè‡ªåŠ¨å­¦ä¹ æœ€ä¼˜å€¼ï¼ˆæ¨èï¼‰<br>**symmetric**ï¼šå¯¹ç§°å…ˆéªŒ |
        | **Chunksize** | åœ¨çº¿å­¦ä¹ çš„æ‰¹é‡å¤§å° | 100-5000 | å¤§è¯­æ–™åº“ç”¨2000ï¼Œå°è¯­æ–™åº“å¯ç”¨å…¨éƒ¨æ–‡æ¡£æ•° |
        | **éšæœºç§å­** | ç¡®ä¿ç»“æœå¯é‡ç° | å›ºå®šå€¼ | å­¦æœ¯ç ”ç©¶**å¿…é¡»**å›ºå®šï¼Œæœ¬ç³»ç»Ÿé»˜è®¤42 |
        | **eval_every** | æ¯éš”å¤šå°‘æ¬¡è¿­ä»£è¯„ä¼°å›°æƒ‘åº¦ | 10-50 | è®¾ä¸º10å¯ç›‘æ§æ”¶æ•›ï¼Œè®¾ä¸ºNoneåŠ å¿«è®­ç»ƒ |
        | **minimum_probability** | ä¸»é¢˜æ¦‚ç‡é˜ˆå€¼ | 0.0-0.1 | 0.0ä¿ç•™æ‰€æœ‰ä¸»é¢˜ï¼Œ0.01è¿‡æ»¤å™ªå£° |
        
        ---
        
        ### ğŸ“Š è¯„ä¼°æŒ‡æ ‡
        
        | æŒ‡æ ‡ | è¯´æ˜ | è§£è¯» |
        |------|------|------|
        | **è¿è´¯æ€§ (Coherence)** | è¡¡é‡ä¸»é¢˜å†…è¯è¯­çš„è¯­ä¹‰ä¸€è‡´æ€§ | u_massæ–¹æ³•ï¼šå€¼ä¸ºè´Ÿï¼Œ**è¶Šæ¥è¿‘0è¶Šå¥½** |
        | **å›°æƒ‘åº¦ (Perplexity)** | è¡¡é‡æ¨¡å‹å¯¹æ–°æ–‡æ¡£çš„é¢„æµ‹èƒ½åŠ› | logå€¼ä¸ºè´Ÿï¼Œ**è¶Šæ¥è¿‘0è¶Šå¥½** |
        
        **æ³¨æ„**ï¼šè¿è´¯æ€§å’Œå›°æƒ‘åº¦å¯èƒ½æŒ‡å‘ä¸åŒçš„æœ€ä¼˜ä¸»é¢˜æ•°ï¼Œå»ºè®®ï¼š
        - ä¼˜å…ˆå‚è€ƒ**è¿è´¯æ€§åˆ†æ•°**ï¼ˆæ›´ç¬¦åˆäººç±»å¯¹ä¸»é¢˜çš„ç†è§£ï¼‰
        - ç»“åˆ**é¢†åŸŸçŸ¥è¯†**å’Œ**ä¸»é¢˜å¯è§£é‡Šæ€§**ç»¼åˆåˆ¤æ–­
        
        ---
        
        ### ğŸ“‹ æ“ä½œæµç¨‹
        
        #### æ–¹å¼ä¸€ï¼šè‡ªåŠ¨å¯»æ‰¾æœ€ä¼˜ä¸»é¢˜æ•°ï¼ˆæ¨èæ–°æ‰‹ï¼‰
        
        1. å±•å¼€"å¯»æ‰¾æœ€ä¼˜ä¸»é¢˜æ•°é‡"é¢æ¿
        2. è®¾ç½®æœç´¢èŒƒå›´ï¼ˆå»ºè®®ï¼šèµ·å§‹2ï¼Œç»“æŸ15ï¼Œæ­¥é•¿1ï¼‰
        3. ç‚¹å‡»"å¯»æ‰¾æœ€ä¼˜ä¸»é¢˜æ•°é‡"æŒ‰é’®
        4. ç­‰å¾…æœç´¢å®Œæˆï¼ŒæŸ¥çœ‹è¿è´¯æ€§å’Œå›°æƒ‘åº¦æ›²çº¿
        5. ç‚¹å‡»"ç›´æ¥ä½¿ç”¨æœ€ä¼˜æ¨¡å‹"åº”ç”¨ç»“æœ
        
        #### æ–¹å¼äºŒï¼šæ‰‹åŠ¨è®¾ç½®å‚æ•°è®­ç»ƒ
        
        1. åœ¨"æ¨¡å‹å‚æ•°é…ç½®"ä¸­è®¾ç½®ä¸»é¢˜æ•°é‡ã€è¿­ä»£æ¬¡æ•°ã€passes
        2. å¦‚éœ€è°ƒæ•´é«˜çº§å‚æ•°ï¼Œå‹¾é€‰"æ˜¾ç¤ºé«˜çº§é€‰é¡¹"
        3. ç‚¹å‡»"å¼€å§‹è®­ç»ƒLDAæ¨¡å‹"æŒ‰é’®
        4. ç­‰å¾…è®­ç»ƒå®Œæˆï¼ŒæŸ¥çœ‹ç»“æœ
        
        #### æ–¹å¼ä¸‰ï¼šåŠ è½½å·²æœ‰æ¨¡å‹
        
        1. å±•å¼€"åŠ è½½å·²æœ‰æ¨¡å‹"é¢æ¿
        2. é€‰æ‹©ä¹‹å‰ä¿å­˜çš„æ¨¡å‹æ–‡ä»¶
        3. ç‚¹å‡»"åŠ è½½æ¨¡å‹"æŒ‰é’®
        
        ---
        
        ### ğŸ’¡ å­¦æœ¯ç ”ç©¶å»ºè®®
        
        1. **å¯é‡å¤æ€§**ï¼šæœ¬ç³»ç»Ÿå·²å›ºå®šéšæœºç§å­(42)ï¼Œç¡®ä¿ç›¸åŒæ•°æ®å¾—åˆ°ç›¸åŒç»“æœ
        2. **å‚æ•°æŠ¥å‘Š**ï¼šè®ºæ–‡ä¸­åº”æŠ¥å‘Šä¸»é¢˜æ•°ã€è¿­ä»£æ¬¡æ•°ã€passesã€alphaã€etaç­‰å‚æ•°
        3. **æ¨¡å‹é€‰æ‹©**ï¼šå»ºè®®å°è¯•å¤šä¸ªä¸»é¢˜æ•°ï¼Œç»“åˆè¿è´¯æ€§åˆ†æ•°å’Œä¸»é¢˜å¯è§£é‡Šæ€§é€‰æ‹©
        4. **æ”¶æ•›æ£€éªŒ**ï¼šç¡®ä¿è¿­ä»£æ¬¡æ•°è¶³å¤Ÿï¼Œå›°æƒ‘åº¦è¶‹äºç¨³å®š
        5. **æ•æ„Ÿæ€§åˆ†æ**ï¼šå¯å°è¯•ä¸åŒå‚æ•°ç»„åˆï¼Œæ£€éªŒç»“æœç¨³å¥æ€§
        
        ---
        
        ### âš ï¸ å¸¸è§é—®é¢˜
        
        | é—®é¢˜ | å¯èƒ½åŸå›  | è§£å†³æ–¹æ¡ˆ |
        |------|----------|----------|
        | ä¸»é¢˜è¯é‡å¤åº¦é«˜ | ä¸»é¢˜æ•°è¿‡å¤šæˆ–è¯­æ–™åº“å¤ªå° | å‡å°‘ä¸»é¢˜æ•°æˆ–å¢åŠ è¯­æ–™ |
        | ä¸»é¢˜ä¸å¯è§£é‡Š | é¢„å¤„ç†ä¸å……åˆ†æˆ–å‚æ•°ä¸å½“ | ä¼˜åŒ–åœç”¨è¯ã€è°ƒæ•´å‚æ•° |
        | è®­ç»ƒæ—¶é—´è¿‡é•¿ | è¿­ä»£æ¬¡æ•°è¿‡å¤šæˆ–è¯­æ–™åº“å¤ªå¤§ | å‡å°‘iterations/passesæˆ–å¢åŠ chunksize |
        | ç»“æœä¸ç¨³å®š | éšæœºç§å­æœªå›ºå®š | æœ¬ç³»ç»Ÿå·²å›ºå®šï¼Œå¦‚ä»ä¸ç¨³å®šè¯·æ£€æŸ¥æ•°æ® |
        """)
    
    # æ£€æŸ¥æ˜¯å¦å®Œæˆäº†é¢„å¤„ç†
    if not st.session_state.texts or not st.session_state.dictionary or not st.session_state.corpus:
        st.warning('è¯·å…ˆåœ¨"æ–‡æœ¬é¢„å¤„ç†"é€‰é¡¹å¡ä¸­å®Œæˆæ–‡æœ¬é¢„å¤„ç†')
        return
    
    # æ˜¾ç¤ºè¯­æ–™åº“åŸºæœ¬ä¿¡æ¯
    st.info(f"ğŸ“Š å½“å‰è¯­æ–™åº“ï¼š{len(st.session_state.texts)} ä¸ªæ–‡æ¡£ï¼Œ{len(st.session_state.dictionary)} ä¸ªè¯æ±‡")
    
    # æ¨¡å‹å‚æ•°é…ç½®
    with st.expander("âš™ï¸ æ¨¡å‹å‚æ•°é…ç½®", expanded=True):
        st.markdown("#### åŸºç¡€å‚æ•°")
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.session_state.num_topics = st.slider(
                "ä¸»é¢˜æ•°é‡ (K)", 
                min_value=2, 
                max_value=30, 
                value=st.session_state.num_topics,
                help="LDAæ¨¡å‹çš„ä¸»é¢˜æ•°é‡ã€‚å»ºè®®å…ˆç”¨è‡ªåŠ¨æœç´¢ç¡®å®šæœ€ä¼˜å€¼ï¼Œæˆ–æ ¹æ®é¢†åŸŸçŸ¥è¯†è®¾å®šã€‚",
                key="main_num_topics_slider"
            )
        
        with col2:
            st.session_state.iterations = st.number_input(
                "è¿­ä»£æ¬¡æ•° (iterations)", 
                min_value=10, 
                max_value=500, 
                value=st.session_state.iterations,
                step=10,
                help="Gibbsé‡‡æ ·çš„è¿­ä»£è½®æ•°ã€‚å­¦æœ¯ç ”ç©¶å»ºè®®â‰¥100ä»¥ç¡®ä¿æ”¶æ•›ã€‚",
                key="main_iterations_input"
            )
        
        with col3:
            st.session_state.passes = st.number_input(
                "éå†æ¬¡æ•° (passes)", 
                min_value=1, 
                max_value=50, 
                value=st.session_state.passes,
                step=1,
                help="æ•´ä¸ªè¯­æ–™åº“çš„éå†æ¬¡æ•°ã€‚å­¦æœ¯ç ”ç©¶å»ºè®®â‰¥10ï¼Œå°è¯­æ–™åº“å¯é€‚å½“å¢åŠ ã€‚",
                key="main_passes_input"
            )
        
        # é«˜çº§é€‰é¡¹åˆ‡æ¢
        show_advanced = st.checkbox("ğŸ”§ æ˜¾ç¤ºé«˜çº§é€‰é¡¹ï¼ˆå­¦æœ¯ç ”ç©¶æ¨èï¼‰", value=False, key="show_advanced_options_checkbox")
        
        if show_advanced:
            st.markdown("---")
            st.markdown("#### é«˜çº§å‚æ•°")
            
            # ç¬¬ä¸€è¡Œé«˜çº§å‚æ•°
            adv_col1, adv_col2, adv_col3 = st.columns(3)
            
            with adv_col1:
                alpha = st.selectbox(
                    "Alpha (Î±) å‚æ•°", 
                    ["auto", "symmetric", "asymmetric"],
                    index=0,
                    help="""æ–‡æ¡£-ä¸»é¢˜åˆ†å¸ƒçš„Dirichletå…ˆéªŒå‚æ•°ï¼š
                    - auto: è‡ªåŠ¨å­¦ä¹ æœ€ä¼˜å€¼ï¼ˆæ¨èï¼‰
                    - symmetric: å¯¹ç§°å…ˆéªŒï¼Œæ‰€æœ‰ä¸»é¢˜æƒé‡ç›¸åŒ
                    - asymmetric: éå¯¹ç§°å…ˆéªŒï¼Œå…è®¸æŸäº›ä¸»é¢˜æ›´å¸¸è§""",
                    key="model_alpha_select"
                )
            
            with adv_col2:
                eta = st.selectbox(
                    "Eta (Î²) å‚æ•°", 
                    ["auto", "symmetric"],
                    index=0,
                    help="""ä¸»é¢˜-è¯è¯­åˆ†å¸ƒçš„Dirichletå…ˆéªŒå‚æ•°ï¼š
                    - auto: è‡ªåŠ¨å­¦ä¹ æœ€ä¼˜å€¼ï¼ˆæ¨èï¼‰
                    - symmetric: å¯¹ç§°å…ˆéªŒ""",
                    key="model_eta_select"
                )
            
            with adv_col3:
                # æ ¹æ®è¯­æ–™åº“å¤§å°å»ºè®®chunksize
                corpus_len = len(st.session_state.corpus) if st.session_state.corpus else 10
                default_chunksize = max(1, min(2000, corpus_len))
                chunksize = st.number_input(
                    "æ‰¹é‡å¤§å° (Chunksize)", 
                    min_value=1, 
                    max_value=10000, 
                    value=default_chunksize, 
                    step=10,
                    help="åœ¨çº¿å­¦ä¹ çš„æ‰¹é‡å¤§å°ã€‚å¤§è¯­æ–™åº“ç”¨2000ï¼Œå°è¯­æ–™åº“å¯ç”¨å…¨éƒ¨æ–‡æ¡£æ•°ã€‚",
                    key="model_chunksize_input"
                )
            
            # ç¬¬äºŒè¡Œé«˜çº§å‚æ•°
            adv_col4, adv_col5, adv_col6 = st.columns(3)
            
            with adv_col4:
                eval_every = st.number_input(
                    "è¯„ä¼°é—´éš” (eval_every)",
                    min_value=1,
                    max_value=100,
                    value=10,
                    step=5,
                    help="æ¯éš”å¤šå°‘æ¬¡è¿­ä»£è¯„ä¼°å›°æƒ‘åº¦ã€‚è®¾ä¸º10å¯ç›‘æ§æ”¶æ•›ï¼Œè®¾ä¸ºè¾ƒå¤§å€¼å¯åŠ å¿«è®­ç»ƒã€‚",
                    key="model_eval_every_input"
                )
            
            with adv_col5:
                minimum_probability = st.number_input(
                    "æœ€å°æ¦‚ç‡é˜ˆå€¼",
                    min_value=0.0,
                    max_value=0.1,
                    value=0.01,
                    step=0.01,
                    format="%.2f",
                    help="ä¸»é¢˜æ¦‚ç‡ä½äºæ­¤é˜ˆå€¼çš„å°†è¢«è¿‡æ»¤ã€‚0.0ä¿ç•™æ‰€æœ‰ï¼Œ0.01è¿‡æ»¤å™ªå£°ã€‚",
                    key="model_min_prob_input"
                )
            
            with adv_col6:
                random_state = st.number_input(
                    "éšæœºç§å­ (random_state)",
                    min_value=0,
                    max_value=9999,
                    value=42,
                    step=1,
                    help="å›ºå®šéšæœºç§å­ç¡®ä¿ç»“æœå¯é‡ç°ã€‚å­¦æœ¯ç ”ç©¶å¿…é¡»å›ºå®šæ­¤å€¼ã€‚",
                    key="model_random_state_input"
                )
            
            # ç¬¬ä¸‰è¡Œé«˜çº§å‚æ•°
            adv_col7, adv_col8, adv_col9 = st.columns(3)
            
            with adv_col7:
                decay = st.number_input(
                    "å­¦ä¹ ç‡è¡°å‡ (decay)",
                    min_value=0.5,
                    max_value=1.0,
                    value=0.5,
                    step=0.1,
                    format="%.1f",
                    help="åœ¨çº¿å­¦ä¹ çš„å­¦ä¹ ç‡è¡°å‡å‚æ•°ã€‚æ§åˆ¶æ—§ä¿¡æ¯çš„é—å¿˜é€Ÿåº¦ã€‚",
                    key="model_decay_input"
                )
            
            with adv_col8:
                offset = st.number_input(
                    "å­¦ä¹ ç‡åç§» (offset)",
                    min_value=1.0,
                    max_value=100.0,
                    value=1.0,
                    step=1.0,
                    format="%.1f",
                    help="åœ¨çº¿å­¦ä¹ çš„åç§»å‚æ•°ã€‚è¾ƒå¤§å€¼ä½¿æ—©æœŸè¿­ä»£å­¦ä¹ ç‡æ›´ä½ã€‚",
                    key="model_offset_input"
                )
            
            with adv_col9:
                gamma_threshold = st.number_input(
                    "æ”¶æ•›é˜ˆå€¼ (gamma_threshold)",
                    min_value=0.0001,
                    max_value=0.01,
                    value=0.001,
                    step=0.0001,
                    format="%.4f",
                    help="Eæ­¥æ”¶æ•›é˜ˆå€¼ã€‚è¾ƒå°å€¼æ›´ç²¾ç¡®ä½†æ›´æ…¢ã€‚",
                    key="model_gamma_threshold_input"
                )
            
            # æ˜¾ç¤ºå½“å‰å‚æ•°æ‘˜è¦
            st.markdown("---")
            st.markdown("#### ğŸ“‹ å½“å‰å‚æ•°æ‘˜è¦ï¼ˆå¯ç”¨äºè®ºæ–‡æŠ¥å‘Šï¼‰")
            params_summary = f"""
            ```
            LDAæ¨¡å‹å‚æ•°é…ç½®ï¼š
            - ä¸»é¢˜æ•°é‡ (K): {st.session_state.num_topics}
            - è¿­ä»£æ¬¡æ•° (iterations): {st.session_state.iterations}
            - éå†æ¬¡æ•° (passes): {st.session_state.passes}
            - Alpha: {alpha}
            - Eta: {eta}
            - Chunksize: {chunksize}
            - éšæœºç§å­: {random_state}
            - è¯„ä¼°é—´éš”: {eval_every}
            - æœ€å°æ¦‚ç‡é˜ˆå€¼: {minimum_probability}
            - å­¦ä¹ ç‡è¡°å‡: {decay}
            - å­¦ä¹ ç‡åç§»: {offset}
            - æ”¶æ•›é˜ˆå€¼: {gamma_threshold}
            ```
            """
            st.markdown(params_summary)
        else:
            # ä½¿ç”¨é»˜è®¤é«˜çº§å‚æ•°
            alpha = "auto"
            eta = "auto"
            chunksize = None
            eval_every = 10
            minimum_probability = 0.01
            random_state = 42
            decay = 0.5
            offset = 1.0
            gamma_threshold = 0.001
    
    # è‡ªåŠ¨å¯»æ‰¾æœ€ä¼˜ä¸»é¢˜æ•°é‡
    with st.expander("ğŸ” å¯»æ‰¾æœ€ä¼˜ä¸»é¢˜æ•°é‡", expanded=False):
        st.markdown("""
        ğŸ’¡ **ä½¿ç”¨è¯´æ˜**ï¼šè‡ªåŠ¨éå†ä¸åŒä¸»é¢˜æ•°ï¼Œè®¡ç®—è¿è´¯æ€§åˆ†æ•°ï¼Œæ‰¾åˆ°æœ€ä¼˜å€¼ã€‚
        æœç´¢è¿‡ç¨‹ä½¿ç”¨è¾ƒå°‘çš„è¿­ä»£æ¬¡æ•°ä»¥åŠ å¿«é€Ÿåº¦ï¼Œæ‰¾åˆ°æœ€ä¼˜å€¼åå¯ç›´æ¥ä½¿ç”¨æˆ–é‡æ–°è®­ç»ƒã€‚
        """)
        
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            start_topics = st.number_input("èµ·å§‹ä¸»é¢˜æ•°", min_value=2, max_value=15, value=2, key="start_topics_input")
        
        with col2:
            end_topics = st.number_input("ç»“æŸä¸»é¢˜æ•°", min_value=3, max_value=30, value=15, key="end_topics_input")
        
        with col3:
            step = st.number_input("æ­¥é•¿", min_value=1, max_value=5, value=1, key="topics_step_input")
        
        with col4:
            search_random_state = st.number_input(
                "éšæœºç§å­", 
                min_value=0, 
                max_value=9999, 
                value=42, 
                key="search_random_state_input",
                help="å›ºå®šéšæœºç§å­ç¡®ä¿æœç´¢ç»“æœå¯é‡ç°"
            )
        
        if st.button("ğŸ” å¯»æ‰¾æœ€ä¼˜ä¸»é¢˜æ•°é‡", key="find_optimal_topics"):
            # æ£€æŸ¥å‚æ•°æœ‰æ•ˆæ€§
            if start_topics >= end_topics:
                st.error("èµ·å§‹ä¸»é¢˜æ•°å¿…é¡»å°äºç»“æŸä¸»é¢˜æ•°")
            else:
                with st.spinner("æ­£åœ¨å¯»æ‰¾æœ€ä¼˜ä¸»é¢˜æ•°é‡..."):
                    start_time = time.time()
                    
                    # åˆ›å»ºLDAåˆ†æå™¨
                    analyzer = LDAAnalyzer(
                        st.session_state.texts,
                        st.session_state.dictionary,
                        st.session_state.corpus
                    )
                    
                    # è¿›åº¦æ¡
                    progress_bar = st.progress(0)
                    status_text = st.empty()
                    
                    # å‡†å¤‡å›è°ƒå‚æ•°
                    callbacks = {
                        'progress_bar': progress_bar,
                        'status_text': status_text
                    }
                    
                    # å¯»æ‰¾æœ€ä¼˜ä¸»é¢˜æ•°é‡
                    results = analyzer.find_optimal_topics(
                        start=start_topics,
                        end=end_topics,
                        step=step,
                        random_state=search_random_state,
                        callbacks=callbacks
                    )
                    
                    # ç¡®ä¿è¿›åº¦è¾¾åˆ°100%
                    progress_bar.progress(1.0)
                    status_text.text("æœ€ä¼˜ä¸»é¢˜æ•°é‡æœç´¢å®Œæˆ")
                    
                    # ä¿å­˜æœç´¢ç»“æœåˆ°ä¼šè¯çŠ¶æ€
                    st.session_state.optimal_search_results = results
                    
                    # ç»˜åˆ¶ç»“æœå›¾è¡¨
                    fig = plot_coherence_perplexity(results)
                    st.pyplot(fig)
                    
                    # æ˜¾ç¤ºæœ€ä¼˜ä¸»é¢˜æ•°é‡
                    optimal_topics = results['optimal_topics']
                    st.success(f"å·²æ‰¾åˆ°æœ€ä¼˜ä¸»é¢˜æ•°é‡: {optimal_topics}")
                    
                    # æ›´æ–°ä¼šè¯çŠ¶æ€ä¸­çš„ä¸»é¢˜æ•°é‡
                    st.session_state.num_topics = optimal_topics
                    
                    # è®°å½•æ—¥å¿—
                    elapsed_time = time.time() - start_time
                    log_message(f"æœ€ä¼˜ä¸»é¢˜æ•°é‡æœç´¢å®Œæˆï¼Œæœ€ä¼˜å€¼: {optimal_topics}ï¼Œè€—æ—¶: {elapsed_time:.2f}ç§’", level="success")
        
        # å¦‚æœæœ‰æœç´¢ç»“æœï¼Œæ˜¾ç¤º"ä½¿ç”¨æœ€ä¼˜æ¨¡å‹"æŒ‰é’®
        if st.session_state.get("optimal_search_results"):
            results = st.session_state.optimal_search_results
            optimal_idx = results['topics_range'].index(results['optimal_topics'])
            
            st.info(f"ğŸ’¡ å·²æ‰¾åˆ°æœ€ä¼˜ä¸»é¢˜æ•°: {results['optimal_topics']}ï¼Œè¿è´¯æ€§: {results['coherence_values'][optimal_idx]:.4f}")
            
            if st.button("ğŸš€ ç›´æ¥ä½¿ç”¨æœ€ä¼˜æ¨¡å‹", key="use_optimal_model", type="primary"):
                with st.spinner("æ­£åœ¨åº”ç”¨æœ€ä¼˜æ¨¡å‹..."):
                    # è·å–æœ€ä¼˜æ¨¡å‹
                    optimal_model = results['model_list'][optimal_idx]
                    
                    # åˆ›å»ºåˆ†æå™¨å¹¶è®¾ç½®æ¨¡å‹
                    analyzer = LDAAnalyzer(
                        st.session_state.texts,
                        st.session_state.dictionary,
                        st.session_state.corpus
                    )
                    analyzer.model = optimal_model
                    analyzer.coherence_score = results['coherence_values'][optimal_idx]
                    analyzer.perplexity = results['perplexity_values'][optimal_idx]
                    
                    # æå–ä¸»é¢˜å…³é”®è¯
                    analyzer.topic_keywords = {i: [word for word, prob in optimal_model.show_topic(i, topn=20)]
                                             for i in range(optimal_model.num_topics)}
                    
                    # è®¡ç®—æ–‡æ¡£-ä¸»é¢˜åˆ†å¸ƒ
                    analyzer.doc_topic_dist = analyzer._get_document_topics()
                    
                    # ä¿å­˜åˆ°ä¼šè¯çŠ¶æ€
                    st.session_state.lda_model = optimal_model
                    st.session_state.num_topics = results['optimal_topics']
                    st.session_state.coherence_score = analyzer.coherence_score
                    st.session_state.perplexity = analyzer.perplexity
                    st.session_state.topic_keywords = analyzer.topic_keywords
                    st.session_state.doc_topic_dist = analyzer.doc_topic_dist
                    st.session_state.training_complete = True
                    
                    # ä¿å­˜æ¨¡å‹
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                    model_path = os.path.join("models", f"lda_model_{results['optimal_topics']}topics_{timestamp}")
                    os.makedirs(os.path.dirname(model_path), exist_ok=True)
                    
                    if analyzer.save_model(model_path):
                        st.session_state.model_path = model_path
                        log_message(f"æœ€ä¼˜æ¨¡å‹å·²ä¿å­˜åˆ°: {model_path}", level="success")
                    
                    st.success(f"å·²åº”ç”¨æœ€ä¼˜æ¨¡å‹ï¼ˆ{results['optimal_topics']}ä¸ªä¸»é¢˜ï¼‰")
                    log_message(f"å·²åº”ç”¨æœ€ä¼˜æ¨¡å‹ï¼Œä¸»é¢˜æ•°: {results['optimal_topics']}", level="success")
                    
                    # æ¸…ç†æœç´¢ç»“æœ
                    del st.session_state.optimal_search_results
                    st.rerun()
    
    # è®­ç»ƒæ¨¡å‹æŒ‰é’®
    if st.button("ğŸš€ å¼€å§‹è®­ç»ƒLDAæ¨¡å‹", key="train_lda_model", type="primary"):
        with st.spinner("æ­£åœ¨è®­ç»ƒLDAæ¨¡å‹..."):
            start_time = time.time()
            
            # è¿›åº¦æ¡å’ŒçŠ¶æ€æ–‡æœ¬
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            # åˆ›å»ºLDAåˆ†æå™¨
            analyzer = LDAAnalyzer(
                st.session_state.texts,
                st.session_state.dictionary,
                st.session_state.corpus
            )
            
            # å‡†å¤‡å›è°ƒå‚æ•°
            callbacks = {
                'progress_bar': progress_bar,
                'status_text': status_text
            }
            
            # è®­ç»ƒæ¨¡å‹ï¼ˆä¼ é€’æ‰€æœ‰å‚æ•°ï¼‰
            model = analyzer.train_model(
                num_topics=st.session_state.num_topics,
                iterations=st.session_state.iterations,
                passes=st.session_state.passes,
                chunksize=chunksize if show_advanced else None,
                alpha=alpha,
                eta=eta,
                eval_every=eval_every if show_advanced else 10,
                random_state=random_state if show_advanced else 42,
                minimum_probability=minimum_probability if show_advanced else 0.01,
                decay=decay if show_advanced else 0.5,
                offset=offset if show_advanced else 1.0,
                gamma_threshold=gamma_threshold if show_advanced else 0.001,
                callbacks=callbacks
            )
            
            # æ‰‹åŠ¨æ›´æ–°è¿›åº¦åˆ°100%
            progress_bar.progress(1.0)
            status_text.text("æ¨¡å‹è®­ç»ƒå®Œæˆ")
            
            # ä¿å­˜åˆ°ä¼šè¯çŠ¶æ€
            st.session_state.lda_model = model
            st.session_state.coherence_score = analyzer.coherence_score
            st.session_state.perplexity = analyzer.perplexity
            st.session_state.topic_keywords = analyzer.topic_keywords
            st.session_state.doc_topic_dist = analyzer.doc_topic_dist
            st.session_state.training_complete = True
            st.session_state.training_time = time.time() - start_time
            
            # ä¿å­˜æ¨¡å‹
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            model_path = os.path.join("models", f"lda_model_{st.session_state.num_topics}topics_{timestamp}")
            os.makedirs(os.path.dirname(model_path), exist_ok=True)
            
            if analyzer.save_model(model_path):
                st.session_state.model_path = model_path
                log_message(f"æ¨¡å‹å·²ä¿å­˜åˆ°: {model_path}", level="success")
            
            # æ˜¾ç¤ºæˆåŠŸæ¶ˆæ¯
            elapsed_time = time.time() - start_time
            st.success(f"LDAæ¨¡å‹è®­ç»ƒå®Œæˆï¼Œè€—æ—¶: {elapsed_time:.2f}ç§’")
            log_message(f"LDAæ¨¡å‹è®­ç»ƒå®Œæˆï¼Œä¸»é¢˜æ•°: {st.session_state.num_topics}ï¼Œè€—æ—¶: {elapsed_time:.2f}ç§’", level="success")
    
    # åŠ è½½å·²æœ‰æ¨¡å‹
    with st.expander("ğŸ“‚ åŠ è½½å·²æœ‰æ¨¡å‹", expanded=False):
        st.markdown("ğŸ’¡ åŠ è½½ä¹‹å‰ä¿å­˜çš„æ¨¡å‹ï¼Œå¯ä»¥ç»§ç»­åˆ†ææˆ–å¯¹æ¯”ä¸åŒå‚æ•°çš„ç»“æœã€‚")
        
        # æ£€æŸ¥modelsç›®å½•æ˜¯å¦å­˜åœ¨
        if not os.path.exists("models"):
            os.makedirs("models")
        
        model_files = [f for f in os.listdir("models") if f.endswith(".gensim")]
        
        if model_files:
            selected_model = st.selectbox("é€‰æ‹©æ¨¡å‹æ–‡ä»¶", model_files, key="model_file_select")
            
            if st.button("ğŸ“‚ åŠ è½½æ¨¡å‹", key="load_model"):
                with st.spinner("æ­£åœ¨åŠ è½½æ¨¡å‹..."):
                    model_path = os.path.join("models", selected_model[:-7])  # å»æ‰.gensimåç¼€
                    
                    # åŠ è½½æ¨¡å‹
                    analyzer = LDAAnalyzer.load_model(
                        model_path,
                        st.session_state.texts,
                        st.session_state.dictionary,
                        st.session_state.corpus
                    )
                    
                    if analyzer and analyzer.model:
                        # ä¿å­˜åˆ°ä¼šè¯çŠ¶æ€
                        st.session_state.lda_model = analyzer.model
                        st.session_state.coherence_score = analyzer.coherence_score
                        st.session_state.perplexity = analyzer.perplexity
                        st.session_state.topic_keywords = analyzer.topic_keywords
                        st.session_state.doc_topic_dist = analyzer.doc_topic_dist
                        st.session_state.training_complete = True
                        st.session_state.model_path = model_path
                        
                        st.success(f"âœ… æˆåŠŸåŠ è½½æ¨¡å‹: {selected_model}")
                        log_message(f"å·²åŠ è½½æ¨¡å‹: {selected_model}", level="success")
                    else:
                        st.error("æ¨¡å‹åŠ è½½å¤±è´¥")
        else:
            st.info("ğŸ“­ æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„æ¨¡å‹æ–‡ä»¶ã€‚è®­ç»ƒæ¨¡å‹åä¼šè‡ªåŠ¨ä¿å­˜åˆ° models ç›®å½•ã€‚")
    
    # æ˜¾ç¤ºè®­ç»ƒç»“æœ
    if st.session_state.training_complete and st.session_state.lda_model:
        st.subheader("æ¨¡å‹è®­ç»ƒç»“æœ")
        
        # è·å–æ¨¡å‹å®é™…çš„ä¸»é¢˜æ•°é‡ï¼ˆä»æ¨¡å‹æœ¬èº«è·å–ï¼Œè€Œä¸æ˜¯session_stateï¼‰
        actual_num_topics = st.session_state.lda_model.num_topics
        
        # æ˜¾ç¤ºæ¨¡å‹åŸºæœ¬ä¿¡æ¯
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.metric("ä¸»é¢˜æ•°é‡", actual_num_topics)
        
        with col2:
            coherence = st.session_state.coherence_score
            if coherence is not None:
                st.metric("è¿è´¯æ€§åˆ†æ•°", f"{coherence:.4f}")
            else:
                st.metric("è¿è´¯æ€§åˆ†æ•°", "N/A")
        
        with col3:
            perplexity = st.session_state.perplexity
            if perplexity is not None:
                st.metric("å›°æƒ‘åº¦", f"{perplexity:.4f}")
            else:
                st.metric("å›°æƒ‘åº¦", "N/A")
        
        # æ˜¾ç¤ºä¸»é¢˜å…³é”®è¯
        st.subheader("ä¸»é¢˜å…³é”®è¯")
        
        # åˆ›å»ºé€‰é¡¹å¡æ˜¾ç¤ºæ¯ä¸ªä¸»é¢˜çš„å…³é”®è¯ï¼ˆä½¿ç”¨æ¨¡å‹å®é™…çš„ä¸»é¢˜æ•°ï¼‰
        topic_tabs = st.tabs([f"ä¸»é¢˜ {i+1}" for i in range(actual_num_topics)])
        
        for i, tab in enumerate(topic_tabs):
            with tab:
                # è·å–ä¸»é¢˜çš„å…³é”®è¯
                if i in st.session_state.topic_keywords:
                    keywords = st.session_state.topic_keywords[i]
                    
                    # æ˜¾ç¤ºå…³é”®è¯åˆ—è¡¨
                    st.write(f"**ä¸»é¢˜ {i+1} çš„å‰20ä¸ªå…³é”®è¯**")
                    
                    # åˆ›å»ºå…³é”®è¯è¡¨æ ¼
                    keywords_df = pd.DataFrame({
                        "å…³é”®è¯": keywords,
                        "ç´¢å¼•": range(1, len(keywords) + 1)
                    }).set_index("ç´¢å¼•")
                    
                    st.dataframe(keywords_df, use_container_width=True)
                else:
                    st.write("è¯¥ä¸»é¢˜æ²¡æœ‰å…³é”®è¯æ•°æ®")
        
        # ä¿å­˜ä¸»é¢˜å…³é”®è¯åˆ°CSV
        if st.button("ä¿å­˜ä¸»é¢˜å…³é”®è¯åˆ°CSV", key="save_keywords"):
            # åˆ›å»ºä¸€ä¸ªåŒ…å«æ‰€æœ‰ä¸»é¢˜å…³é”®è¯çš„DataFrame
            all_keywords = {}
            max_keywords = 0
            
            for topic_id, keywords in st.session_state.topic_keywords.items():
                all_keywords[f"ä¸»é¢˜{topic_id+1}"] = keywords
                max_keywords = max(max_keywords, len(keywords))
            
            # ç¡®ä¿æ‰€æœ‰åˆ—çš„é•¿åº¦ç›¸åŒ
            for topic, keywords in all_keywords.items():
                if len(keywords) < max_keywords:
                    all_keywords[topic] = keywords + [""] * (max_keywords - len(keywords))
            
            # åˆ›å»ºDataFrame
            df = pd.DataFrame(all_keywords)
            
            # ä¿å­˜åˆ°CSV
            csv_path = os.path.join("results", f"topic_keywords_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv")
            os.makedirs(os.path.dirname(csv_path), exist_ok=True)
            df.to_csv(csv_path, encoding="utf-8-sig", index=False)
            
            st.success(f"ä¸»é¢˜å…³é”®è¯å·²ä¿å­˜åˆ°: {csv_path}")
            log_message(f"ä¸»é¢˜å…³é”®è¯å·²ä¿å­˜åˆ°: {csv_path}", level="success") 