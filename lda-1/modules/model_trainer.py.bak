import streamlit as st
import os
import time
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import plotly.express as px
import plotly.graph_objects as go
from gensim.models import LdaModel, CoherenceModel
from datetime import datetime
import pickle
from pathlib import Path
import multiprocessing
from multiprocessing import Pool
import platform
from utils.session_state import get_session_state, log_message, update_progress
import json
import jieba

# 在Windows平台上确保多进程正常工作
if platform.system() == 'Windows':
    multiprocessing.freeze_support()

def render_model_trainer():
    """
    在Streamlit应用中渲染LDA模型训练器界面
    
    此函数处理模型训练、评估和可视化的界面逻辑
    """
    st.header("LDA主题模型训练")
    
    # 获取会话状态
    state = get_session_state()
    
    # 创建标签页
    tabs = st.tabs(["训练模型", "评估模型", "可视化结果"])
    
    # 训练模型标签页
    with tabs[0]:
        st.subheader("训练LDA主题模型")
        
        # 检查是否已加载数据
        if not hasattr(state, 'preprocessed_data') or state.preprocessed_data is None:
            st.warning("请先在数据预处理页面加载并处理数据")
            return
        
        # 模型参数设置
        col1, col2 = st.columns(2)
        
        with col1:
            num_topics = st.slider("主题数量", min_value=2, max_value=50, value=10, 
                                 help="较少的主题易于解释，较多的主题可捕获更细微的差异")
            iterations = st.slider("迭代次数", min_value=50, max_value=200, value=100, 
                                 help="值越大模型越稳定，但训练时间更长")
            passes = st.slider("语料库遍历次数", min_value=1, max_value=10, value=5,
                             help="值越大训练越充分但耗时更长")
        
        with col2:
            alpha_options = {"不对称先验 (asymmetric)": "asymmetric", 
                           "对称先验 (symmetric)": "symmetric", 
                           "自动优化 (auto)": "auto"}
            alpha = st.selectbox("主题-文档分布先验(alpha)", 
                               options=list(alpha_options.keys()),
                               format_func=lambda x: x,
                               help="asymmetric通常效果最佳，允许某些主题在语料库中更常见")
            
            eta_options = {"固定值 (0.01)": 0.01, 
                         "自动优化 (auto)": "auto"}
            eta = st.selectbox("词-主题分布先验(eta)",
                             options=list(eta_options.keys()),
                             format_func=lambda x: x,
                             help="较小值产生更具体的主题，较大值产生更多共享词汇的主题")
            
            use_multiprocessing = st.checkbox("使用多进程训练", value=True, 
                                           help="加速训练，但可能增加内存消耗")
        
        # 转换选项值
        alpha = alpha_options[alpha]
        eta = eta_options[eta]
        
        # 训练按钮
        if st.button("训练模型", type="primary"):
            # 创建进度条和状态文本
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            try:
                status_text.text("准备训练数据...")
                
                # 获取处理后的文本数据
                texts = state.preprocessed_data['tokenized_texts']
                
                # 创建LDA分析器
                if not hasattr(state, 'lda_analyzer') or state.lda_analyzer is None:
                    # 创建词典和语料库
                    status_text.text("创建词典和语料库...")
                    
                    from gensim.corpora import Dictionary
                    dictionary = Dictionary(texts)
                    # 过滤极端词汇
                    dictionary.filter_extremes(no_below=5, no_above=0.5)
                    corpus = [dictionary.doc2bow(text) for text in texts]
                    
                    # 创建LDA分析器
                    state.lda_analyzer = LDAAnalyzer(texts, dictionary, corpus)
                
                # 设置回调函数
                callbacks = {
                    'progress_bar': progress_bar,
                    'status_text': status_text
                }
                
                # 训练模型
                status_text.text("训练LDA模型中...")
                state.lda_analyzer.train_model(
                    num_topics=num_topics,
                    iterations=iterations,
                    passes=passes,
                    alpha=alpha,
                    eta=eta,
                    use_multiprocessing=use_multiprocessing,
                    callbacks=callbacks
                )
                
                # 保存训练好的模型
                if not os.path.exists('models'):
                    os.makedirs('models')
                
                model_path = f"models/lda_model_{num_topics}topics_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
                state.lda_analyzer.save_model(model_path)
                
                # 更新会话状态
                state.current_model_path = model_path
                
                # 完成
                progress_bar.progress(1.0)
                status_text.text("模型训练完成！")
                st.success(f"LDA模型已成功训练，主题数量: {num_topics}")
                
                # 显示模型评估指标
                st.subheader("模型评估指标")
                metrics = {
                    "困惑度 (Log Perplexity)": state.lda_analyzer.perplexity,
                    "连贯性分数 (U_Mass)": state.lda_analyzer.coherence_score,
                    "连贯性分数 (C_V)": state.lda_analyzer.coherence_score_cv
                }
                
                # 创建指标表格
                metrics_df = pd.DataFrame({
                    "指标": list(metrics.keys()),
                    "值": list(metrics.values())
                })
                st.table(metrics_df)
                
                # 显示主题关键词
                st.subheader("主题关键词")
                st.write(state.lda_analyzer.get_topic_words(format_as_html=True), unsafe_allow_html=True)
                
            except Exception as e:
                st.error(f"训练模型时出错: {str(e)}")
                log_message(f"训练模型失败: {str(e)}", level="error")
    
    # 评估模型标签页
    with tabs[1]:
        st.subheader("评估最佳主题数量")
        
        if not hasattr(state, 'lda_analyzer') or state.lda_analyzer is None:
            st.warning("请先训练模型")
        else:
            col1, col2 = st.columns(2)
            
            with col1:
                start_topics = st.number_input("起始主题数", min_value=2, max_value=20, value=2)
                end_topics = st.number_input("结束主题数", min_value=5, max_value=50, value=20)
                
            with col2:
                step = st.number_input("步长", min_value=1, max_value=5, value=1)
                fast_mode = st.checkbox("快速评估模式", value=True, 
                                     help="减少迭代次数和passes以快速筛选，适合初步探索")
            
            if st.button("评估最佳主题数", type="primary"):
                # 创建进度条和状态文本
                progress_bar = st.progress(0)
                status_text = st.empty()
                
                try:
                    # 设置回调函数
                    callbacks = {
                        'progress_bar': progress_bar,
                        'status_text': status_text
                    }
                    
                    # 寻找最优主题数
                    status_text.text("评估最佳主题数中...")
                    results = state.lda_analyzer.find_optimal_topics(
                        start=start_topics,
                        end=end_topics,
                        step=step,
                        callbacks=callbacks,
                        fast_mode=fast_mode
                    )
                    
                    # 保存结果
                    state.topic_evaluation_results = results
                    
                    # 完成
                    progress_bar.progress(1.0)
                    status_text.text("评估完成！")
                    
                    # 显示最佳主题数
                    st.subheader("评估结果")
                    best_topics = {
                        "基于U_Mass连贯性": results['optimal_topics_umass'],
                        "基于C_V连贯性": results['optimal_topics_cv'],
                        "基于困惑度": results['optimal_topics_perplexity']
                    }
                    
                    # 创建结果表格
                    best_df = pd.DataFrame({
                        "评估方法": list(best_topics.keys()),
                        "最佳主题数": list(best_topics.values())
                    })
                    st.table(best_df)
                    
                    # 绘制评估指标图表
                    st.subheader("评估指标图表")
                    
                    # 创建数据框
                    plot_data = pd.DataFrame({
                        "主题数": results['topics_range'],
                        "U_Mass连贯性": results['coherence_values'],
                        "困惑度": results['perplexity_values']
                    })
                    
                    # 添加C_V连贯性（可能只有部分值）
                    cv_data = []
                    for i, val in enumerate(results['coherence_values_cv']):
                        if results['cv_computed'][i]:
                            cv_data.append((results['topics_range'][i], val))
                    
                    # 创建图表
                    fig = go.Figure()
                    
                    # 添加U_Mass连贯性曲线
                    fig.add_trace(go.Scatter(
                        x=plot_data["主题数"], 
                        y=plot_data["U_Mass连贯性"],
                        mode='lines+markers',
                        name='U_Mass连贯性',
                        line=dict(color='blue')
                    ))
                    
                    # 添加困惑度曲线（使用次坐标轴）
                    fig.add_trace(go.Scatter(
                        x=plot_data["主题数"], 
                        y=plot_data["困惑度"],
                        mode='lines+markers',
                        name='困惑度',
                        line=dict(color='red'),
                        yaxis='y2'
                    ))
                    
                    # 添加C_V连贯性点（如果有）
                    if cv_data:
                        cv_x, cv_y = zip(*cv_data)
                        fig.add_trace(go.Scatter(
                            x=cv_x, 
                            y=cv_y,
                            mode='markers',
                            marker=dict(size=10),
                            name='C_V连贯性',
                            line=dict(color='green'),
                            yaxis='y3'
                        ))
                    
                    # 更新布局
                    fig.update_layout(
                        title='主题数量评估指标',
                        xaxis=dict(title='主题数量'),
                        yaxis=dict(title='U_Mass连贯性', side='left', showgrid=False),
                        yaxis2=dict(title='困惑度', side='right', overlaying='y', showgrid=False),
                        yaxis3=dict(title='C_V连贯性', side='right', overlaying='y', showgrid=False, position=0.85),
                        legend=dict(x=0.01, y=0.99),
                        height=500
                    )
                    
                    st.plotly_chart(fig, use_container_width=True)
                    
                except Exception as e:
                    st.error(f"评估主题数时出错: {str(e)}")
                    log_message(f"评估主题数失败: {str(e)}", level="error")
    
    # 可视化结果标签页
    with tabs[2]:
        st.subheader("主题模型可视化")
        
        if not hasattr(state, 'lda_analyzer') or state.lda_analyzer is None or state.lda_analyzer.model is None:
            st.warning("请先训练模型")
        else:
            # 创建可视化选项
            viz_options = st.selectbox(
                "选择可视化类型",
                ["主题关键词", "主题分布热图", "主题相似度网络", "文档-主题分布"]
            )
            
            if viz_options == "主题关键词":
                # 显示主题关键词
                num_words = st.slider("每个主题显示词数", min_value=5, max_value=30, value=10)
                st.write(state.lda_analyzer.get_topic_words(num_words=num_words, format_as_html=True), unsafe_allow_html=True)
                
            elif viz_options == "主题分布热图":
                # 主题-词分布热图
                st.write("主题-词分布热图")
                
                # 获取前N个主题的关键词
                num_topics = state.lda_analyzer.model.num_topics
                num_words = st.slider("每个主题显示词数", min_value=5, max_value=20, value=10)
                
                # 创建热图数据
                topics_words = []
                for i in range(num_topics):
                    topics_words.append(state.lda_analyzer.model.show_topic(i, topn=num_words))
                
                # 转换为DataFrame
                heat_data = []
                for topic_idx, topic in enumerate(topics_words):
                    for word, prob in topic:
                        heat_data.append({
                            "主题": f"主题 {topic_idx+1}",
                            "词": word,
                            "概率": prob
                        })
                
                heat_df = pd.DataFrame(heat_data)
                
                # 创建热图
                heat_pivot = heat_df.pivot(index="主题", columns="词", values="概率")
                
                fig = px.imshow(
                    heat_pivot,
                    labels=dict(x="词", y="主题", color="概率"),
                    x=heat_pivot.columns,
                    y=heat_pivot.index,
                    color_continuous_scale="Viridis",
                    aspect="auto",
                    height=600
                )
                
                st.plotly_chart(fig, use_container_width=True)
                
            elif viz_options == "主题相似度网络":
                # 主题相似度网络
                st.write("主题相似度网络")
                
                # 计算主题间相似度
                num_topics = state.lda_analyzer.model.num_topics
                
                # 获取主题向量
                topic_vectors = []
                for i in range(num_topics):
                    # 获取主题-词分布
                    topic_vec = state.lda_analyzer.model.get_topic_terms(i, topn=100)
                    # 转换为向量形式
                    vec = np.zeros(len(state.lda_analyzer.dictionary))
                    for idx, prob in topic_vec:
                        vec[idx] = prob
                    topic_vectors.append(vec)
                
                # 计算相似度矩阵
                similarity_matrix = np.zeros((num_topics, num_topics))
                for i in range(num_topics):
                    for j in range(num_topics):
                        # 计算余弦相似度
                        similarity = np.dot(topic_vectors[i], topic_vectors[j]) / (
                            np.linalg.norm(topic_vectors[i]) * np.linalg.norm(topic_vectors[j]) + 1e-10)
                        similarity_matrix[i, j] = similarity
                
                # 设置相似度阈值
                threshold = st.slider("相似度阈值", min_value=0.0, max_value=1.0, value=0.3, step=0.05,
                                    help="仅显示相似度高于此阈值的连接")
                
                # 创建网络图数据
                import networkx as nx
                G = nx.Graph()
                
                # 添加节点
                for i in range(num_topics):
                    # 获取主题关键词
                    keywords = [word for word, _ in state.lda_analyzer.model.show_topic(i, topn=5)]
                    G.add_node(i, keywords=", ".join(keywords))
                
                # 添加边
                for i in range(num_topics):
                    for j in range(i+1, num_topics):
                        if similarity_matrix[i, j] > threshold:
                            G.add_edge(i, j, weight=similarity_matrix[i, j])
                
                # 创建网络图
                pos = nx.spring_layout(G, seed=42)
                
                # 节点大小基于连接数
                node_size = [300 * (1 + len(list(G.neighbors(n)))) for n in G.nodes()]
                
                # 创建边的权重列表
                edge_weights = [G[u][v]['weight'] for u, v in G.edges()]
                
                # 使用Plotly绘制网络图
                edge_x = []
                edge_y = []
                edge_text = []
                
                for edge in G.edges():
                    x0, y0 = pos[edge[0]]
                    x1, y1 = pos[edge[1]]
                    edge_x.extend([x0, x1, None])
                    edge_y.extend([y0, y1, None])
                    edge_text.append(f"相似度: {G[edge[0]][edge[1]]['weight']:.3f}")
                
                edge_trace = go.Scatter(
                    x=edge_x, y=edge_y,
                    line=dict(width=1, color='#888'),
                    hoverinfo='none',
                    mode='lines')
                
                node_x = []
                node_y = []
                node_text = []
                
                for node in G.nodes():
                    x, y = pos[node]
                    node_x.append(x)
                    node_y.append(y)
                    node_text.append(f"主题 {node+1}: {G.nodes[node]['keywords']}")
                
                node_trace = go.Scatter(
                    x=node_x, y=node_y,
                    mode='markers+text',
                    text=[f"T{i+1}" for i in G.nodes()],
                    textposition="top center",
                    hovertext=node_text,
                    marker=dict(
                        showscale=True,
                        colorscale='YlGnBu',
                        size=node_size,
                        colorbar=dict(
                            thickness=15,
                            title='连接数',
                            xanchor='left',
                            titleside='right'
                        ),
                        color=[len(list(G.neighbors(n))) for n in G.nodes()],
                        line_width=2))
                
                fig = go.Figure(data=[edge_trace, node_trace],
                              layout=go.Layout(
                                  title=f'主题相似度网络 (阈值: {threshold})',
                                  showlegend=False,
                                  hovermode='closest',
                                  margin=dict(b=20, l=5, r=5, t=40),
                                  xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
                                  yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
                                  height=600
                              ))
                
                st.plotly_chart(fig, use_container_width=True)
                
            elif viz_options == "文档-主题分布":
                # 文档-主题分布可视化
                st.write("文档-主题分布")
                
                # 检查是否已计算文档-主题分布
                if state.lda_analyzer.doc_topic_dist is None:
                    st.warning("文档-主题分布未计算")
                else:
                    # 转换为数组
                    doc_topic_array = state.lda_analyzer.doc_topic_dist.toarray()
                    
                    # 选择可视化方式
                    viz_type = st.radio("选择可视化方式", ["热图", "散点图"])
                    
                    if viz_type == "热图":
                        # 限制显示的文档数量
                        max_docs = min(100, doc_topic_array.shape[0])
                        doc_indices = st.slider("选择文档范围", 0, doc_topic_array.shape[0]-max_docs, 0)
                        
                        # 创建热图
                        fig = px.imshow(
                            doc_topic_array[doc_indices:doc_indices+max_docs],
                            labels=dict(x="主题", y="文档", color="概率"),
                            x=[f"主题 {i+1}" for i in range(doc_topic_array.shape[1])],
                            color_continuous_scale="Viridis",
                            aspect="auto",
                            height=600
                        )
                        
                        st.plotly_chart(fig, use_container_width=True)
                        
                    elif viz_type == "散点图":
                        # 使用PCA或t-SNE降维
                        dim_method = st.selectbox("降维方法", ["PCA", "t-SNE"])
                        
                        # 执行降维
                        if dim_method == "PCA":
                            from sklearn.decomposition import PCA
                            reducer = PCA(n_components=2, random_state=42)
                        else:  # t-SNE
                            from sklearn.manifold import TSNE
                            reducer = TSNE(n_components=2, random_state=42)
                        
                        # 降维
                        with st.spinner(f"正在使用{dim_method}进行降维..."):
                            embedding = reducer.fit_transform(doc_topic_array)
                        
                        # 找出每个文档的主要主题
                        main_topics = np.argmax(doc_topic_array, axis=1)
                        
                        # 创建散点图数据
                        scatter_data = pd.DataFrame({
                            "x": embedding[:, 0],
                            "y": embedding[:, 1],
                            "主题": [f"主题 {i+1}" for i in main_topics]
                        })
                        
                        # 绘制散点图
                        fig = px.scatter(
                            scatter_data, 
                            x="x", 
                            y="y", 
                            color="主题",
                            title=f"文档-主题分布 ({dim_method}降维)",
                            height=600
                        )
                        
                        st.plotly_chart(fig, use_container_width=True)
                        
                        # 显示每个主题的文档数量
                        topic_counts = pd.Series(main_topics).value_counts().sort_index()
                        topic_counts.index = [f"主题 {i+1}" for i in topic_counts.index]
                        
                        st.subheader("各主题文档数量")
                        fig = px.bar(
                            x=topic_counts.index,
                            y=topic_counts.values,
                            labels={"x": "主题", "y": "文档数量"}
                        )
                        st.plotly_chart(fig, use_container_width=True)

class LDAAnalyzer:
    """LDA主题模型分析类"""
    
    def __init__(self, texts, dictionary, corpus):
        self.texts = texts
        self.dictionary = dictionary
        self.corpus = corpus
        self.model = None
        self.coherence_score = None
        self.perplexity = None
        self.topic_keywords = {}
        self.doc_topic_dist = None
    
    def _calculate_coherence(self, coherence_measure='u_mass'):
        """
        计算模型的主题连贯性分数
        
        参数:
            coherence_measure: 连贯性计算方法，'u_mass'或'c_v'
                'u_mass': 基于词共现统计，计算快速但值通常为负
                'c_v': 基于外部验证，计算较慢但评估更准确，值通常为正
        
        返回:
            连贯性分数 (float)
        """
        if not self.model:
            return None
        
        # 提取所有主题的关键词
        topics = []
        for topic_id in range(self.model.num_topics):
            topic_keywords = [word for word, _ in self.model.show_topic(topic_id, topn=10)]
            topics.append(topic_keywords)
        
        # 创建连贯性模型
        if coherence_measure == 'c_v':
            # c_v需要原始文本
            coherence_model = CoherenceModel(
                topics=topics,
                texts=self.texts,
                dictionary=self.dictionary, 
                coherence=coherence_measure
            )
        else:
            # u_mass只需要语料库和词典
            coherence_model = CoherenceModel(
                topics=topics,
                corpus=self.corpus,
                dictionary=self.dictionary, 
                coherence=coherence_measure
            )
        
        # 计算连贯性分数
        coherence = coherence_model.get_coherence()
        
        return coherence
    
    def _get_document_topics(self, threshold=0.01):
        """
        获取所有文档的主题分布
        
        参数:
            threshold: 忽略低于此阈值的主题概率，推荐0.01-0.05
                       较小的值保留更多主题信息，较大的值提高主题明确性
        
        返回:
            文档-主题分布矩阵 (sparse) 或 None
        """
        if not self.model or not self.corpus:
            return None
        
        # 创建文档-主题分布稀疏矩阵
        num_docs = len(self.corpus)
        num_topics = self.model.num_topics
        
        # 使用SciPy的稀疏矩阵存储
        from scipy.sparse import lil_matrix
        doc_topic_matrix = lil_matrix((num_docs, num_topics), dtype=np.float32)
        
        # 填充矩阵
        for doc_id, doc in enumerate(self.corpus):
            topic_dist = self.model.get_document_topics(doc, minimum_probability=threshold)
            for topic_id, prob in topic_dist:
                doc_topic_matrix[doc_id, topic_id] = prob
        
        # 转换为CSR格式以提高检索效率
        return doc_topic_matrix.tocsr()
    
    def get_topic_words(self, num_words=10, format_as_html=False):
        """
        获取每个主题的关键词及其权重
        
        参数:
            num_words: 每个主题返回的关键词数量
            format_as_html: 是否以HTML格式返回，适用于在网页中显示
            
        返回:
            主题关键词列表或HTML格式的字符串
        """
        if not self.model:
            return None
        
        topics = []
        for topic_id in range(self.model.num_topics):
            topic_words = self.model.show_topic(topic_id, topn=num_words)
            topics.append(topic_words)
        
        if format_as_html:
            html_output = "<div style='font-family: Arial, sans-serif;'>"
            for i, topic in enumerate(topics):
                html_output += f"<h3>主题 #{i+1}</h3>"
                html_output += "<div style='margin-left: 20px;'>"
                for word, prob in topic:
                    # 根据概率设置字体大小，范围从90%到150%
                    size = 90 + int(60 * prob / topic[0][1])
                    opacity = 0.5 + 0.5 * prob / topic[0][1]
                    html_output += f"<span style='font-size: {size}%; opacity: {opacity:.2f}; margin-right: 10px;'>{word}</span>"
                html_output += "</div>"
            html_output += "</div>"
            return html_output
        
        return topics
        
    def save_model(self, model_path, dictionary_path=None, metadata_path=None, compress=True):
        """
        保存LDA模型及相关数据
        
        参数:
            model_path: 模型保存路径
            dictionary_path: 词典保存路径，若为None则使用model_path的同名词典路径
            metadata_path: 元数据保存路径，若为None则使用model_path的同名元数据路径
            compress: 是否压缩保存，减小文件体积但增加加载时间
            
        返回:
            成功标志 (bool)
        """
        if not self.model:
            log_message("尝试保存未训练的模型", level="error")
            return False
        
        try:
            # 设置默认路径
            if dictionary_path is None:
                dictionary_path = model_path + '.dict'
            if metadata_path is None:
                metadata_path = model_path + '.meta'
            
            # 保存模型
            if compress:
                self.model.save(model_path, separately=['state', 'expElogbeta'], ignore=['dispatcher', 'projection'])
            else:
                self.model.save(model_path)
            
            # 保存词典
            self.dictionary.save(dictionary_path)
            
            # 保存元数据
            metadata = {
                'num_topics': self.model.num_topics,
                'perplexity': float(self.perplexity) if self.perplexity is not None else None,
                'coherence_score': float(self.coherence_score) if self.coherence_score is not None else None,
                'coherence_score_cv': float(self.coherence_score_cv) if self.coherence_score_cv is not None else None,
                'timestamp': datetime.now().isoformat(),
                'model_params': {
                    'iterations': self.model.iterations,
                    'passes': self.model.passes,
                    'alpha': str(self.model.alpha),
                    'eta': str(self.model.eta)
                }
            }
            
            with open(metadata_path, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)
            
            log_message(f"模型已保存: {model_path}", level="info")
            return True
            
        except Exception as e:
            log_message(f"保存模型失败: {str(e)}", level="error")
            return False

    def train_model(self, num_topics=10, iterations=100, passes=5, chunksize=2000, 
                    alpha='asymmetric', eta=0.01, eval_every=10, callbacks=None, use_multiprocessing=False):
        """
        训练LDA模型
        
        参数:
            num_topics: 主题数量 - 推荐10-20(小型语料库)或20-50(大型语料库)
                        较少的主题易于解释，较多的主题可捕获更细微的差异
            iterations: 迭代次数 - 推荐50-100，值越大模型越稳定
                        小型语料库可使用较大值(100)，大型语料库可适当减少(50)
            passes: 遍历语料库的次数 - 推荐5-10，影响模型收敛性
                    值越大训练越充分但耗时更长，建议大语料库设为5，小语料库设为10
            chunksize: 每次更新时处理的文档数量 - 推荐为语料库大小的2%-5%，最小2000
                       较大的chunksize提高处理速度但增加内存消耗
            alpha: 主题-文档分布的先验参数 - 推荐'asymmetric'(不对称先验)
                   'asymmetric': 允许某些主题在语料库中更常见(通常效果最佳)
                   'symmetric': 假设所有主题同等重要(适合主题均衡分布的语料库)
                   'auto': 自动优化(训练过程中动态调整)
                   固定值(如0.01): 较小值(如0.01)产生稀疏主题分布，主题更加明确
            eta: 词-主题分布的先验参数 - 推荐0.01(专业领域文本)或'auto'(通用文本)
                 较小值(如0.001)产生更具体、更少共享词汇的主题
                 较大值产生更多共享词汇的主题
            eval_every: 每多少次迭代评估一次困惑度 - 仅用于监控收敛性，推荐10
                        设为0关闭中间困惑度计算，提高训练速度
            callbacks: 回调函数字典，包含progress_bar和status_text
            use_multiprocessing: 是否使用多进程训练（仅适用于大型语料库）
        """
        # 确定合适的chunksize
        if chunksize is None:
            chunksize = max(len(self.corpus) // 10, 2000)
        
        # 设置随机种子以确保结果可重现
        np.random.seed(42)
        
        # 进度条和状态文本
        progress_bar = None
        status_text = None
        if callbacks:
            progress_bar = callbacks.get('progress_bar')
            status_text = callbacks.get('status_text')
        
        # 训练LDA模型
        if use_multiprocessing and len(self.corpus) > 1000:  # 只对大型语料库使用多进程
            if status_text:
                status_text.text("使用多进程训练LDA模型...")
            
            try:
                # 使用多进程训练
                args = (self.corpus, self.dictionary, num_topics, iterations, passes, alpha, eta)
                self.model = _train_lda_model_mp(args)
                
                if status_text:
                    status_text.text("多进程训练完成")
            except Exception as e:
                log_message(f"多进程训练失败，回退到单进程模式: {str(e)}", level="warning")
                if status_text:
                    status_text.text("回退到单进程训练...")
                
                # 回退到单进程训练
                self.model = LdaModel(
                    corpus=self.corpus,
                    id2word=self.dictionary,
                    num_topics=num_topics,
                    iterations=iterations,
                    passes=passes,
                    chunksize=chunksize,
                    alpha=alpha,
                    eta=eta,
                    eval_every=eval_every,
                    random_state=42,  # 确保结果可重现
                    callbacks=None  # 不使用Gensim内部回调
                )
        else:
            # 使用单进程训练
            self.model = LdaModel(
                corpus=self.corpus,
                id2word=self.dictionary,
                num_topics=num_topics,
                iterations=iterations,
                passes=passes,
                chunksize=chunksize,
                alpha=alpha,
                eta=eta,
                eval_every=eval_every,
                random_state=42,  # 确保结果可重现
                callbacks=None  # 不使用Gensim内部回调
            )
        
        # 计算困惑度
        self.perplexity = self.model.log_perplexity(self.corpus)
        
        # 计算连贯性分数 (u_mass和c_v两种方法)
        self.coherence_score = self._calculate_coherence('u_mass')
        self.coherence_score_cv = self._calculate_coherence('c_v')
        
        # 提取主题关键词
        self.topic_keywords = {i: [word for word, prob in self.model.show_topic(i, topn=20)]
                             for i in range(num_topics)}
        
        # 计算文档-主题分布
        self.doc_topic_dist = self._get_document_topics()
        
        return self.model
    
    def find_optimal_topics(self, start=2, end=20, step=1, callbacks=None, fast_mode=True, cv_timeout=90):
        """
        寻找最优主题数量
        
        参数:
            start: 起始主题数量 - 建议从2-5开始
            end: 结束主题数量 - 建议不超过语料库大小的10%，一般20-30为上限
            step: 步长 - 一般为1，语料库较大时可设为2以减少计算量
            callbacks: 回调函数字典，包含progress_bar和status_text
            fast_mode: 是否使用快速评估模式
                       True: 减少迭代次数和passes以快速筛选，适合初步探索
                       False: 使用较高质量参数进行评估，适合最终确定主题数量
            cv_timeout: C_V连贯性计算的超时时间（秒）- 90-120秒通常足够
                        如有强大的计算资源可增加到180秒以获得更准确的评估
            
        注意：
        - 对于u_mass一致性测量，值通常为负，越接近0越好
        - 对于c_v一致性测量，值通常为正，越大越好
        """
        coherence_values = []  # u_mass方法
        coherence_values_cv = []  # c_v方法
        perplexity_values = []
        model_list = []
        topics_range = list(range(start, end+1, step))  # 转换为列表以确保一致性
        
        # 记录哪些主题数实际计算了C_V值
        cv_computed = [False] * len(topics_range)
        
        total_iterations = len(topics_range)
        
        # 进度条和状态文本
        progress_bar = None
        status_text = None
        if callbacks:
            progress_bar = callbacks.get('progress_bar')
            status_text = callbacks.get('status_text')
        
        # 更新状态
        if status_text:
            status_text.text("准备多进程训练环境...")
        
        # 使用多进程并行训练不同主题数的模型
        try:
            # 确定进程数量（留出1个核心给系统使用）
            num_processes = max(1, multiprocessing.cpu_count() - 1)
            
            if status_text:
                status_text.text(f"使用{num_processes}个进程并行训练模型...")
            
            # 准备参数列表 - 根据fast_mode调整参数
            iterations = 50 if fast_mode else 100
            passes = 3 if fast_mode else 5
            topn = 10  # 统一使用10个关键词评估主题质量
            alpha = 'asymmetric'  # 使用不对称先验，通常效果更好
            eta = 0.01  # 使用固定值，产生更清晰的主题
            
            args_list = [(self.corpus, self.dictionary, num_topics, iterations, passes, alpha, eta) 
                         for num_topics in topics_range]
            
            # 并行训练模型
            with Pool(processes=num_processes) as pool:
                # 使用imap以便可以显示进度
                results = []
                for i, model in enumerate(pool.imap(_train_lda_model_mp, args_list)):
                    results.append(model)
                    # 更新进度
                    if progress_bar:
                        progress = (i + 1) / (total_iterations * 2)  # 训练占总进度的一半
                        progress_bar.progress(progress)
                    if status_text:
                        status_text.text(f"训练模型 {topics_range[i]} 主题 ({i+1}/{total_iterations})")
        except Exception as e:
            log_message(f"寻找最优主题数量失败: {str(e)}", level="error")
        
        # 计算困惑度和一致性分数
        if status_text:
            status_text.text("计算评估指标...")
        
        # 先计算所有模型的u_mass连贯性（较快）和困惑度
        for i, (num_topics, model) in enumerate(zip(topics_range, results)):
            # 更新进度
            if progress_bar:
                progress = 0.5 + (i + 1) / (total_iterations * 2)  # 评估占总进度的另一半
                progress_bar.progress(progress)
            
            # 计算困惑度
            try:
                perplexity = model.log_perplexity(self.corpus)
                perplexity_values.append(perplexity)
            except Exception as e:
                log_message(f"计算主题数量={num_topics}的困惑度失败: {str(e)}", level="error")
                perplexity_values.append(None)
            
            try:
                # 提取主题关键词
                topics = []
                for j in range(num_topics):
                    top_words = [word for word, _ in model.show_topic(j, topn=10)]
                    topics.append(top_words)
                
                # 使用u_mass连贯性测量
                coherence_model = CoherenceModel(
                    topics=topics,
                    corpus=self.corpus,
                    dictionary=self.dictionary, 
                    coherence='u_mass'
                )
                coherence = coherence_model.get_coherence()
                coherence_values.append(coherence)
                
                # 先添加占位符，后面再计算c_v
                coherence_values_cv.append(None)
                cv_computed.append(False)
                
            except Exception as e:
                log_message(f"计算主题数量={num_topics}的u_mass连贯性失败: {str(e)}", level="error")
                coherence_values.append(None)
                coherence_values_cv.append(None)
                cv_computed.append(False)
            
            # 添加到模型列表
            model_list.append(model)
        
        # 根据u_mass连贯性结果，选择部分模型计算c_v连贯性
        # 1. 计算前3个和后3个主题的c_v值
        # 2. 如果主题数量不多，则计算全部
        # 3. 对于中间的主题，选择u_mass表现最好的几个
        
        # 确定要计算c_v的主题索引
        cv_indices = []
        
        # 如果主题总数小于等于7，计算所有的
        if len(topics_range) <= 7:
            cv_indices = list(range(len(topics_range)))
        else:
            # 添加前3个和后3个
            cv_indices.extend(range(min(3, len(topics_range))))
            cv_indices.extend(range(len(topics_range) - 3, len(topics_range)))
            
            # 添加u_mass表现最好的3个（排除已添加的）
            if coherence_values:
                # 创建(索引,值)对，并排序
                indexed_values = [(i, v) for i, v in enumerate(coherence_values) if i not in cv_indices and v is not None]
                
                if all(v < 0 for _, v in indexed_values if v is not None):
                    # 对于负值，按绝对值从小到大排序
                    sorted_indices = sorted(indexed_values, key=lambda x: abs(x[1]) if x[1] is not None else float('inf'))
                else:
                    # 对于正值，按值从大到小排序
                    sorted_indices = sorted(indexed_values, key=lambda x: -x[1] if x[1] is not None else float('-inf'))
                
                # 取前3个或更少
                best_indices = [i for i, _ in sorted_indices[:3]]
                cv_indices.extend(best_indices)
        
        # 去重并排序
        cv_indices = sorted(set(cv_indices))
        
        # 计算选定主题的c_v连贯性
        for idx in cv_indices:
            num_topics = topics_range[idx]
            model = model_list[idx]
            
            if status_text:
                status_text.text(f"计算主题数量={num_topics}的c_v连贯性（耗时较长）...")
            
            try:
                # 提取主题关键词
                topics = []
                for j in range(num_topics):
                    top_words = [word for word, _ in model.show_topic(j, topn=10)]
                    topics.append(top_words)
                
                # 使用c_v连贯性测量，添加超时机制
                import signal
                import threading
                
                # 为Windows平台创建一个基于线程的超时机制
                if platform.system() == 'Windows':
                    cv_result = [None]
                    cv_exception = [None]
                    
                    def calculate_cv():
                        try:
                            coherence_model_cv = CoherenceModel(
                                topics=topics,
                                texts=self.texts,
                                dictionary=self.dictionary, 
                                coherence='c_v'
                            )
                            cv_result[0] = coherence_model_cv.get_coherence()
                        except Exception as e:
                            cv_exception[0] = e
                    
                    # 创建并启动线程
                    cv_thread = threading.Thread(target=calculate_cv)
                    cv_thread.daemon = True
                    cv_thread.start()
                    
                    # 等待线程完成或超时
                    cv_thread.join(timeout=cv_timeout)
                    
                    if cv_thread.is_alive():
                        # 线程仍在运行，表示超时
                        log_message(f"计算主题数量={num_topics}的c_v连贯性超时（>{cv_timeout}秒）", level="warning")
                        # 线程将继续运行，但我们不再等待它的结果
                    elif cv_exception[0]:
                        # 线程遇到异常
                        raise cv_exception[0]
                    else:
                        # 线程成功完成
                        coherence_cv = cv_result[0]
                        coherence_values_cv[idx] = coherence_cv
                        cv_computed[idx] = True
                else:
                    # 非Windows平台使用信号超时机制
                    def timeout_handler(signum, frame):
                        raise TimeoutError(f"C_V连贯性计算超时（>{cv_timeout}秒）")
                    
                    # 设置超时信号
                    signal.signal(signal.SIGALRM, timeout_handler)
                    signal.alarm(cv_timeout)
                    
                    try:
                        coherence_model_cv = CoherenceModel(
                            topics=topics,
                            texts=self.texts,
                            dictionary=self.dictionary, 
                            coherence='c_v'
                        )
                        coherence_cv = coherence_model_cv.get_coherence()
                        coherence_values_cv[idx] = coherence_cv
                        cv_computed[idx] = True
                        
                        # 取消超时信号
                        signal.alarm(0)
                            
                    except TimeoutError as e:
                        log_message(f"计算主题数量={num_topics}的c_v连贯性超时: {str(e)}", level="warning")
                        # 保持为None
                    except Exception as e:
                        log_message(f"计算主题数量={num_topics}的c_v连贯性失败: {str(e)}", level="error")
                        # 保持为None
                    
            except Exception as e:
                log_message(f"准备主题数量={num_topics}的c_v连贯性计算失败: {str(e)}", level="error")
                # 保持为None
            
            # 记录日志
            cv_value = coherence_values_cv[idx]
            cv_status = "计算成功" if cv_computed[idx] else "计算失败"
            log_message(f"主题数量={num_topics}, u_mass连贯性={coherence_values[idx]:.4f if coherence_values[idx] is not None else 'N/A'}, " 
                       f"c_v连贯性={cv_value:.4f if cv_value is not None else 'N/A'} ({cv_status}), "
                       f"困惑度={perplexity_values[idx]:.4f if perplexity_values[idx] is not None else 'N/A'}")
        
        # 计算结果
        # 1. 基于u_mass的最佳主题数（值越接近0越好）
        optimal_idx_umass = None
        if any(v is not None for v in coherence_values):
            if all(v < 0 for v in coherence_values if v is not None):
                # 所有值为负，选择绝对值最小的
                optimal_idx_umass = coherence_values.index(max(filter(lambda x: x is not None, coherence_values)))
            else:
                # 有正值，直接选择最大的
                optimal_idx_umass = coherence_values.index(max(filter(lambda x: x is not None, coherence_values)))
        
        # 2. 基于c_v的最佳主题数（值越大越好）
        optimal_idx_cv = None
        if any(v is not None for v in coherence_values_cv):
            coherence_values_cv_filtered = [v for v in coherence_values_cv if v is not None]
            if coherence_values_cv_filtered:
                max_cv = max(coherence_values_cv_filtered)
                optimal_idx_cv = coherence_values_cv.index(max_cv)
        
        # 3. 基于困惑度的最佳主题数（值越大越好，因为我们计算的是log-perplexity）
        optimal_idx_perplexity = None
        if any(v is not None for v in perplexity_values):
            perplexity_values_filtered = [v for v in perplexity_values if v is not None]
            if perplexity_values_filtered:
                max_perplexity = max(perplexity_values_filtered)
                optimal_idx_perplexity = perplexity_values.index(max_perplexity)
        
        # 结果整合
        results = {
            'topics_range': topics_range,
            'coherence_values': coherence_values,
            'coherence_values_cv': coherence_values_cv,
            'cv_computed': cv_computed,
            'perplexity_values': perplexity_values,
            'models': model_list,
            'optimal_topics_umass': topics_range[optimal_idx_umass] if optimal_idx_umass is not None else None,
            'optimal_topics_cv': topics_range[optimal_idx_cv] if optimal_idx_cv is not None else None,
            'optimal_topics_perplexity': topics_range[optimal_idx_perplexity] if optimal_idx_perplexity is not None else None
        }
        
        # 日志记录最佳结果
        log_message(f"最佳主题数(u_mass): {results['optimal_topics_umass']} - "
                   f"连贯性: {coherence_values[optimal_idx_umass]:.4f if optimal_idx_umass is not None else 'N/A'}")
        
        if optimal_idx_cv is not None:
            log_message(f"最佳主题数(c_v): {results['optimal_topics_cv']} - "
                       f"连贯性: {coherence_values_cv[optimal_idx_cv]:.4f}")
        
        log_message(f"最佳主题数(困惑度): {results['optimal_topics_perplexity']} - "
                   f"困惑度: {perplexity_values[optimal_idx_perplexity]:.4f if optimal_idx_perplexity is not None else 'N/A'}")
        
        return results 
    
    @classmethod
    def load_model(cls, model_path, dictionary_path=None, metadata_path=None):
        """
        加载已保存的LDA模型
        
        参数:
            model_path: 模型文件路径
            dictionary_path: 词典文件路径，若为None则尝试使用model_path同名词典
            metadata_path: 元数据文件路径，若为None则尝试使用model_path同名元数据
            
        返回:
            已加载模型的ModelTrainer实例
        """
        try:
            # 设置默认路径
            if dictionary_path is None:
                dictionary_path = model_path + '.dict'
            if metadata_path is None:
                metadata_path = model_path + '.meta'
            
            # 加载模型
            model = LdaModel.load(model_path)
            
            # 加载词典
            dictionary = Dictionary.load(dictionary_path)
            
            # 创建ModelTrainer实例
            trainer = cls()
            trainer.model = model
            trainer.dictionary = dictionary
            
            # 加载元数据（如果存在）
            try:
                with open(metadata_path, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)
                
                trainer.perplexity = metadata.get('perplexity')
                trainer.coherence_score = metadata.get('coherence_score')
                trainer.coherence_score_cv = metadata.get('coherence_score_cv')
                
                # 提取主题关键词
                trainer.topic_keywords = {i: [word for word, prob in trainer.model.show_topic(i, topn=20)]
                                         for i in range(model.num_topics)}
                
            except Exception as e:
                log_message(f"加载元数据失败: {str(e)}", level="warning")
            
            log_message(f"模型已加载: {model_path}", level="info")
            return trainer
            
        except Exception as e:
            log_message(f"加载模型失败: {str(e)}", level="error")
            return None

    def analyze_document(self, document, preprocessing_func=None, threshold=0.05, keywords_only=False):
        """
        分析单个文档的主题分布
        
        参数:
            document: 要分析的文档文本
            preprocessing_func: 预处理函数，若为None则使用简单分词
            threshold: 低于此概率的主题将被忽略
            keywords_only: 若为True，则只返回关键词，不返回概率
            
        返回:
            (主题分布, 主题关键词)元组，或None（如果模型未训练）
        """
        if not self.model or not self.dictionary:
            log_message("模型未训练，无法分析文档", level="error")
            return None
        
        try:
            # 预处理文档
            if preprocessing_func:
                tokens = preprocessing_func(document)
            else:
                # 简单的分词处理
                tokens = [word for word in jieba.cut(document) 
                         if len(word.strip()) > 1 and not word.isdigit()]
            
            # 转换为词袋表示
            bow = self.dictionary.doc2bow(tokens)
            
            # 获取主题分布
            topic_dist = self.model.get_document_topics(bow, minimum_probability=threshold)
            
            # 获取主题关键词
            topic_keywords = {}
            for topic_id, prob in topic_dist:
                keywords = self.model.show_topic(topic_id, topn=10)
                if keywords_only:
                    topic_keywords[topic_id] = [word for word, _ in keywords]
                else:
                    topic_keywords[topic_id] = keywords
            
            return (topic_dist, topic_keywords)
            
        except Exception as e:
            log_message(f"分析文档失败: {str(e)}", level="error")
            return None
    
    def get_similar_documents(self, query_doc, preprocessing_func=None, topn=10, threshold=0.5):
        """
        找出与查询文档主题相似的文档
        
        参数:
            query_doc: 查询文档文本
            preprocessing_func: 预处理函数，若为None则使用简单分词
            topn: 返回的相似文档数量
            threshold: 相似度阈值，低于此值的文档将被过滤
            
        返回:
            相似文档索引及其相似度分数列表，按相似度降序排列
        """
        if not self.model or not self.corpus or self.doc_topic_dist is None:
            log_message("模型未训练或文档-主题分布未计算，无法查找相似文档", level="error")
            return []
        
        try:
            # 获取查询文档的主题分布
            doc_analysis = self.analyze_document(query_doc, preprocessing_func)
            if not doc_analysis:
                return []
            
            topic_dist, _ = doc_analysis
            
            # 转换为向量
            query_vec = np.zeros(self.model.num_topics)
            for topic_id, prob in topic_dist:
                query_vec[topic_id] = prob
            
            # 计算与所有文档的相似度
            similarities = []
            
            # 将稀疏矩阵转换为正常数组进行处理
            doc_topic_array = self.doc_topic_dist.toarray()
            
            for doc_id, doc_vec in enumerate(doc_topic_array):
                # 计算余弦相似度
                similarity = np.dot(query_vec, doc_vec) / (np.linalg.norm(query_vec) * np.linalg.norm(doc_vec) + 1e-10)
                if similarity >= threshold:
                    similarities.append((doc_id, similarity))
            
            # 按相似度排序
            similarities.sort(key=lambda x: x[1], reverse=True)
            
            # 返回前topn个结果
            return similarities[:topn]
            
        except Exception as e:
            log_message(f"查找相似文档失败: {str(e)}", level="error")
            return []


def _train_lda_model_mp(args):
    """
    多进程训练LDA模型的辅助函数
    
    参数:
        args: (corpus, dictionary, num_topics, iterations, passes, alpha, eta)元组
        
    返回:
        训练好的LDA模型
    """
    corpus, dictionary, num_topics, iterations, passes, alpha, eta = args
    
    model = LdaModel(
        corpus=corpus,
        id2word=dictionary,
        num_topics=num_topics,
        iterations=iterations,
        passes=passes,
        chunksize=2000,
        alpha=alpha,
        eta=eta,
        eval_every=0,  # 禁用中间困惑度计算以加速
        random_state=42  # 确保结果可重现
    )
    
    return model 