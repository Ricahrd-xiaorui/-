# -*- coding: utf-8 -*-
"""
ç»“æœå¯¼å‡ºæ¨¡å— - æ•´åˆæ‰€æœ‰åˆ†æç»“æœçš„å¯¼å‡ºåŠŸèƒ½
"""

import streamlit as st
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import plotly.express as px
import plotly.graph_objects as go
from wordcloud import WordCloud
import base64
from io import BytesIO
import zipfile
import tempfile
from datetime import datetime
from pathlib import Path
import pyLDAvis
import pyLDAvis.gensim_models
from utils.session_state import get_session_state, log_message

# å®‰å…¨å¯¼å…¥æ–°æ¨¡å—
def safe_import(module_name, class_name):
    """å®‰å…¨å¯¼å…¥æ¨¡å—ç±»"""
    try:
        module = __import__(f'modules.{module_name}', fromlist=[class_name])
        return getattr(module, class_name)
    except (ImportError, AttributeError):
        return None

# å¯¼å…¥æ–°æ¨¡å—çš„ç±»
FrequencyAnalyzer = safe_import('frequency_analyzer', 'FrequencyAnalyzer')
CooccurrenceAnalyzer = safe_import('frequency_analyzer', 'CooccurrenceAnalyzer')
TemporalAnalyzer = safe_import('temporal_analyzer', 'TemporalAnalyzer')
CitationAnalyzer = safe_import('citation_analyzer', 'CitationAnalyzer')
SemanticNetworkBuilder = safe_import('semantic_network', 'SemanticNetworkBuilder')
QualitativeCoder = safe_import('qualitative_coding', 'QualitativeCoder')
CodingScheme = safe_import('qualitative_coding', 'CodingScheme')


def generate_report_html(file_names, texts, lda_model, topic_keywords, doc_topic_dist, 
                         coherence_score, perplexity, pyldavis_html=None):
    """ç”ŸæˆHTMLåˆ†ææŠ¥å‘Š"""
    coherence_value = f"{coherence_score:.4f}" if coherence_score is not None else "N/A"
    perplexity_value = f"{perplexity:.4f}" if perplexity is not None else "N/A"
    
    html = f"""
    <!DOCTYPE html>
    <html>
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>LDAä¸»é¢˜æ¨¡å‹åˆ†ææŠ¥å‘Š</title>
        <style>
            body {{ font-family: Arial, sans-serif; line-height: 1.6; margin: 0; padding: 20px; color: #333; max-width: 1200px; margin: 0 auto; }}
            h1, h2, h3 {{ color: #2c3e50; }}
            table {{ border-collapse: collapse; width: 100%; margin-bottom: 20px; }}
            th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}
            th {{ background-color: #f2f2f2; }}
            tr:nth-child(even) {{ background-color: #f9f9f9; }}
            .section {{ margin-bottom: 30px; border-bottom: 1px solid #eee; padding-bottom: 20px; }}
            .topic-keywords {{ background-color: #f5f5f5; padding: 15px; border-radius: 5px; margin-bottom: 10px; }}
            .keyword {{ display: inline-block; background-color: #e0f7fa; padding: 3px 8px; margin: 3px; border-radius: 3px; }}
            .metric {{ font-size: 18px; font-weight: bold; color: #0288d1; }}
            .pyldavis-container {{ width: 100%; height: 800px; border: none; }}
        </style>
    </head>
    <body>
        <h1>LDAä¸»é¢˜æ¨¡å‹åˆ†ææŠ¥å‘Š</h1>
        <p>ç”Ÿæˆæ—¶é—´: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}</p>
        
        <div class="section">
            <h2>1. åˆ†ææ¦‚è¿°</h2>
            <ul>
                <li>æ–‡æ¡£æ•°é‡: {len(texts)}</li>
                <li>ä¸»é¢˜æ•°é‡: {lda_model.num_topics}</li>
                <li>è¿è´¯æ€§åˆ†æ•°: <span class="metric">{coherence_value}</span></li>
                <li>å›°æƒ‘åº¦: <span class="metric">{perplexity_value}</span></li>
            </ul>
        </div>
        
        <div class="section">
            <h2>2. ä¸»é¢˜å…³é”®è¯</h2>
    """
    
    for topic_id, keywords in topic_keywords.items():
        html += f'<div class="topic-keywords"><h3>ä¸»é¢˜ {topic_id + 1}</h3><p>'
        for word in keywords[:20]:
            html += f'<span class="keyword">{word}</span> '
        html += '</p></div>'
    
    html += '</div><div class="section"><h2>3. æ–‡æ¡£-ä¸»é¢˜åˆ†å¸ƒ</h2><table><tr><th>æ–‡æ¡£</th>'
    
    for i in range(lda_model.num_topics):
        html += f"<th>ä¸»é¢˜ {i+1}</th>"
    html += "</tr>"
    
    if len(doc_topic_dist) > 0:
        for i, file_name in enumerate(file_names[:len(doc_topic_dist)]):
            html += f"<tr><td>{file_name}</td>"
            for j in range(doc_topic_dist.shape[1]):
                html += f"<td>{doc_topic_dist[i, j]:.4f}</td>"
            html += "</tr>"
    
    html += "</table></div>"
    
    if pyldavis_html:
        html += f"""
        <div class="section">
            <h2>4. äº¤äº’å¼ä¸»é¢˜å¯è§†åŒ–</h2>
            <iframe class="pyldavis-container" srcdoc='{pyldavis_html.replace("'", "\\'")}'></iframe>
        </div>
        """
    
    html += """
        <footer><p>ç”Ÿæˆè‡ª: æ”¿ç­–æ–‡ä»¶LDAä¸»é¢˜æ¨¡å‹å¯è§†åŒ–åˆ†æç³»ç»Ÿ v2.0</p></footer>
    </body></html>
    """
    
    return html


def dataframe_to_csv(df):
    """å°†DataFrameè½¬æ¢ä¸ºCSVå­—ç¬¦ä¸²"""
    return df.to_csv(index=False).encode('utf-8-sig')


def render_exporter():
    """æ¸²æŸ“ç»“æœå¯¼å‡ºæ¨¡å—"""
    st.header("ğŸ’¾ ç»“æœå¯¼å‡º")
    
    with st.expander("ğŸ“– åŠŸèƒ½ä»‹ç»", expanded=False):
        st.markdown("""
        ### å¯¼å‡ºå†…å®¹
        
        | ç±»åˆ« | å¯¼å‡ºå†…å®¹ | æ–‡ä»¶æ ¼å¼ |
        |------|----------|----------|
        | ä¸»é¢˜åˆ†æ | åˆ†ææŠ¥å‘Šã€ä¸»é¢˜å…³é”®è¯ã€æ–‡æ¡£-ä¸»é¢˜åˆ†å¸ƒ | HTML/CSV |
        | åŸºç¡€åˆ†æ | æ–‡æœ¬ç»Ÿè®¡ã€è¯é¢‘è¡¨ã€å…±ç°çŸ©é˜µ | CSV |
        | é«˜çº§åˆ†æ | èšç±»ã€æ—¶åºã€æ¯”è¾ƒã€å¼•ç”¨ã€è¯­ä¹‰ç½‘ç»œã€ç¼–ç  | CSV |
        | æ¨¡å‹æ–‡ä»¶ | LDAæ¨¡å‹ | ZIP |
        
        **ä½¿ç”¨è¯´æ˜**ï¼šCSVæ–‡ä»¶ä½¿ç”¨UTF-8-BOMç¼–ç ï¼Œå…¼å®¹Excel
        """)
    
    # åˆ›å»ºé€‰é¡¹å¡
    export_tabs = st.tabs([
        "ğŸ“Š ä¸»é¢˜åˆ†æ", 
        "ğŸ“ˆ åŸºç¡€åˆ†æ", 
        "ğŸ”¬ é«˜çº§åˆ†æ",
        "ğŸ’¾ æ¨¡å‹å¯¼å‡º"
    ])
    
    with export_tabs[0]:
        render_topic_export()
    
    with export_tabs[1]:
        render_basic_analysis_export()
    
    with export_tabs[2]:
        render_advanced_analysis_export()
    
    with export_tabs[3]:
        render_model_export()


def render_topic_export():
    """æ¸²æŸ“ä¸»é¢˜åˆ†æå¯¼å‡º"""
    st.subheader("ğŸ“Š ä¸»é¢˜åˆ†æç»“æœå¯¼å‡º")
    
    if not st.session_state.get("training_complete") or not st.session_state.get("lda_model"):
        st.warning('è¯·å…ˆåœ¨"ä¸»é¢˜å»ºæ¨¡"æ ‡ç­¾é¡µä¸­å®ŒæˆLDAæ¨¡å‹è®­ç»ƒ')
        return
    
    topic_tabs = st.tabs(["åˆ†ææŠ¥å‘Š", "æ•°æ®è¡¨æ ¼"])
    
    with topic_tabs[0]:
        st.markdown("#### å¯¼å‡ºå®Œæ•´åˆ†ææŠ¥å‘Š")
        
        report_format = st.radio("æŠ¥å‘Šæ ¼å¼", ["HTML", "PDF"], horizontal=True, key="report_format_radio")
        
        st.write("åŒ…å«å†…å®¹:")
        include_topics = st.checkbox("ä¸»é¢˜å…³é”®è¯", value=True, key="include_topics_checkbox")
        include_doc_dist = st.checkbox("æ–‡æ¡£-ä¸»é¢˜åˆ†å¸ƒ", value=True, key="include_doc_dist_checkbox")
        include_pyldavis = st.checkbox("PyLDAviså¯è§†åŒ–", value=True, key="include_pyldavis_checkbox")
        
        if st.button("ç”Ÿæˆåˆ†ææŠ¥å‘Š", key="generate_report"):
            with st.spinner("æ­£åœ¨ç”ŸæˆæŠ¥å‘Š..."):
                try:
                    pyldavis_html = st.session_state.get("pyldavis_html") if include_pyldavis else None
                    
                    html_report = generate_report_html(
                        st.session_state.file_names,
                        st.session_state.texts,
                        st.session_state.lda_model,
                        st.session_state.topic_keywords if include_topics else {},
                        st.session_state.doc_topic_dist if include_doc_dist else np.array([]),
                        st.session_state.get("coherence_score"),
                        st.session_state.get("perplexity"),
                        pyldavis_html
                    )
                    
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                    
                    st.download_button(
                        label="ğŸ“¥ ä¸‹è½½HTMLæŠ¥å‘Š",
                        data=html_report.encode('utf-8'),
                        file_name=f"lda_report_{timestamp}.html",
                        mime="text/html"
                    )
                    
                    if report_format == "PDF":
                        st.info("æç¤º: å¯ç”¨æµè§ˆå™¨æ‰“å°åŠŸèƒ½å°†HTMLè½¬æ¢ä¸ºPDF")
                
                except Exception as e:
                    st.error(f"ç”ŸæˆæŠ¥å‘Šæ—¶å‡ºé”™: {str(e)}")
    
    with topic_tabs[1]:
        st.markdown("#### å¯¼å‡ºæ•°æ®è¡¨æ ¼")
        
        data_type = st.selectbox("é€‰æ‹©æ•°æ®", ["ä¸»é¢˜å…³é”®è¯", "æ–‡æ¡£-ä¸»é¢˜åˆ†å¸ƒ", "ä¸»é¢˜ç›¸ä¼¼åº¦çŸ©é˜µ"], key="export_topic_data_type")
        
        if data_type == "ä¸»é¢˜å…³é”®è¯" and st.session_state.get("topic_keywords"):
            all_keywords = {}
            max_keywords = 0
            for topic_id, keywords in st.session_state.topic_keywords.items():
                all_keywords[f"ä¸»é¢˜{topic_id+1}"] = keywords
                max_keywords = max(max_keywords, len(keywords))
            
            for topic, keywords in all_keywords.items():
                if len(keywords) < max_keywords:
                    all_keywords[topic] = keywords + [""] * (max_keywords - len(keywords))
            
            df = pd.DataFrame(all_keywords)
            st.dataframe(df.head(10), use_container_width=True)
            
            st.download_button("ğŸ“¥ ä¸‹è½½CSV", dataframe_to_csv(df), 
                             f"topic_keywords_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv", "text/csv")
        
        elif data_type == "æ–‡æ¡£-ä¸»é¢˜åˆ†å¸ƒ" and st.session_state.get("doc_topic_dist") is not None:
            topics = [f"ä¸»é¢˜{i+1}" for i in range(st.session_state.doc_topic_dist.shape[1])]
            df = pd.DataFrame(st.session_state.doc_topic_dist, columns=topics)
            df.insert(0, 'æ–‡æ¡£', st.session_state.file_names[:len(df)])
            st.dataframe(df.head(10), use_container_width=True)
            
            st.download_button("ğŸ“¥ ä¸‹è½½CSV", dataframe_to_csv(df),
                             f"doc_topic_dist_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv", "text/csv")
        
        elif data_type == "ä¸»é¢˜ç›¸ä¼¼åº¦çŸ©é˜µ":
            try:
                num_topics = st.session_state.lda_model.num_topics
                topic_vectors = []
                for i in range(num_topics):
                    topic_vector = [0] * len(st.session_state.dictionary)
                    for word_id, weight in st.session_state.lda_model.get_topic_terms(i, topn=len(st.session_state.dictionary)):
                        topic_vector[word_id] = weight
                    topic_vectors.append(topic_vector)
                
                topic_vectors = np.array(topic_vectors)
                similarity_matrix = np.zeros((num_topics, num_topics))
                
                for i in range(num_topics):
                    for j in range(num_topics):
                        if i == j:
                            similarity_matrix[i, j] = 1.0
                        else:
                            dot_product = np.dot(topic_vectors[i], topic_vectors[j])
                            norm_i = np.linalg.norm(topic_vectors[i])
                            norm_j = np.linalg.norm(topic_vectors[j])
                            if norm_i > 0 and norm_j > 0:
                                similarity_matrix[i, j] = dot_product / (norm_i * norm_j)
                
                topics = [f"ä¸»é¢˜{i+1}" for i in range(num_topics)]
                df = pd.DataFrame(similarity_matrix, index=topics, columns=topics)
                st.dataframe(df, use_container_width=True)
                
                st.download_button("ğŸ“¥ ä¸‹è½½CSV", dataframe_to_csv(df.reset_index().rename(columns={"index": "ä¸»é¢˜"})),
                                 f"topic_similarity_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv", "text/csv")
            except Exception as e:
                st.error(f"è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µæ—¶å‡ºé”™: {str(e)}")
        else:
            st.warning("æ²¡æœ‰å¯ç”¨æ•°æ®")


def render_basic_analysis_export():
    """æ¸²æŸ“åŸºç¡€åˆ†æå¯¼å‡º"""
    st.subheader("ğŸ“ˆ åŸºç¡€åˆ†æç»“æœå¯¼å‡º")
    
    if not st.session_state.get("texts"):
        st.warning('è¯·å…ˆåœ¨"æ–‡æœ¬é¢„å¤„ç†"æ ‡ç­¾é¡µä¸­å®Œæˆæ–‡æœ¬é¢„å¤„ç†')
        return
    
    data_type = st.selectbox("é€‰æ‹©æ•°æ®", ["æ–‡æœ¬ç»Ÿè®¡", "è¯é¢‘è¡¨", "å…±ç°çŸ©é˜µ"], key="export_basic_data_type")
    
    if data_type == "æ–‡æœ¬ç»Ÿè®¡":
        try:
            from modules.text_statistics import create_multi_doc_statistics
            
            raw_texts = st.session_state.get("raw_texts", [])
            texts = st.session_state.get("texts", [])
            file_names = st.session_state.get("file_names", [])
            
            if raw_texts and texts:
                all_stats = create_multi_doc_statistics(raw_texts, texts, file_names)
                csv_content = all_stats.export_comparison()
                
                df = pd.read_csv(BytesIO(csv_content.encode('utf-8-sig')))
                st.dataframe(df, use_container_width=True)
                
                st.download_button("ğŸ“¥ ä¸‹è½½æ–‡æœ¬ç»Ÿè®¡CSV", csv_content,
                                 f"text_statistics_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv", "text/csv")
            else:
                st.warning("æ²¡æœ‰å¯ç”¨çš„æ–‡æœ¬æ•°æ®")
        except Exception as e:
            st.error(f"å¯¼å‡ºæ–‡æœ¬ç»Ÿè®¡æ—¶å‡ºé”™: {str(e)}")
    
    elif data_type == "è¯é¢‘è¡¨":
        try:
            texts = st.session_state.get("texts", [])
            pos_tags = st.session_state.get("pos_tags", [])
            
            if texts and FrequencyAnalyzer:
                analyzer = FrequencyAnalyzer(texts, pos_tags if pos_tags else None)
                csv_content = analyzer.export_frequency_csv(include_pos=bool(pos_tags))
                
                df = pd.read_csv(BytesIO(csv_content.encode('utf-8-sig')))
                st.dataframe(df.head(20), use_container_width=True)
                
                st.download_button("ğŸ“¥ ä¸‹è½½è¯é¢‘è¡¨CSV", csv_content,
                                 f"word_frequency_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv", "text/csv")
            else:
                st.warning("æ²¡æœ‰å¯ç”¨çš„åˆ†è¯æ•°æ®")
        except Exception as e:
            st.error(f"å¯¼å‡ºè¯é¢‘è¡¨æ—¶å‡ºé”™: {str(e)}")
    
    elif data_type == "å…±ç°çŸ©é˜µ":
        try:
            texts = st.session_state.get("texts", [])
            
            if texts and CooccurrenceAnalyzer:
                window_size = st.slider("å…±ç°çª—å£å¤§å°", 2, 10, 5, key="export_cooc_window")
                min_freq = st.slider("æœ€å°å…±ç°é¢‘ç‡", 1, 10, 2, key="export_cooc_min_freq")
                
                analyzer = CooccurrenceAnalyzer(texts, window_size)
                analyzer.calculate_cooccurrence()
                
                col1, col2 = st.columns(2)
                
                with col1:
                    csv_content = analyzer.export_matrix_csv(min_freq)
                    st.download_button("ğŸ“¥ ä¸‹è½½å…±ç°åˆ—è¡¨CSV", csv_content,
                                     f"cooccurrence_list_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv", "text/csv")
                
                with col2:
                    adj_csv = analyzer.export_adjacency_matrix_csv(min_freq, 50)
                    if adj_csv:
                        st.download_button("ğŸ“¥ ä¸‹è½½é‚»æ¥çŸ©é˜µCSV", adj_csv,
                                         f"cooccurrence_matrix_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv", "text/csv")
            else:
                st.warning("æ²¡æœ‰å¯ç”¨çš„åˆ†è¯æ•°æ®")
        except Exception as e:
            st.error(f"å¯¼å‡ºå…±ç°çŸ©é˜µæ—¶å‡ºé”™: {str(e)}")


def render_advanced_analysis_export():
    """æ¸²æŸ“é«˜çº§åˆ†æå¯¼å‡º"""
    st.subheader("ğŸ”¬ é«˜çº§åˆ†æç»“æœå¯¼å‡º")
    
    if not st.session_state.get("texts"):
        st.warning('è¯·å…ˆåœ¨"æ–‡æœ¬é¢„å¤„ç†"æ ‡ç­¾é¡µä¸­å®Œæˆæ–‡æœ¬é¢„å¤„ç†')
        return
    
    data_type = st.selectbox("é€‰æ‹©æ•°æ®", 
                            ["èšç±»ç»“æœ", "æ—¶åºåˆ†æ", "æ¯”è¾ƒåˆ†æ", "å¼•ç”¨åˆ†æ", "è¯­ä¹‰ç½‘ç»œ", "è´¨æ€§ç¼–ç "],
                            key="export_advanced_data_type")
    
    if data_type == "èšç±»ç»“æœ":
        render_clustering_export()
    elif data_type == "æ—¶åºåˆ†æ":
        render_temporal_export()
    elif data_type == "æ¯”è¾ƒåˆ†æ":
        render_comparative_export()
    elif data_type == "å¼•ç”¨åˆ†æ":
        render_citation_export()
    elif data_type == "è¯­ä¹‰ç½‘ç»œ":
        render_semantic_export()
    elif data_type == "è´¨æ€§ç¼–ç ":
        render_coding_export()


def render_clustering_export():
    """æ¸²æŸ“èšç±»ç»“æœå¯¼å‡º"""
    cluster_labels = st.session_state.get("cluster_labels")
    classification_labels = st.session_state.get("classification_labels", {})
    file_names = st.session_state.get("file_names", [])
    
    if cluster_labels is not None:
        st.markdown("**èšç±»ç»“æœ**")
        data = {"æ–‡æ¡£": file_names[:len(cluster_labels)], "èšç±»ID": cluster_labels.tolist()}
        df = pd.DataFrame(data)
        st.dataframe(df, use_container_width=True)
        
        st.download_button("ğŸ“¥ ä¸‹è½½èšç±»ç»“æœCSV", dataframe_to_csv(df),
                         f"clustering_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv", "text/csv")
    elif classification_labels:
        st.markdown("**åˆ†ç±»ç»“æœ**")
        data = {"æ–‡æ¡£": list(classification_labels.keys()), "åˆ†ç±»æ ‡ç­¾": list(classification_labels.values())}
        df = pd.DataFrame(data)
        st.dataframe(df, use_container_width=True)
        
        st.download_button("ğŸ“¥ ä¸‹è½½åˆ†ç±»ç»“æœCSV", dataframe_to_csv(df),
                         f"classification_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv", "text/csv")
    else:
        st.info("è¯·å…ˆåœ¨ã€Œé«˜çº§ç ”ç©¶åˆ†æ â†’ èšç±»åˆ†ç±»ã€ä¸­å®Œæˆèšç±»æˆ–åˆ†ç±»åˆ†æ")


def render_temporal_export():
    """æ¸²æŸ“æ—¶åºåˆ†æå¯¼å‡º"""
    time_labels = st.session_state.get("time_labels", {})
    
    if time_labels and TemporalAnalyzer:
        texts = st.session_state.get("texts", [])
        file_names = st.session_state.get("file_names", [])
        
        analyzer = TemporalAnalyzer(texts, file_names)
        for doc, label in time_labels.items():
            analyzer.set_time_label(doc, label)
        
        st.markdown("**æ—¶é—´æ ‡ç­¾æ•°æ®**")
        csv_content = analyzer.export_time_labels()
        
        df = pd.read_csv(BytesIO(csv_content.encode('utf-8-sig')))
        st.dataframe(df, use_container_width=True)
        
        st.download_button("ğŸ“¥ ä¸‹è½½æ—¶é—´æ ‡ç­¾CSV", csv_content,
                         f"time_labels_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv", "text/csv")
        
        keyword_trends = st.session_state.get("keyword_trends")
        if keyword_trends:
            keywords = list(keyword_trends.keys())
            trend_csv = analyzer.export_trend_data(keywords)
            st.download_button("ğŸ“¥ ä¸‹è½½å…³é”®è¯è¶‹åŠ¿CSV", trend_csv,
                             f"keyword_trends_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv", "text/csv")
    else:
        st.info("è¯·å…ˆåœ¨ã€Œé«˜çº§ç ”ç©¶åˆ†æ â†’ æ—¶åºåˆ†æã€ä¸­è®¾ç½®æ—¶é—´æ ‡ç­¾")


def render_comparative_export():
    """æ¸²æŸ“æ¯”è¾ƒåˆ†æå¯¼å‡º"""
    sim_matrix_csv = st.session_state.get("sim_matrix_csv")
    comparison_csv = st.session_state.get("comparison_csv")
    
    if sim_matrix_csv or comparison_csv:
        col1, col2 = st.columns(2)
        
        with col1:
            if sim_matrix_csv:
                st.download_button("ğŸ“¥ ä¸‹è½½ç›¸ä¼¼åº¦çŸ©é˜µCSV", sim_matrix_csv,
                                 f"similarity_matrix_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv", "text/csv")
        
        with col2:
            if comparison_csv:
                st.download_button("ğŸ“¥ ä¸‹è½½æ¯”è¾ƒæŠ¥å‘ŠCSV", comparison_csv,
                                 f"comparison_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv", "text/csv")
    else:
        st.info("è¯·å…ˆåœ¨ã€Œé«˜çº§ç ”ç©¶åˆ†æ â†’ æ¯”è¾ƒåˆ†æã€ä¸­å®Œæˆæ¯”è¾ƒåˆ†æ")


def render_citation_export():
    """æ¸²æŸ“å¼•ç”¨åˆ†æå¯¼å‡º"""
    citation_network = st.session_state.get("citation_network")
    
    if citation_network and CitationAnalyzer:
        raw_texts = st.session_state.get("raw_texts", [])
        file_names = st.session_state.get("file_names", [])
        
        analyzer = CitationAnalyzer(raw_texts, file_names)
        analyzer.extract_citations()
        analyzer.build_citation_network()
        
        col1, col2 = st.columns(2)
        
        with col1:
            citation_csv = analyzer.export_citation_list()
            st.download_button("ğŸ“¥ ä¸‹è½½å¼•ç”¨åˆ—è¡¨CSV", citation_csv,
                             f"citation_list_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv", "text/csv")
        
        with col2:
            network_csv = analyzer.export_network_data()
            st.download_button("ğŸ“¥ ä¸‹è½½å¼•ç”¨ç½‘ç»œCSV", network_csv,
                             f"citation_network_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv", "text/csv")
        
        stats_csv = analyzer.export_citation_stats()
        st.download_button("ğŸ“¥ ä¸‹è½½å¼•ç”¨ç»Ÿè®¡CSV", stats_csv,
                         f"citation_stats_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv", "text/csv")
    else:
        st.info("è¯·å…ˆåœ¨ã€Œé«˜çº§ç ”ç©¶åˆ†æ â†’ å¼•ç”¨åˆ†æã€ä¸­å®Œæˆå¼•ç”¨åˆ†æ")


def render_semantic_export():
    """æ¸²æŸ“è¯­ä¹‰ç½‘ç»œå¯¼å‡º"""
    semantic_network = st.session_state.get("semantic_network")
    
    if semantic_network and SemanticNetworkBuilder:
        texts = st.session_state.get("texts", [])
        cooccurrence_data = st.session_state.get("cooccurrence_matrix", {})
        
        if cooccurrence_data:
            builder = SemanticNetworkBuilder(texts, cooccurrence_data)
            builder.network = semantic_network
            
            nodes_csv, edges_csv = builder.export_network()
            
            col1, col2 = st.columns(2)
            
            with col1:
                st.download_button("ğŸ“¥ ä¸‹è½½èŠ‚ç‚¹åˆ—è¡¨CSV", nodes_csv,
                                 f"semantic_nodes_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv", "text/csv")
            
            with col2:
                st.download_button("ğŸ“¥ ä¸‹è½½è¾¹åˆ—è¡¨CSV", edges_csv,
                                 f"semantic_edges_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv", "text/csv")
        else:
            st.info("è¯·å…ˆåœ¨ã€ŒåŸºç¡€æ–‡æœ¬åˆ†æ â†’ è¯è¯­å…±ç°ã€ä¸­è®¡ç®—å…±ç°å…³ç³»")
    else:
        st.info("è¯·å…ˆåœ¨ã€Œé«˜çº§ç ”ç©¶åˆ†æ â†’ è¯­ä¹‰ç½‘ç»œã€ä¸­æ„å»ºè¯­ä¹‰ç½‘ç»œ")


def render_coding_export():
    """æ¸²æŸ“è´¨æ€§ç¼–ç å¯¼å‡º"""
    coding_scheme = st.session_state.get("coding_scheme")
    coded_segments = st.session_state.get("coded_segments", [])
    
    if coding_scheme and QualitativeCoder and CodingScheme:
        # å¦‚æœcoding_schemeæ˜¯å­—å…¸ï¼Œéœ€è¦è½¬æ¢ä¸ºCodingSchemeå¯¹è±¡
        if isinstance(coding_scheme, dict):
            scheme = CodingScheme()
            scheme.from_dict(coding_scheme)
        else:
            scheme = coding_scheme
        
        coder = QualitativeCoder(scheme)
        coder.segments = coded_segments
        
        col1, col2 = st.columns(2)
        
        with col1:
            csv_data = coder.export_to_csv()
            st.download_button("ğŸ“¥ å¯¼å‡ºç¼–ç ç»“æœCSV", csv_data,
                             f"coding_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv", "text/csv")
        
        with col2:
            stats_csv = coder.export_statistics_csv()
            st.download_button("ğŸ“¥ å¯¼å‡ºç»Ÿè®¡æ•°æ®CSV", stats_csv,
                             f"coding_statistics_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv", "text/csv")
    else:
        st.info("è¯·å…ˆåœ¨ã€Œé«˜çº§ç ”ç©¶åˆ†æ â†’ è´¨æ€§ç¼–ç ã€ä¸­åˆ›å»ºç¼–ç æ–¹æ¡ˆ")


def render_model_export():
    """æ¸²æŸ“æ¨¡å‹å¯¼å‡º"""
    st.subheader("ğŸ’¾ å¯¼å‡ºLDAæ¨¡å‹")
    
    if st.session_state.get("model_path"):
        st.write(f"å½“å‰æ¨¡å‹è·¯å¾„: {st.session_state.model_path}")
        
        if st.button("å¯¼å‡ºæ¨¡å‹æ–‡ä»¶", key="export_model"):
            with st.spinner("æ­£åœ¨å‡†å¤‡æ¨¡å‹æ–‡ä»¶..."):
                try:
                    with tempfile.TemporaryDirectory() as temp_dir:
                        model_files = [
                            f"{st.session_state.model_path}.gensim",
                            f"{st.session_state.model_path}.pkl"
                        ]
                        
                        zip_path = os.path.join(temp_dir, "lda_model.zip")
                        
                        with zipfile.ZipFile(zip_path, 'w') as zipf:
                            for file in model_files:
                                if os.path.exists(file):
                                    zipf.write(file, os.path.basename(file))
                        
                        with open(zip_path, "rb") as f:
                            zip_data = f.read()
                        
                        st.download_button("ğŸ“¥ ä¸‹è½½æ¨¡å‹æ–‡ä»¶", zip_data,
                                         f"lda_model_{datetime.now().strftime('%Y%m%d_%H%M%S')}.zip",
                                         "application/zip")
                        
                        st.success("æ¨¡å‹æ–‡ä»¶å·²å‡†å¤‡å¥½")
                
                except Exception as e:
                    st.error(f"å¯¼å‡ºæ¨¡å‹æ—¶å‡ºé”™: {str(e)}")
    else:
        st.warning("æœªæ‰¾åˆ°ä¿å­˜çš„æ¨¡å‹æ–‡ä»¶ï¼Œè¯·åœ¨æ¨¡å‹è®­ç»ƒå®Œæˆåå†å°è¯•å¯¼å‡º")
