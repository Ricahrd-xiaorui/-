import streamlit as st
import os
import re
import jieba
import pandas as pd
import numpy as np
from collections import Counter
from gensim import corpora
from gensim.models import Phrases
from pathlib import Path
import time
from utils.session_state import get_session_state, log_message, update_progress

# é¢„å®šä¹‰çš„æ”¿ç­–ç‰¹å®šåœç”¨è¯
DEFAULT_POLICY_STOPWORDS = [
    "æ„è§", "é€šçŸ¥", "å®æ–½", "æ¨è¿›", "å…³äº", "å·¥ä½œ", "æ–¹æ¡ˆ", "è§„åˆ’", "è®¡åˆ’", "æŠ¥å‘Š", 
    "å†³å®š", "éƒ¨ç½²", "è¦æ±‚", "åŠæ³•", "ç»†åˆ™", "è§„å®š", "æ¡ä¾‹", "å®‰æ’", "éƒ¨é—¨", "åœ°æ–¹",
    "å„åœ°", "å„çº§", "æ–‡ä»¶", "ç²¾ç¥", "è®¤çœŸ", "åšæŒ", "ä¸¥æ ¼", "åˆ‡å®", "å…¨é¢", "è¿›ä¸€æ­¥",
    "æªæ–½", "æ”¿ç­–", "å»ºè®®", "åŒå¿—", "é¢†å¯¼", "ç ”ç©¶", "æ˜ç¡®", "å¼ºåŒ–", "çªå‡º", "æ‰©å¤§", 
    "ä¿ƒè¿›", "æé«˜", "åŠ å¿«", "æ¨åŠ¨", "åŠ å¼º", "è½å®"
]

# å¸¸ç”¨ä¸­æ–‡åœç”¨è¯
DEFAULT_COMMON_STOPWORDS = [
    "çš„", "äº†", "å’Œ", "æ˜¯", "å°±", "éƒ½", "è€Œ", "åŠ", "ä¸", "ç€", "æˆ–", "ä¸€ä¸ª", "æ²¡æœ‰",
    "æˆ‘ä»¬", "ä½ ä»¬", "ä»–ä»¬", "å¥¹ä»¬", "å®ƒä»¬", "è¿™ä¸ª", "é‚£ä¸ª", "è¿™äº›", "é‚£äº›", "ä¸æ˜¯",
    "ä»€ä¹ˆ", "è¿™æ ·", "é‚£æ ·", "å¦‚æ­¤", "åªæ˜¯", "ä½†æ˜¯", "å¯æ˜¯", "ç„¶è€Œ", "è€Œä¸”", "å¹¶ä¸”",
    "å› ä¸º", "æ‰€ä»¥", "å¦‚æœ", "è™½ç„¶", "å³ä½¿", "æ— è®º", "åªè¦", "æ—¢ç„¶", "ä¸€æ—¦", "ä¸€ç›´",
    "ä¸€å®š", "å¿…é¡»", "å¯ä»¥", "åº”è¯¥", "èƒ½å¤Ÿ", "éœ€è¦", "ä¸€äº›", "è®¸å¤š", "å¾ˆå¤š", "ä»»ä½•"
]

# ä»é¡¹ç›®æ ¹ç›®å½•çš„stopwords.txtæ–‡ä»¶åŠ è½½åœç”¨è¯
def load_default_stopwords(file_path="stopwords.txt"):
    """ä»é»˜è®¤æ–‡ä»¶åŠ è½½åœç”¨è¯"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            words = f.read().strip().split('\n')
            log_message(f"å·²ä»é»˜è®¤æ–‡ä»¶åŠ è½½ {len(words)} ä¸ªåœç”¨è¯")
            return set(words)
    except Exception as e:
        log_message(f"åŠ è½½é»˜è®¤åœç”¨è¯æ–‡ä»¶å¤±è´¥: {str(e)}", level="warning")
        return set()

class TextPreprocessor:
    """æ–‡æœ¬é¢„å¤„ç†ç±»"""
    
    def __init__(self):
        # åˆå§‹åŒ–åœç”¨è¯
        self.stopwords = set()
        
        # æ ¹æ®ä¼šè¯çŠ¶æ€å†³å®šæ˜¯å¦åŠ è½½é»˜è®¤çš„stopwords.txtæ–‡ä»¶
        if st.session_state.use_default_stopwords_file:
            default_stopwords = load_default_stopwords()
            if default_stopwords:
                self.stopwords.update(default_stopwords)
                log_message(f"å·²ä½¿ç”¨é»˜è®¤stopwords.txtä½œä¸ºåœç”¨è¯åº“ï¼Œå…± {len(default_stopwords)} ä¸ªè¯")
            else:
                # å¦‚æœé»˜è®¤æ–‡ä»¶åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨å†…ç½®åœç”¨è¯
                self.stopwords.update(DEFAULT_COMMON_STOPWORDS)
                if st.session_state.remove_policy_words:
                    self.stopwords.update(DEFAULT_POLICY_STOPWORDS)
                log_message("é»˜è®¤åœç”¨è¯æ–‡ä»¶åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨å†…ç½®åœç”¨è¯", level="warning")
        else:
            # ä¸ä½¿ç”¨é»˜è®¤åœç”¨è¯æ–‡ä»¶ï¼Œä½¿ç”¨å†…ç½®åœç”¨è¯
            self.stopwords.update(DEFAULT_COMMON_STOPWORDS)
            if st.session_state.remove_policy_words:
                self.stopwords.update(DEFAULT_POLICY_STOPWORDS)
        
        # ä»ä¼šè¯çŠ¶æ€åŠ è½½è‡ªå®šä¹‰åœç”¨è¯
        if st.session_state.custom_stopwords:
            self.stopwords.update(st.session_state.custom_stopwords)
        
        # è®¾ç½®å‚æ•°
        self.min_word_length = st.session_state.min_word_length
        self.no_below = st.session_state.no_below
        self.no_above = st.session_state.no_above
        self.min_word_count = st.session_state.min_word_count
    
    def tokenize(self, text):
        """åˆ†è¯å¤„ç†"""
        # æ¸…ç†æ–‡æœ¬
        text = re.sub(r'\s+', ' ', text)  # åˆå¹¶å¤šä½™ç©ºç™½
        text = re.sub(r'[^\w\s\u4e00-\u9fff]', '', text)  # åªä¿ç•™ä¸­æ–‡ã€å­—æ¯ã€æ•°å­—å’Œç©ºç™½
        
        # ä½¿ç”¨jiebaåˆ†è¯
        tokens = jieba.lcut(text)
        
        # è¿‡æ»¤åœç”¨è¯å’ŒçŸ­è¯
        filtered_tokens = [
            token for token in tokens 
            if token not in self.stopwords and len(token) >= self.min_word_length
        ]
        
        return filtered_tokens
    
    def preprocess_texts(self, texts, file_names=None):
        """é¢„å¤„ç†å¤šä¸ªæ–‡æœ¬"""
        tokenized_texts = []
        total = len(texts)
        
        # æ›´æ–°è¿›åº¦
        update_progress(0.0, "å¼€å§‹æ–‡æœ¬é¢„å¤„ç†")
        
        for i, text in enumerate(texts):
            # æ›´æ–°è¿›åº¦
            update_progress(i/total, f"é¢„å¤„ç†æ–‡æœ¬ {i+1}/{total}")
            
            # åˆ†è¯
            tokens = self.tokenize(text)
            tokenized_texts.append(tokens)
            
            # è®°å½•æ—¥å¿—
            if file_names and i < len(file_names):
                log_message(f"å·²å¤„ç†æ–‡ä»¶: {file_names[i]} ({len(tokens)} ä¸ªè¯)")
        
        # æ›´æ–°è¿›åº¦
        update_progress(1.0, "æ–‡æœ¬é¢„å¤„ç†å®Œæˆ")
        
        return tokenized_texts
    
    def create_dictionary_and_corpus(self, tokenized_texts):
        """åˆ›å»ºè¯å…¸å’Œè¯­æ–™åº“"""
        # æ›´æ–°è¿›åº¦
        update_progress(0.0, "å¼€å§‹åˆ›å»ºè¯å…¸å’Œè¯­æ–™åº“")
        
        # åˆ›å»ºè¯å…¸
        dictionary = corpora.Dictionary(tokenized_texts)
        
        # è¿‡æ»¤æç«¯é¢‘ç‡çš„è¯
        dictionary.filter_extremes(
            no_below=self.no_below,
            no_above=self.no_above,
            keep_n=100000  # ä¿ç•™è¶³å¤Ÿå¤šçš„è¯
        )
        
        # åº”ç”¨æœ€å°è¯é¢‘è¿‡æ»¤
        if self.min_word_count > 1:
            # è®¡ç®—è¯é¢‘
            word_counts = {}
            for text in tokenized_texts:
                for word in text:
                    word_counts[word] = word_counts.get(word, 0) + 1
            
            # è¿‡æ»¤ä½é¢‘è¯
            low_freq_ids = [
                dictionary.token2id[word] 
                for word in dictionary.token2id 
                if word_counts.get(word, 0) < self.min_word_count
            ]
            dictionary.filter_tokens(low_freq_ids)
            dictionary.compactify()
            log_message(f"å·²è¿‡æ»¤è¯é¢‘ä½äº{self.min_word_count}çš„è¯è¯­")
        
        # æ›´æ–°è¿›åº¦
        update_progress(0.5, "è¯å…¸åˆ›å»ºå®Œæˆ")
        
        # åˆ›å»ºè¯­æ–™åº“ (è¯è¢‹æ¨¡å‹)
        corpus = [dictionary.doc2bow(text) for text in tokenized_texts]
        
        # è®°å½•æ—¥å¿—
        log_message(f"è¯å…¸å¤§å°: {len(dictionary)}")
        log_message(f"è¯­æ–™åº“å¤§å°: {len(corpus)}")
        
        # æ›´æ–°è¿›åº¦
        update_progress(1.0, "è¯­æ–™åº“åˆ›å»ºå®Œæˆ")
        
        return dictionary, corpus

def load_stopwords_from_file(file):
    """ä»æ–‡ä»¶åŠ è½½åœç”¨è¯"""
    try:
        content = file.read().decode('utf-8')
        words = content.strip().split('\n')
        return set(words)
    except Exception as e:
        st.error(f"åŠ è½½åœç”¨è¯æ–‡ä»¶å¤±è´¥: {str(e)}")
        return set()

def save_stopwords_to_file(stopwords, filename="custom_stopwords.txt"):
    """ä¿å­˜åœç”¨è¯åˆ°æ–‡ä»¶"""
    filepath = os.path.join("temp", filename)
    with open(filepath, 'w', encoding='utf-8') as f:
        for word in sorted(stopwords):
            f.write(word + '\n')
    return filepath

def render_text_processor():
    """æ¸²æŸ“æ–‡æœ¬é¢„å¤„ç†æ¨¡å—"""
    st.header("æ–‡æœ¬é¢„å¤„ç†")
    
    # åŠŸèƒ½ä»‹ç»
    with st.expander("ğŸ“– åŠŸèƒ½ä»‹ç»", expanded=False):
        st.markdown("""
        **æ–‡æœ¬é¢„å¤„ç†æ¨¡å—** å¯¹åŸå§‹æ–‡æœ¬è¿›è¡Œåˆ†è¯ã€æ¸…æ´—å’Œç‰¹å¾æå–ï¼Œä¸ºLDAå»ºæ¨¡åšå‡†å¤‡ã€‚
        
        **ä¸»è¦åŠŸèƒ½ï¼š**
        - ğŸ”¤ **ä¸­æ–‡åˆ†è¯**ï¼šä½¿ç”¨jiebaåˆ†è¯å™¨å¯¹æ–‡æœ¬è¿›è¡Œç²¾ç¡®åˆ†è¯
        - ğŸš« **åœç”¨è¯è¿‡æ»¤**ï¼šç§»é™¤æ— æ„ä¹‰çš„å¸¸ç”¨è¯å’Œæ”¿ç­–ç‰¹å®šè¯æ±‡
        - ğŸ“Š **è¯é¢‘ç»Ÿè®¡**ï¼šç»Ÿè®¡è¯è¯­å‡ºç°é¢‘ç‡ï¼Œè¿‡æ»¤ä½é¢‘å’Œé«˜é¢‘è¯
        - ğŸ“š **è¯å…¸æ„å»º**ï¼šç”Ÿæˆç”¨äºLDAå»ºæ¨¡çš„è¯å…¸å’Œè¯­æ–™åº“
        
        **å‚æ•°è¯´æ˜ï¼š**
        - **æœ€å°è¯é•¿åº¦**ï¼šè¿‡æ»¤çŸ­äºæŒ‡å®šé•¿åº¦çš„è¯è¯­ï¼ˆå»ºè®®2-3ï¼‰
        - **æœ€å°æ–‡æ¡£é¢‘ç‡**ï¼šè¯è¯­è‡³å°‘åœ¨å¤šå°‘æ–‡æ¡£ä¸­å‡ºç°æ‰ä¿ç•™
        - **æœ€å¤§æ–‡æ¡£é¢‘ç‡**ï¼šè¯è¯­æœ€å¤šåœ¨å¤šå°‘æ¯”ä¾‹çš„æ–‡æ¡£ä¸­å‡ºç°ï¼ˆè¿‡æ»¤è¿‡äºå¸¸è§çš„è¯ï¼‰
        - **æœ€å°è¯é¢‘**ï¼šè¯è¯­åœ¨æ•´ä¸ªè¯­æ–™åº“ä¸­çš„æœ€å°å‡ºç°æ¬¡æ•°
        
        **åœç”¨è¯ç®¡ç†ï¼š**
        - æ”¯æŒä½¿ç”¨é»˜è®¤åœç”¨è¯æ–‡ä»¶ï¼ˆstopwords.txtï¼‰
        - æ”¯æŒæ·»åŠ è‡ªå®šä¹‰åœç”¨è¯
        - æ”¯æŒä¸Šä¼ åœç”¨è¯æ–‡ä»¶
        """)
    
    # æ£€æŸ¥æ˜¯å¦å·²åŠ è½½æ–‡ä»¶
    if not st.session_state.raw_texts:
        st.warning('è¯·å…ˆåœ¨"æ•°æ®åŠ è½½"é€‰é¡¹å¡ä¸­åŠ è½½æ–‡ä»¶')
        return
    
    # é¢„å¤„ç†å‚æ•°è®¾ç½®
    with st.expander("é¢„å¤„ç†å‚æ•°è®¾ç½®", expanded=True):
        col1, col2 = st.columns(2)
        
        with col1:
            st.session_state.min_word_length = st.slider(
                "æœ€å°è¯é•¿åº¦", 
                min_value=1, 
                max_value=5, 
                value=st.session_state.min_word_length,
                help="è¿‡æ»¤æ‰çŸ­äºæ­¤é•¿åº¦çš„è¯",
                key="min_word_length_slider"
            )
            
            st.session_state.remove_policy_words = st.checkbox(
                "ç§»é™¤æ”¿ç­–ç‰¹å®šåœç”¨è¯", 
                value=st.session_state.remove_policy_words,
                help="ç§»é™¤å¸¸è§æ”¿ç­–æ–‡ä»¶ä¸­çš„æ— æ„ä¹‰è¯è¯­",
                key="remove_policy_words_checkbox"
            )
        
        with col2:
            st.session_state.no_below = st.slider(
                "æœ€å°æ–‡æ¡£é¢‘ç‡", 
                min_value=1, 
                max_value=10, 
                value=st.session_state.no_below,
                help="è¯è¯­è‡³å°‘åœ¨å¤šå°‘æ–‡æ¡£ä¸­å‡ºç°",
                key="no_below_slider"
            )
            
            st.session_state.no_above = st.slider(
                "æœ€å¤§æ–‡æ¡£é¢‘ç‡", 
                min_value=0.1, 
                max_value=1.0, 
                value=st.session_state.no_above,
                step=0.05,
                help="è¯è¯­æœ€å¤šåœ¨å¤šå°‘æ¯”ä¾‹çš„æ–‡æ¡£ä¸­å‡ºç°",
                key="no_above_slider"
            )
            
            st.session_state.min_word_count = st.slider(
                "æœ€å°è¯é¢‘", 
                min_value=1, 
                max_value=10, 
                value=st.session_state.min_word_count,
                help="è¯è¯­åœ¨æ•´ä¸ªè¯­æ–™åº“ä¸­çš„æœ€å°å‡ºç°æ¬¡æ•°",
                key="min_word_count_slider"
            )
    
    # åœç”¨è¯ç®¡ç†
    with st.expander("åœç”¨è¯ç®¡ç†", expanded=True):
        # æ·»åŠ ä¸€ä¸ªå¤é€‰æ¡†é€‰æ‹©æ˜¯å¦ä½¿ç”¨é»˜è®¤çš„stopwords.txtæ–‡ä»¶
        st.session_state.use_default_stopwords_file = st.checkbox(
            "ä½¿ç”¨é»˜è®¤çš„stopwords.txtæ–‡ä»¶ä½œä¸ºåœç”¨è¯", 
            value=st.session_state.use_default_stopwords_file,
            help="é€‰ä¸­æ—¶å°†ä¼˜å…ˆä½¿ç”¨é¡¹ç›®æ ¹ç›®å½•ä¸‹çš„stopwords.txtæ–‡ä»¶ä½œä¸ºåœç”¨è¯",
            key="use_default_stopwords_file_checkbox"
        )
        
        # è·å–é»˜è®¤åœç”¨è¯
        default_stopwords = load_default_stopwords() if st.session_state.use_default_stopwords_file else set()
        
        # æ˜¾ç¤ºå½“å‰åœç”¨è¯ç»Ÿè®¡
        current_stopwords = set()
        if default_stopwords:
            current_stopwords.update(default_stopwords)
            st.info(f"å·²åŠ è½½é»˜è®¤åœç”¨è¯æ–‡ä»¶(stopwords.txt)ï¼ŒåŒ…å« {len(default_stopwords)} ä¸ªåœç”¨è¯")
        else:
            current_stopwords.update(DEFAULT_COMMON_STOPWORDS)
            if st.session_state.remove_policy_words:
                current_stopwords.update(DEFAULT_POLICY_STOPWORDS)
        
        # æ·»åŠ è‡ªå®šä¹‰åœç”¨è¯
        current_stopwords.update(st.session_state.custom_stopwords)
        
        st.write(f"å½“å‰åœç”¨è¯æ€»æ•°é‡: {len(current_stopwords)}")
        
        # åœç”¨è¯ç®¡ç†é€‰é¡¹
        tabs = st.tabs(["æ·»åŠ åœç”¨è¯", "ä¸Šä¼ åœç”¨è¯æ–‡ä»¶", "æŸ¥çœ‹å’Œç¼–è¾‘"])
        
        # æ·»åŠ åœç”¨è¯æ ‡ç­¾é¡µ
        with tabs[0]:
            new_stopwords = st.text_area(
                "è¾“å…¥åœç”¨è¯(æ¯è¡Œä¸€ä¸ª)", 
                height=150,
                help="æ·»åŠ è‡ªå®šä¹‰åœç”¨è¯ï¼Œæ¯è¡Œè¾“å…¥ä¸€ä¸ªè¯",
                key="new_stopwords_textarea"
            )
            
            # ä½¿ç”¨ä¸åŒçš„æ–¹å¼å¤„ç†æŒ‰é’®ç‚¹å‡»ï¼Œé¿å…ä¼šè¯çŠ¶æ€å†²çª
            add_stopwords_clicked = st.button("æ·»åŠ åœç”¨è¯", key="add_stopwords_button")
            if add_stopwords_clicked and new_stopwords:
                words = new_stopwords.strip().split('\n')
                words = [word.strip() for word in words if word.strip()]
                if words:
                    st.session_state.custom_stopwords.update(words)
                    st.success(f"å·²æ·»åŠ  {len(words)} ä¸ªåœç”¨è¯")
                    log_message(f"å·²æ·»åŠ  {len(words)} ä¸ªåœç”¨è¯", level="success")
        
        # ä¸Šä¼ åœç”¨è¯æ–‡ä»¶æ ‡ç­¾é¡µ
        with tabs[1]:
            uploaded_stopwords = st.file_uploader(
                "ä¸Šä¼ åœç”¨è¯æ–‡ä»¶", 
                type=["txt"], 
                help="ä¸Šä¼ åŒ…å«åœç”¨è¯çš„TXTæ–‡ä»¶ï¼Œæ¯è¡Œä¸€ä¸ªè¯",
                key="stopwords_file_uploader"
            )
            
            if uploaded_stopwords is not None:
                load_stopwords_clicked = st.button("ä»æ–‡ä»¶åŠ è½½åœç”¨è¯", key="load_stopwords_button")
                if load_stopwords_clicked:
                    new_words = load_stopwords_from_file(uploaded_stopwords)
                    if new_words:
                        st.session_state.custom_stopwords.update(new_words)
                        st.success(f"å·²ä»æ–‡ä»¶åŠ è½½ {len(new_words)} ä¸ªåœç”¨è¯")
                        log_message(f"å·²ä»æ–‡ä»¶åŠ è½½ {len(new_words)} ä¸ªåœç”¨è¯", level="success")
        
        # æŸ¥çœ‹å’Œç¼–è¾‘æ ‡ç­¾é¡µ
        with tabs[2]:
            if current_stopwords:
                # å°†åœç”¨è¯è½¬æ¢ä¸ºDataFrameä»¥ä¾¿æŸ¥çœ‹
                stopwords_df = pd.DataFrame({
                    "åœç”¨è¯": sorted(current_stopwords)
                })
                
                st.dataframe(stopwords_df, use_container_width=True, height=300)
                
                # ä¿å­˜åœç”¨è¯
                save_stopwords_clicked = st.button("ä¿å­˜åœç”¨è¯åˆ°æ–‡ä»¶", key="save_stopwords_button")
                if save_stopwords_clicked:
                    filepath = save_stopwords_to_file(current_stopwords)
                    st.success(f"åœç”¨è¯å·²ä¿å­˜åˆ°: {filepath}")
                    log_message(f"åœç”¨è¯å·²ä¿å­˜åˆ°: {filepath}", level="success")
                
                # æ¸…ç©ºè‡ªå®šä¹‰åœç”¨è¯
                clear_stopwords_clicked = st.button("æ¸…ç©ºè‡ªå®šä¹‰åœç”¨è¯", key="clear_custom_stopwords_button")
                if clear_stopwords_clicked:
                    st.session_state.custom_stopwords = set()
                    st.success("å·²æ¸…ç©ºè‡ªå®šä¹‰åœç”¨è¯")
                    log_message("å·²æ¸…ç©ºè‡ªå®šä¹‰åœç”¨è¯", level="success")
    
    # å¼€å§‹é¢„å¤„ç†æŒ‰é’®
    if st.button("å¼€å§‹æ–‡æœ¬é¢„å¤„ç†", key="start_preprocessing_button"):
        with st.spinner("æ­£åœ¨è¿›è¡Œæ–‡æœ¬é¢„å¤„ç†..."):
            start_time = time.time()
            
            # åˆ›å»ºé¢„å¤„ç†å™¨å®ä¾‹
            preprocessor = TextPreprocessor()
            
            # å¤„ç†æ–‡æœ¬
            tokenized_texts = preprocessor.preprocess_texts(
                st.session_state.raw_texts, 
                st.session_state.file_names
            )
            
            # åˆ›å»ºè¯å…¸å’Œè¯­æ–™åº“
            dictionary, corpus = preprocessor.create_dictionary_and_corpus(tokenized_texts)
            
            # ä¿å­˜åˆ°ä¼šè¯çŠ¶æ€
            st.session_state.texts = tokenized_texts
            st.session_state.dictionary = dictionary
            st.session_state.corpus = corpus
            
            # è®¡ç®—å¹¶æ˜¾ç¤ºè€—æ—¶
            elapsed_time = time.time() - start_time
            log_message(f"é¢„å¤„ç†å®Œæˆï¼Œè€—æ—¶: {elapsed_time:.2f}ç§’", level="success")
            
            # æ˜¾ç¤ºæˆåŠŸæ¶ˆæ¯
            st.success(f"æ–‡æœ¬é¢„å¤„ç†å®Œæˆï¼Œè€—æ—¶: {elapsed_time:.2f}ç§’")
    
    # å¦‚æœå·²ç»å®Œæˆé¢„å¤„ç†ï¼Œæ˜¾ç¤ºç»“æœ
    if st.session_state.texts and st.session_state.dictionary and st.session_state.corpus:
        st.subheader("é¢„å¤„ç†ç»“æœ")
        
        # æ˜¾ç¤ºåŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
        col1, col2, col3 = st.columns(3)
        col1.metric("æ–‡æ¡£æ•°é‡", len(st.session_state.texts))
        col2.metric("è¯å…¸å¤§å°", len(st.session_state.dictionary))
        col3.metric("å¹³å‡æ–‡æ¡£é•¿åº¦", f"{sum(len(text) for text in st.session_state.texts) / len(st.session_state.texts):.1f}è¯")
        
        # è¯é¢‘ç»Ÿè®¡
        with st.expander("è¯é¢‘ç»Ÿè®¡", expanded=False):
            # è®¡ç®—è¯é¢‘
            word_counts = Counter()
            for text in st.session_state.texts:
                word_counts.update(text)
            
            # è·å–å‰50ä¸ªé«˜é¢‘è¯
            top_words = word_counts.most_common(50)
            
            # è½¬æ¢ä¸ºDataFrame
            df_word_counts = pd.DataFrame(top_words, columns=["è¯è¯­", "é¢‘æ¬¡"])
            
            # æ˜¾ç¤ºè¯é¢‘è¡¨æ ¼
            st.dataframe(df_word_counts, use_container_width=True)
            
            # è¯é¢‘ç›´æ–¹å›¾
            if len(top_words) > 0:
                st.bar_chart(df_word_counts.set_index("è¯è¯­"))
        
        # æ–‡æ¡£é•¿åº¦åˆ†å¸ƒ
        with st.expander("æ–‡æ¡£é•¿åº¦åˆ†å¸ƒ", expanded=False):
            doc_lengths = [len(text) for text in st.session_state.texts]
            
            # åˆ›å»ºDataFrame
            df_lengths = pd.DataFrame({
                "æ–‡ä»¶å": st.session_state.file_names[:len(doc_lengths)],
                "è¯è¯­æ•°é‡": doc_lengths
            })
            
            # æ˜¾ç¤ºæ–‡æ¡£é•¿åº¦è¡¨æ ¼
            st.dataframe(df_lengths.sort_values("è¯è¯­æ•°é‡", ascending=False), use_container_width=True)
            
            # æ–‡æ¡£é•¿åº¦ç›´æ–¹å›¾
            st.bar_chart(df_lengths.set_index("æ–‡ä»¶å"))
        
        # é¢„å¤„ç†æ–‡æœ¬é¢„è§ˆ
        with st.expander("é¢„å¤„ç†æ–‡æœ¬é¢„è§ˆ", expanded=False):
            preview_idx = st.selectbox(
                "é€‰æ‹©æ–‡æ¡£é¢„è§ˆ", 
                range(len(st.session_state.texts)),
                format_func=lambda i: st.session_state.file_names[i] if i < len(st.session_state.file_names) else f"æ–‡æ¡£ {i+1}",
                key="text_preview_select"
            )
            
            if preview_idx is not None:
                st.write(f"**åˆ†è¯ç»“æœ** (å…± {len(st.session_state.texts[preview_idx])} ä¸ªè¯):")
                st.write(" ".join(st.session_state.texts[preview_idx])) 