import streamlit as st
import os
import re
import jieba
import pandas as pd
import numpy as np
from collections import Counter
from gensim import corpora
from gensim.models import Phrases
from pathlib import Path
import time
from utils.session_state import get_session_state, log_message, update_progress

# é¢„å®šä¹‰çš„æ”¿ç­–ç‰¹å®šåœç”¨è¯
DEFAULT_POLICY_STOPWORDS = [
    "æ„è§", "é€šçŸ¥", "å®æ–½", "æ¨è¿›", "å…³äº", "å·¥ä½œ", "æ–¹æ¡ˆ", "è§„åˆ’", "è®¡åˆ’", "æŠ¥å‘Š", 
    "å†³å®š", "éƒ¨ç½²", "è¦æ±‚", "åŠæ³•", "ç»†åˆ™", "è§„å®š", "æ¡ä¾‹", "å®‰æ’", "éƒ¨é—¨", "åœ°æ–¹",
    "å„åœ°", "å„çº§", "æ–‡ä»¶", "ç²¾ç¥", "è®¤çœŸ", "åšæŒ", "ä¸¥æ ¼", "åˆ‡å®", "å…¨é¢", "è¿›ä¸€æ­¥",
    "æªæ–½", "æ”¿ç­–", "å»ºè®®", "åŒå¿—", "é¢†å¯¼", "ç ”ç©¶", "æ˜ç¡®", "å¼ºåŒ–", "çªå‡º", "æ‰©å¤§", 
    "ä¿ƒè¿›", "æé«˜", "åŠ å¿«", "æ¨åŠ¨", "åŠ å¼º", "è½å®"
]

# å¸¸ç”¨ä¸­æ–‡åœç”¨è¯
DEFAULT_COMMON_STOPWORDS = [
    "çš„", "äº†", "å’Œ", "æ˜¯", "å°±", "éƒ½", "è€Œ", "åŠ", "ä¸", "ç€", "æˆ–", "ä¸€ä¸ª", "æ²¡æœ‰",
    "æˆ‘ä»¬", "ä½ ä»¬", "ä»–ä»¬", "å¥¹ä»¬", "å®ƒä»¬", "è¿™ä¸ª", "é‚£ä¸ª", "è¿™äº›", "é‚£äº›", "ä¸æ˜¯",
    "ä»€ä¹ˆ", "è¿™æ ·", "é‚£æ ·", "å¦‚æ­¤", "åªæ˜¯", "ä½†æ˜¯", "å¯æ˜¯", "ç„¶è€Œ", "è€Œä¸”", "å¹¶ä¸”",
    "å› ä¸º", "æ‰€ä»¥", "å¦‚æœ", "è™½ç„¶", "å³ä½¿", "æ— è®º", "åªè¦", "æ—¢ç„¶", "ä¸€æ—¦", "ä¸€ç›´",
    "ä¸€å®š", "å¿…é¡»", "å¯ä»¥", "åº”è¯¥", "èƒ½å¤Ÿ", "éœ€è¦", "ä¸€äº›", "è®¸å¤š", "å¾ˆå¤š", "ä»»ä½•"
]

# ä»é¡¹ç›®æ ¹ç›®å½•çš„stopwords.txtæ–‡ä»¶åŠ è½½åœç”¨è¯
def load_default_stopwords(file_path="stopwords.txt"):
    """ä»é»˜è®¤æ–‡ä»¶åŠ è½½åœç”¨è¯"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            words = f.read().strip().split('\n')
            log_message(f"å·²ä»é»˜è®¤æ–‡ä»¶åŠ è½½ {len(words)} ä¸ªåœç”¨è¯")
            return set(words)
    except Exception as e:
        log_message(f"åŠ è½½é»˜è®¤åœç”¨è¯æ–‡ä»¶å¤±è´¥: {str(e)}", level="warning")
        return set()

class TextPreprocessor:
    """æ–‡æœ¬é¢„å¤„ç†ç±»"""
    
    def __init__(self):
        # åˆå§‹åŒ–åœç”¨è¯
        self.stopwords = set()
        
        # æ ¹æ®ä¼šè¯çŠ¶æ€å†³å®šæ˜¯å¦åŠ è½½é»˜è®¤çš„stopwords.txtæ–‡ä»¶
        if st.session_state.use_default_stopwords_file:
            default_stopwords = load_default_stopwords()
            if default_stopwords:
                self.stopwords.update(default_stopwords)
                log_message(f"å·²ä½¿ç”¨é»˜è®¤stopwords.txtä½œä¸ºåœç”¨è¯åº“ï¼Œå…± {len(default_stopwords)} ä¸ªè¯")
            else:
                # å¦‚æœé»˜è®¤æ–‡ä»¶åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨å†…ç½®åœç”¨è¯
                self.stopwords.update(DEFAULT_COMMON_STOPWORDS)
                if st.session_state.remove_policy_words:
                    self.stopwords.update(DEFAULT_POLICY_STOPWORDS)
                log_message("é»˜è®¤åœç”¨è¯æ–‡ä»¶åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨å†…ç½®åœç”¨è¯", level="warning")
        else:
            # ä¸ä½¿ç”¨é»˜è®¤åœç”¨è¯æ–‡ä»¶ï¼Œä½¿ç”¨å†…ç½®åœç”¨è¯
            self.stopwords.update(DEFAULT_COMMON_STOPWORDS)
            if st.session_state.remove_policy_words:
                self.stopwords.update(DEFAULT_POLICY_STOPWORDS)
        
        # ä»ä¼šè¯çŠ¶æ€åŠ è½½è‡ªå®šä¹‰åœç”¨è¯
        if st.session_state.custom_stopwords:
            self.stopwords.update(st.session_state.custom_stopwords)
        
        # è®¾ç½®å‚æ•°
        self.min_word_length = st.session_state.min_word_length
        self.no_below = st.session_state.no_below
        self.no_above = st.session_state.no_above
        self.min_word_count = st.session_state.min_word_count
    
    def tokenize(self, text):
        """åˆ†è¯å¤„ç†"""
        # æ¸…ç†æ–‡æœ¬
        text = re.sub(r'\s+', ' ', text)  # åˆå¹¶å¤šä½™ç©ºç™½
        text = re.sub(r'[^\w\s\u4e00-\u9fff]', '', text)  # åªä¿ç•™ä¸­æ–‡ã€å­—æ¯ã€æ•°å­—å’Œç©ºç™½
        
        # ä½¿ç”¨jiebaåˆ†è¯
        tokens = jieba.lcut(text)
        
        # è¿‡æ»¤åœç”¨è¯å’ŒçŸ­è¯
        filtered_tokens = [
            token for token in tokens 
            if token not in self.stopwords and len(token) >= self.min_word_length
        ]
        
        return filtered_tokens
    
    def preprocess_texts(self, texts, file_names=None):
        """é¢„å¤„ç†å¤šä¸ªæ–‡æœ¬"""
        tokenized_texts = []
        total = len(texts)
        
        # æ›´æ–°è¿›åº¦
        update_progress(0.0, "å¼€å§‹æ–‡æœ¬é¢„å¤„ç†")
        
        for i, text in enumerate(texts):
            # æ›´æ–°è¿›åº¦
            update_progress(i/total, f"é¢„å¤„ç†æ–‡æœ¬ {i+1}/{total}")
            
            # åˆ†è¯
            tokens = self.tokenize(text)
            tokenized_texts.append(tokens)
            
            # è®°å½•æ—¥å¿—
            if file_names and i < len(file_names):
                log_message(f"å·²å¤„ç†æ–‡ä»¶: {file_names[i]} ({len(tokens)} ä¸ªè¯)")
        
        # æ›´æ–°è¿›åº¦
        update_progress(1.0, "æ–‡æœ¬é¢„å¤„ç†å®Œæˆ")
        
        return tokenized_texts
    
    def create_dictionary_and_corpus(self, tokenized_texts):
        """åˆ›å»ºè¯å…¸å’Œè¯­æ–™åº“"""
        # æ›´æ–°è¿›åº¦
        update_progress(0.0, "å¼€å§‹åˆ›å»ºè¯å…¸å’Œè¯­æ–™åº“")
        
        # åˆ›å»ºè¯å…¸
        dictionary = corpora.Dictionary(tokenized_texts)
        
        # è¿‡æ»¤æç«¯é¢‘ç‡çš„è¯
        dictionary.filter_extremes(
            no_below=self.no_below,
            no_above=self.no_above,
            keep_n=100000  # ä¿ç•™è¶³å¤Ÿå¤šçš„è¯
        )
        
        # åº”ç”¨æœ€å°è¯é¢‘è¿‡æ»¤
        if self.min_word_count > 1:
            # è®¡ç®—è¯é¢‘
            word_counts = {}
            for text in tokenized_texts:
                for word in text:
                    word_counts[word] = word_counts.get(word, 0) + 1
            
            # è¿‡æ»¤ä½é¢‘è¯
            low_freq_ids = [
                dictionary.token2id[word] 
                for word in dictionary.token2id 
                if word_counts.get(word, 0) < self.min_word_count
            ]
            dictionary.filter_tokens(low_freq_ids)
            dictionary.compactify()
            log_message(f"å·²è¿‡æ»¤è¯é¢‘ä½äº{self.min_word_count}çš„è¯è¯­")
        
        # æ›´æ–°è¿›åº¦
        update_progress(0.5, "è¯å…¸åˆ›å»ºå®Œæˆ")
        
        # åˆ›å»ºè¯­æ–™åº“ (è¯è¢‹æ¨¡å‹)
        corpus = [dictionary.doc2bow(text) for text in tokenized_texts]
        
        # è®°å½•æ—¥å¿—
        log_message(f"è¯å…¸å¤§å°: {len(dictionary)}")
        log_message(f"è¯­æ–™åº“å¤§å°: {len(corpus)}")
        
        # æ›´æ–°è¿›åº¦
        update_progress(1.0, "è¯­æ–™åº“åˆ›å»ºå®Œæˆ")
        
        return dictionary, corpus

def load_stopwords_from_file(file):
    """ä»æ–‡ä»¶åŠ è½½åœç”¨è¯"""
    try:
        content = file.read().decode('utf-8')
        words = content.strip().split('\n')
        return set(words)
    except Exception as e:
        st.error(f"åŠ è½½åœç”¨è¯æ–‡ä»¶å¤±è´¥: {str(e)}")
        return set()

def save_stopwords_to_file(stopwords, filename="custom_stopwords.txt"):
    """ä¿å­˜åœç”¨è¯åˆ°æ–‡ä»¶"""
    filepath = os.path.join("temp", filename)
    with open(filepath, 'w', encoding='utf-8') as f:
        for word in sorted(stopwords):
            f.write(word + '\n')
    return filepath

def render_text_processor():
    """æ¸²æŸ“æ–‡æœ¬é¢„å¤„ç†æ¨¡å—"""
    st.header("æ–‡æœ¬é¢„å¤„ç†")
    
    # åŠŸèƒ½ä»‹ç»ä¸æ“ä½œæ‰‹å†Œ
    with st.expander("ğŸ“– åŠŸèƒ½ä»‹ç»ä¸æ“ä½œæ‰‹å†Œ", expanded=False):
        st.markdown("""
        ## ğŸ”§ æ–‡æœ¬é¢„å¤„ç†æ¨¡å—
        
        **åŠŸèƒ½æ¦‚è¿°**ï¼šå¯¹åŸå§‹æ–‡æœ¬è¿›è¡Œåˆ†è¯ã€æ¸…æ´—å’Œç‰¹å¾æå–ï¼Œä¸ºLDAä¸»é¢˜å»ºæ¨¡åšå‡†å¤‡ã€‚
        
        ---
        
        ### ğŸ¯ ä½¿ç”¨åœºæ™¯
        
        | åœºæ™¯ | å‚æ•°å»ºè®® | è¯´æ˜ |
        |------|----------|------|
        | æ”¿ç­–æ–‡æœ¬åˆ†æ | æœ€å°è¯é•¿=2ï¼Œç§»é™¤æ”¿ç­–åœç”¨è¯ | è¿‡æ»¤æ”¿ç­–æ–‡ä»¶ä¸­çš„å¸¸è§æ— æ„ä¹‰è¯ |
        | å­¦æœ¯è®ºæ–‡ç ”ç©¶ | æœ€å°è¯é•¿=2ï¼Œæœ€å°æ–‡æ¡£é¢‘ç‡=2 | ç¡®ä¿è¯è¯­å…·æœ‰ç»Ÿè®¡æ„ä¹‰ |
        | çŸ­æ–‡æœ¬åˆ†æ | æœ€å°è¯é•¿=1ï¼Œæœ€å°æ–‡æ¡£é¢‘ç‡=1 | ä¿ç•™æ›´å¤šè¯è¯­ä¿¡æ¯ |
        | å¤§è§„æ¨¡è¯­æ–™ | æœ€å¤§æ–‡æ¡£é¢‘ç‡=0.5ï¼Œæœ€å°è¯é¢‘=3 | è¿‡æ»¤è¿‡äºå¸¸è§å’Œç¨€æœ‰çš„è¯ |
        
        ---
        
        ### ğŸ“‹ æ“ä½œæ­¥éª¤
        
        **æ­¥éª¤1ï¼šé…ç½®ä¸“ä¸šè¯å…¸ï¼ˆå¯é€‰ä½†æ¨èï¼‰**
        1. å±•å¼€"ä¸“ä¸šè¯å…¸ç®¡ç†"
        2. æ·»åŠ é¢†åŸŸä¸“ä¸šæœ¯è¯­ï¼ˆå¦‚"ä¹¡æ‘æŒ¯å…´"ã€"æ•°å­—ç»æµ"ç­‰ï¼‰
        3. è¿™äº›è¯æ±‡ä¼šè¢«ä½œä¸ºæ•´ä½“è¯†åˆ«ï¼Œä¸ä¼šè¢«é”™è¯¯åˆ†è¯
        
        **æ­¥éª¤2ï¼šè®¾ç½®é¢„å¤„ç†å‚æ•°**
        1. å±•å¼€"é¢„å¤„ç†å‚æ•°è®¾ç½®"
        2. æ ¹æ®ç ”ç©¶éœ€è¦è°ƒæ•´å„é¡¹å‚æ•°
        3. å‚è€ƒä¸‹æ–¹å‚æ•°è¯´æ˜è¡¨æ ¼
        
        **æ­¥éª¤3ï¼šé…ç½®åœç”¨è¯**
        1. å±•å¼€"åœç”¨è¯ç®¡ç†"
        2. é€‰æ‹©æ˜¯å¦ä½¿ç”¨é»˜è®¤åœç”¨è¯æ–‡ä»¶
        3. å¯æ·»åŠ è‡ªå®šä¹‰åœç”¨è¯æˆ–ä¸Šä¼ åœç”¨è¯æ–‡ä»¶
        
        **æ­¥éª¤4ï¼šæ‰§è¡Œé¢„å¤„ç†**
        1. ç‚¹å‡»"å¼€å§‹æ–‡æœ¬é¢„å¤„ç†"æŒ‰é’®
        2. ç­‰å¾…å¤„ç†å®Œæˆ
        3. æŸ¥çœ‹é¢„å¤„ç†ç»“æœç»Ÿè®¡
        
        ---
        
        ### âš™ï¸ å‚æ•°è¯¦è§£
        
        | å‚æ•° | èŒƒå›´ | é»˜è®¤å€¼ | è¯´æ˜ | å­¦æœ¯ç ”ç©¶å»ºè®® |
        |------|------|--------|------|--------------|
        | æœ€å°è¯é•¿åº¦ | 1-5 | 2 | è¿‡æ»¤çŸ­äºæ­¤é•¿åº¦çš„è¯ | ä¸­æ–‡å»ºè®®2ï¼Œä¿ç•™åŒå­—è¯ |
        | æœ€å°æ–‡æ¡£é¢‘ç‡ | 1-10 | 2 | è¯è¯­è‡³å°‘å‡ºç°åœ¨å¤šå°‘æ–‡æ¡£ä¸­ | å»ºè®®2-3ï¼Œè¿‡æ»¤å¶å‘è¯ |
        | æœ€å¤§æ–‡æ¡£é¢‘ç‡ | 0.1-1.0 | 0.85 | è¯è¯­æœ€å¤šå‡ºç°åœ¨å¤šå°‘æ¯”ä¾‹çš„æ–‡æ¡£ä¸­ | å»ºè®®0.5-0.8ï¼Œè¿‡æ»¤è¿‡äºå¸¸è§çš„è¯ |
        | æœ€å°è¯é¢‘ | 1-10 | 2 | è¯è¯­åœ¨è¯­æ–™åº“ä¸­çš„æœ€å°å‡ºç°æ¬¡æ•° | å»ºè®®2-5ï¼Œç¡®ä¿ç»Ÿè®¡æ„ä¹‰ |
        
        ---
        
        ### ğŸ”¤ åˆ†è¯ç®—æ³•è¯´æ˜
        
        æœ¬ç³»ç»Ÿä½¿ç”¨ **jiebaåˆ†è¯å™¨** è¿›è¡Œä¸­æ–‡åˆ†è¯ï¼š
        - **ç²¾ç¡®æ¨¡å¼**ï¼šå°†å¥å­æœ€ç²¾ç¡®åœ°åˆ‡å¼€ï¼Œé€‚åˆæ–‡æœ¬åˆ†æ
        - **ç”¨æˆ·è¯å…¸**ï¼šæ”¯æŒæ·»åŠ ä¸“ä¸šæœ¯è¯­ï¼Œæé«˜åˆ†è¯å‡†ç¡®æ€§
        - **è¯æ€§æ ‡æ³¨**ï¼šå¯é€‰åŠŸèƒ½ï¼Œç”¨äºåç»­è¯æ€§ç­›é€‰
        
        ---
        
        ### ğŸš« åœç”¨è¯è¯´æ˜
        
        **åœç”¨è¯ç±»å‹ï¼š**
        - **é€šç”¨åœç”¨è¯**ï¼šçš„ã€äº†ã€å’Œã€æ˜¯ã€åœ¨ç­‰å¸¸è§è™šè¯
        - **æ”¿ç­–åœç”¨è¯**ï¼šæ„è§ã€é€šçŸ¥ã€å®æ–½ã€æ¨è¿›ç­‰æ”¿ç­–æ–‡ä»¶å¸¸è§è¯
        - **è‡ªå®šä¹‰åœç”¨è¯**ï¼šç”¨æˆ·æ ¹æ®ç ”ç©¶éœ€è¦æ·»åŠ çš„è¯è¯­
        
        **åœç”¨è¯æ¥æºä¼˜å…ˆçº§ï¼š**
        1. é»˜è®¤stopwords.txtæ–‡ä»¶ï¼ˆå¦‚å¯ç”¨ï¼‰
        2. å†…ç½®é€šç”¨åœç”¨è¯
        3. æ”¿ç­–ç‰¹å®šåœç”¨è¯ï¼ˆå¦‚å¯ç”¨ï¼‰
        4. ç”¨æˆ·è‡ªå®šä¹‰åœç”¨è¯
        
        ---
        
        ### ğŸ’¡ ä½¿ç”¨å»ºè®®
        
        **å­¦æœ¯ç ”ç©¶å»ºè®®ï¼š**
        - è®°å½•æ‰€æœ‰é¢„å¤„ç†å‚æ•°è®¾ç½®ï¼Œä¾¿äºè®ºæ–‡æ–¹æ³•éƒ¨åˆ†æ’°å†™
        - å»ºè®®å…ˆç”¨é»˜è®¤å‚æ•°é¢„å¤„ç†ï¼ŒæŸ¥çœ‹ç»“æœåå†è°ƒæ•´
        - å…³æ³¨è¯é¢‘ç»Ÿè®¡ä¸­çš„é«˜é¢‘è¯ï¼Œåˆ¤æ–­æ˜¯å¦éœ€è¦æ·»åŠ åœç”¨è¯
        
        **å‚æ•°è°ƒä¼˜å»ºè®®ï¼š**
        - å¦‚æœä¸»é¢˜è¯ä¸­å‡ºç°å¤ªå¤šæ— æ„ä¹‰è¯ï¼Œå¢åŠ åœç”¨è¯
        - å¦‚æœè¯å…¸å¤ªå°ï¼Œé™ä½æœ€å°æ–‡æ¡£é¢‘ç‡å’Œæœ€å°è¯é¢‘
        - å¦‚æœè¯å…¸å¤ªå¤§ï¼Œæé«˜æœ€å°æ–‡æ¡£é¢‘ç‡æˆ–é™ä½æœ€å¤§æ–‡æ¡£é¢‘ç‡
        
        ---
        
        ### â“ å¸¸è§é—®é¢˜
        
        **Q: ä¸“ä¸šæœ¯è¯­è¢«é”™è¯¯åˆ†è¯æ€ä¹ˆåŠï¼Ÿ**
        A: åœ¨"ä¸“ä¸šè¯å…¸ç®¡ç†"ä¸­æ·»åŠ è¯¥æœ¯è¯­ï¼Œç³»ç»Ÿä¼šå°†å…¶ä½œä¸ºæ•´ä½“è¯†åˆ«ã€‚
        
        **Q: é¢„å¤„ç†åè¯å…¸å¤ªå°æ€ä¹ˆåŠï¼Ÿ**
        A: é™ä½æœ€å°æ–‡æ¡£é¢‘ç‡å’Œæœ€å°è¯é¢‘å‚æ•°ï¼Œæˆ–å‡å°‘åœç”¨è¯ã€‚
        
        **Q: å¦‚ä½•åˆ¤æ–­é¢„å¤„ç†æ•ˆæœå¥½ä¸å¥½ï¼Ÿ**
        A: æŸ¥çœ‹è¯é¢‘ç»Ÿè®¡ä¸­çš„é«˜é¢‘è¯ï¼Œåº”è¯¥æ˜¯æœ‰æ„ä¹‰çš„å®è¯ï¼›æŸ¥çœ‹æ–‡æ¡£é•¿åº¦åˆ†å¸ƒï¼Œä¸åº”æœ‰è¿‡çŸ­çš„æ–‡æ¡£ã€‚
        
        **Q: é¢„å¤„ç†å‚æ•°å¦‚ä½•åœ¨è®ºæ–‡ä¸­æŠ¥å‘Šï¼Ÿ**
        A: å»ºè®®æŠ¥å‘Šï¼šåˆ†è¯å·¥å…·ï¼ˆjiebaï¼‰ã€æœ€å°è¯é•¿ã€æ–‡æ¡£é¢‘ç‡èŒƒå›´ã€åœç”¨è¯æ¥æºç­‰ã€‚
        """)
    
    # æ£€æŸ¥æ˜¯å¦å·²åŠ è½½æ–‡ä»¶
    if not st.session_state.raw_texts:
        st.warning('è¯·å…ˆåœ¨"æ•°æ®åŠ è½½"é€‰é¡¹å¡ä¸­åŠ è½½æ–‡ä»¶')
        return
    
    # ========== ä¸“ä¸šè¯å…¸ç®¡ç†ï¼ˆæ”¾åœ¨æœ€å‰é¢ï¼Œå› ä¸ºå½±å“åˆ†è¯ç»“æœï¼‰==========
    with st.expander("ğŸ“š ä¸“ä¸šè¯å…¸ç®¡ç†", expanded=False):
        st.markdown("""
        **ä¸“ä¸šè¯å…¸** ç”¨äºæé«˜åˆ†è¯å‡†ç¡®æ€§ï¼Œç¡®ä¿ä¸“ä¸šæœ¯è¯­è¢«æ­£ç¡®è¯†åˆ«ã€‚
        è¯å…¸ä¸­çš„è¯æ±‡ä¼šè¢«æ·»åŠ åˆ°jiebaåˆ†è¯å™¨çš„ç”¨æˆ·è¯å…¸ä¸­ã€‚
        
        âš ï¸ **æ³¨æ„**ï¼šè¯·åœ¨æ‰§è¡Œé¢„å¤„ç†ä¹‹å‰é…ç½®å¥½è¯å…¸ï¼
        """)
        
        try:
            from modules.dictionary_manager import DictionaryManager, render_dictionary_manager_compact
            
            # åˆå§‹åŒ–è¯å…¸ç®¡ç†å™¨
            if st.session_state.get("dictionary_manager") is None:
                st.session_state["dictionary_manager"] = DictionaryManager()
            
            # æ¸²æŸ“ç´§å‡‘ç‰ˆè¯å…¸ç®¡ç†ç•Œé¢
            render_dictionary_manager_compact()
            
        except ImportError:
            st.info("ğŸ“¦ è¯å…¸ç®¡ç†æ¨¡å—æ­£åœ¨åŠ è½½ä¸­...")
            
            # æä¾›ç®€å•çš„è¯å…¸è¾“å…¥åŠŸèƒ½ä½œä¸ºå¤‡é€‰
            custom_dict_words = st.text_area(
                "è¾“å…¥ä¸“ä¸šè¯æ±‡ï¼ˆæ¯è¡Œä¸€ä¸ªï¼‰",
                height=100,
                help="è¿™äº›è¯æ±‡ä¼šè¢«æ·»åŠ åˆ°jiebaåˆ†è¯å™¨ä¸­",
                key="simple_custom_dict"
            )
            
            if st.button("åº”ç”¨ä¸“ä¸šè¯æ±‡", key="apply_simple_dict"):
                if custom_dict_words:
                    words = [w.strip() for w in custom_dict_words.strip().split('\n') if w.strip()]
                    for word in words:
                        jieba.add_word(word)
                    st.success(f"å·²æ·»åŠ  {len(words)} ä¸ªä¸“ä¸šè¯æ±‡åˆ°åˆ†è¯å™¨")
                    log_message(f"å·²æ·»åŠ  {len(words)} ä¸ªä¸“ä¸šè¯æ±‡", level="success")
    
    # ========== é¢„å¤„ç†å‚æ•°è®¾ç½® ==========
    with st.expander("é¢„å¤„ç†å‚æ•°è®¾ç½®", expanded=True):
        col1, col2 = st.columns(2)
        
        with col1:
            st.session_state.min_word_length = st.slider(
                "æœ€å°è¯é•¿åº¦", 
                min_value=1, 
                max_value=5, 
                value=st.session_state.min_word_length,
                help="è¿‡æ»¤æ‰çŸ­äºæ­¤é•¿åº¦çš„è¯",
                key="min_word_length_slider"
            )
            
            st.session_state.remove_policy_words = st.checkbox(
                "ç§»é™¤æ”¿ç­–ç‰¹å®šåœç”¨è¯", 
                value=st.session_state.remove_policy_words,
                help="ç§»é™¤å¸¸è§æ”¿ç­–æ–‡ä»¶ä¸­çš„æ— æ„ä¹‰è¯è¯­",
                key="remove_policy_words_checkbox"
            )
        
        with col2:
            st.session_state.no_below = st.slider(
                "æœ€å°æ–‡æ¡£é¢‘ç‡", 
                min_value=1, 
                max_value=10, 
                value=st.session_state.no_below,
                help="è¯è¯­è‡³å°‘åœ¨å¤šå°‘æ–‡æ¡£ä¸­å‡ºç°",
                key="no_below_slider"
            )
            
            st.session_state.no_above = st.slider(
                "æœ€å¤§æ–‡æ¡£é¢‘ç‡", 
                min_value=0.1, 
                max_value=1.0, 
                value=st.session_state.no_above,
                step=0.05,
                help="è¯è¯­æœ€å¤šåœ¨å¤šå°‘æ¯”ä¾‹çš„æ–‡æ¡£ä¸­å‡ºç°",
                key="no_above_slider"
            )
            
            st.session_state.min_word_count = st.slider(
                "æœ€å°è¯é¢‘", 
                min_value=1, 
                max_value=10, 
                value=st.session_state.min_word_count,
                help="è¯è¯­åœ¨æ•´ä¸ªè¯­æ–™åº“ä¸­çš„æœ€å°å‡ºç°æ¬¡æ•°",
                key="min_word_count_slider"
            )
    
    # åœç”¨è¯ç®¡ç†
    with st.expander("åœç”¨è¯ç®¡ç†", expanded=True):
        # æ·»åŠ ä¸€ä¸ªå¤é€‰æ¡†é€‰æ‹©æ˜¯å¦ä½¿ç”¨é»˜è®¤çš„stopwords.txtæ–‡ä»¶
        st.session_state.use_default_stopwords_file = st.checkbox(
            "ä½¿ç”¨é»˜è®¤çš„stopwords.txtæ–‡ä»¶ä½œä¸ºåœç”¨è¯", 
            value=st.session_state.use_default_stopwords_file,
            help="é€‰ä¸­æ—¶å°†ä¼˜å…ˆä½¿ç”¨é¡¹ç›®æ ¹ç›®å½•ä¸‹çš„stopwords.txtæ–‡ä»¶ä½œä¸ºåœç”¨è¯",
            key="use_default_stopwords_file_checkbox"
        )
        
        # è·å–é»˜è®¤åœç”¨è¯
        default_stopwords = load_default_stopwords() if st.session_state.use_default_stopwords_file else set()
        
        # æ˜¾ç¤ºå½“å‰åœç”¨è¯ç»Ÿè®¡
        current_stopwords = set()
        if default_stopwords:
            current_stopwords.update(default_stopwords)
            st.info(f"å·²åŠ è½½é»˜è®¤åœç”¨è¯æ–‡ä»¶(stopwords.txt)ï¼ŒåŒ…å« {len(default_stopwords)} ä¸ªåœç”¨è¯")
        else:
            current_stopwords.update(DEFAULT_COMMON_STOPWORDS)
            if st.session_state.remove_policy_words:
                current_stopwords.update(DEFAULT_POLICY_STOPWORDS)
        
        # æ·»åŠ è‡ªå®šä¹‰åœç”¨è¯
        current_stopwords.update(st.session_state.custom_stopwords)
        
        st.write(f"å½“å‰åœç”¨è¯æ€»æ•°é‡: {len(current_stopwords)}")
        
        # åœç”¨è¯ç®¡ç†é€‰é¡¹
        tabs = st.tabs(["æ·»åŠ åœç”¨è¯", "ä¸Šä¼ åœç”¨è¯æ–‡ä»¶", "æŸ¥çœ‹å’Œç¼–è¾‘"])
        
        # æ·»åŠ åœç”¨è¯æ ‡ç­¾é¡µ
        with tabs[0]:
            new_stopwords = st.text_area(
                "è¾“å…¥åœç”¨è¯(æ¯è¡Œä¸€ä¸ª)", 
                height=150,
                help="æ·»åŠ è‡ªå®šä¹‰åœç”¨è¯ï¼Œæ¯è¡Œè¾“å…¥ä¸€ä¸ªè¯",
                key="new_stopwords_textarea"
            )
            
            # ä½¿ç”¨ä¸åŒçš„æ–¹å¼å¤„ç†æŒ‰é’®ç‚¹å‡»ï¼Œé¿å…ä¼šè¯çŠ¶æ€å†²çª
            add_stopwords_clicked = st.button("æ·»åŠ åœç”¨è¯", key="add_stopwords_button")
            if add_stopwords_clicked and new_stopwords:
                words = new_stopwords.strip().split('\n')
                words = [word.strip() for word in words if word.strip()]
                if words:
                    st.session_state.custom_stopwords.update(words)
                    st.success(f"å·²æ·»åŠ  {len(words)} ä¸ªåœç”¨è¯")
                    log_message(f"å·²æ·»åŠ  {len(words)} ä¸ªåœç”¨è¯", level="success")
        
        # ä¸Šä¼ åœç”¨è¯æ–‡ä»¶æ ‡ç­¾é¡µ
        with tabs[1]:
            uploaded_stopwords = st.file_uploader(
                "ä¸Šä¼ åœç”¨è¯æ–‡ä»¶", 
                type=["txt"], 
                help="ä¸Šä¼ åŒ…å«åœç”¨è¯çš„TXTæ–‡ä»¶ï¼Œæ¯è¡Œä¸€ä¸ªè¯",
                key="stopwords_file_uploader"
            )
            
            if uploaded_stopwords is not None:
                load_stopwords_clicked = st.button("ä»æ–‡ä»¶åŠ è½½åœç”¨è¯", key="load_stopwords_button")
                if load_stopwords_clicked:
                    new_words = load_stopwords_from_file(uploaded_stopwords)
                    if new_words:
                        st.session_state.custom_stopwords.update(new_words)
                        st.success(f"å·²ä»æ–‡ä»¶åŠ è½½ {len(new_words)} ä¸ªåœç”¨è¯")
                        log_message(f"å·²ä»æ–‡ä»¶åŠ è½½ {len(new_words)} ä¸ªåœç”¨è¯", level="success")
        
        # æŸ¥çœ‹å’Œç¼–è¾‘æ ‡ç­¾é¡µ
        with tabs[2]:
            if current_stopwords:
                # å°†åœç”¨è¯è½¬æ¢ä¸ºDataFrameä»¥ä¾¿æŸ¥çœ‹
                stopwords_df = pd.DataFrame({
                    "åœç”¨è¯": sorted(current_stopwords)
                })
                
                st.dataframe(stopwords_df, use_container_width=True, height=300)
                
                # ä¿å­˜åœç”¨è¯
                save_stopwords_clicked = st.button("ä¿å­˜åœç”¨è¯åˆ°æ–‡ä»¶", key="save_stopwords_button")
                if save_stopwords_clicked:
                    filepath = save_stopwords_to_file(current_stopwords)
                    st.success(f"åœç”¨è¯å·²ä¿å­˜åˆ°: {filepath}")
                    log_message(f"åœç”¨è¯å·²ä¿å­˜åˆ°: {filepath}", level="success")
                
                # æ¸…ç©ºè‡ªå®šä¹‰åœç”¨è¯
                clear_stopwords_clicked = st.button("æ¸…ç©ºè‡ªå®šä¹‰åœç”¨è¯", key="clear_custom_stopwords_button")
                if clear_stopwords_clicked:
                    st.session_state.custom_stopwords = set()
                    st.success("å·²æ¸…ç©ºè‡ªå®šä¹‰åœç”¨è¯")
                    log_message("å·²æ¸…ç©ºè‡ªå®šä¹‰åœç”¨è¯", level="success")
    
    # å¼€å§‹é¢„å¤„ç†æŒ‰é’®
    if st.button("å¼€å§‹æ–‡æœ¬é¢„å¤„ç†", key="start_preprocessing_button"):
        with st.spinner("æ­£åœ¨è¿›è¡Œæ–‡æœ¬é¢„å¤„ç†..."):
            start_time = time.time()
            
            # åˆ›å»ºé¢„å¤„ç†å™¨å®ä¾‹
            preprocessor = TextPreprocessor()
            
            # å¤„ç†æ–‡æœ¬
            tokenized_texts = preprocessor.preprocess_texts(
                st.session_state.raw_texts, 
                st.session_state.file_names
            )
            
            # åˆ›å»ºè¯å…¸å’Œè¯­æ–™åº“
            dictionary, corpus = preprocessor.create_dictionary_and_corpus(tokenized_texts)
            
            # ä¿å­˜åˆ°ä¼šè¯çŠ¶æ€
            st.session_state.texts = tokenized_texts
            st.session_state.dictionary = dictionary
            st.session_state.corpus = corpus
            
            # è®¡ç®—å¹¶æ˜¾ç¤ºè€—æ—¶
            elapsed_time = time.time() - start_time
            log_message(f"é¢„å¤„ç†å®Œæˆï¼Œè€—æ—¶: {elapsed_time:.2f}ç§’", level="success")
            
            # æ˜¾ç¤ºæˆåŠŸæ¶ˆæ¯
            st.success(f"æ–‡æœ¬é¢„å¤„ç†å®Œæˆï¼Œè€—æ—¶: {elapsed_time:.2f}ç§’")
    
    # å¦‚æœå·²ç»å®Œæˆé¢„å¤„ç†ï¼Œæ˜¾ç¤ºç»“æœ
    if st.session_state.texts and st.session_state.dictionary and st.session_state.corpus:
        st.subheader("é¢„å¤„ç†ç»“æœ")
        
        # æ˜¾ç¤ºåŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
        col1, col2, col3 = st.columns(3)
        col1.metric("æ–‡æ¡£æ•°é‡", len(st.session_state.texts))
        col2.metric("è¯å…¸å¤§å°", len(st.session_state.dictionary))
        col3.metric("å¹³å‡æ–‡æ¡£é•¿åº¦", f"{sum(len(text) for text in st.session_state.texts) / len(st.session_state.texts):.1f}è¯")
        
        # è¯é¢‘ç»Ÿè®¡
        with st.expander("è¯é¢‘ç»Ÿè®¡", expanded=False):
            # è®¡ç®—è¯é¢‘
            word_counts = Counter()
            for text in st.session_state.texts:
                word_counts.update(text)
            
            # è·å–å‰50ä¸ªé«˜é¢‘è¯
            top_words = word_counts.most_common(50)
            
            # è½¬æ¢ä¸ºDataFrame
            df_word_counts = pd.DataFrame(top_words, columns=["è¯è¯­", "é¢‘æ¬¡"])
            
            # æ˜¾ç¤ºè¯é¢‘è¡¨æ ¼
            st.dataframe(df_word_counts, use_container_width=True)
            
            # è¯é¢‘ç›´æ–¹å›¾
            if len(top_words) > 0:
                st.bar_chart(df_word_counts.set_index("è¯è¯­"))
        
        # æ–‡æ¡£é•¿åº¦åˆ†å¸ƒ
        with st.expander("æ–‡æ¡£é•¿åº¦åˆ†å¸ƒ", expanded=False):
            doc_lengths = [len(text) for text in st.session_state.texts]
            
            # åˆ›å»ºDataFrame
            df_lengths = pd.DataFrame({
                "æ–‡ä»¶å": st.session_state.file_names[:len(doc_lengths)],
                "è¯è¯­æ•°é‡": doc_lengths
            })
            
            # æ˜¾ç¤ºæ–‡æ¡£é•¿åº¦è¡¨æ ¼
            st.dataframe(df_lengths.sort_values("è¯è¯­æ•°é‡", ascending=False), use_container_width=True)
            
            # æ–‡æ¡£é•¿åº¦ç›´æ–¹å›¾
            st.bar_chart(df_lengths.set_index("æ–‡ä»¶å"))
        
        # é¢„å¤„ç†æ–‡æœ¬é¢„è§ˆ
        with st.expander("é¢„å¤„ç†æ–‡æœ¬é¢„è§ˆ", expanded=False):
            preview_idx = st.selectbox(
                "é€‰æ‹©æ–‡æ¡£é¢„è§ˆ", 
                range(len(st.session_state.texts)),
                format_func=lambda i: st.session_state.file_names[i] if i < len(st.session_state.file_names) else f"æ–‡æ¡£ {i+1}",
                key="text_preview_select"
            )
            
            if preview_idx is not None:
                st.write(f"**åˆ†è¯ç»“æœ** (å…± {len(st.session_state.texts[preview_idx])} ä¸ªè¯):")
                st.write(" ".join(st.session_state.texts[preview_idx])) 