# -*- coding: utf-8 -*-
"""
æ—¶åºæ¼”å˜åˆ†ææ¨¡å— (Temporal Evolution Analysis Module)
=====================================================

æœ¬æ¨¡å—æä¾›æ”¿ç­–æ–‡æœ¬çš„æ—¶åºæ¼”å˜åˆ†æåŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š
- æ–‡æ¡£æ—¶é—´æ ‡ç­¾ç®¡ç†
- å…³é”®è¯æ—¶åºè¶‹åŠ¿åˆ†æ
- ä¸»é¢˜æ¼”å˜åˆ†æ
- æ—¶åºæ•°æ®å¯è§†åŒ–
- ç»“æœå¯¼å‡º

Requirements: 4.1, 4.2, 4.3, 4.4, 4.5, 4.6, 4.7
"""

from typing import List, Dict, Tuple, Optional, Any
from collections import Counter, defaultdict
from dataclasses import dataclass, field
import pandas as pd
import numpy as np
import re
from datetime import datetime


@dataclass
class TimeLabel:
    """æ—¶é—´æ ‡ç­¾æ•°æ®ç±»"""
    doc_name: str
    time_label: str  # æ—¶é—´æ ‡ç­¾ï¼ˆå¹´ä»½æˆ–æ—¥æœŸå­—ç¬¦ä¸²ï¼‰
    sort_key: str = ""  # ç”¨äºæ’åºçš„æ ‡å‡†åŒ–é”®
    
    def __post_init__(self):
        """åˆå§‹åŒ–åå¤„ç†ï¼Œç”Ÿæˆæ’åºé”®"""
        if not self.sort_key:
            self.sort_key = self._normalize_time_label(self.time_label)
    
    @staticmethod
    def _normalize_time_label(label: str) -> str:
        """
        æ ‡å‡†åŒ–æ—¶é—´æ ‡ç­¾ç”¨äºæ’åº
        
        æ”¯æŒçš„æ ¼å¼ï¼š
        - å¹´ä»½: "2020", "2021å¹´"
        - å¹´æœˆ: "2020-01", "2020å¹´1æœˆ"
        - å®Œæ•´æ—¥æœŸ: "2020-01-15", "2020å¹´1æœˆ15æ—¥"
        """
        if not label:
            return "9999"  # æ— æ•ˆæ ‡ç­¾æ’åˆ°æœ€å
        
        # ç§»é™¤ä¸­æ–‡å­—ç¬¦ï¼Œæå–æ•°å­—
        cleaned = re.sub(r'[å¹´æœˆæ—¥]', '-', label)
        cleaned = re.sub(r'-+', '-', cleaned).strip('-')
        
        # å°è¯•è§£æä¸åŒæ ¼å¼
        parts = cleaned.split('-')
        
        # è¡¥é½ä¸ºæ ‡å‡†æ ¼å¼ YYYY-MM-DD
        if len(parts) == 1:
            # åªæœ‰å¹´ä»½
            return f"{parts[0]:0>4}-00-00"
        elif len(parts) == 2:
            # å¹´æœˆ
            return f"{parts[0]:0>4}-{parts[1]:0>2}-00"
        elif len(parts) >= 3:
            # å®Œæ•´æ—¥æœŸ
            return f"{parts[0]:0>4}-{parts[1]:0>2}-{parts[2]:0>2}"
        
        return label


class TemporalAnalyzer:
    """
    æ—¶åºåˆ†æå™¨ - åˆ†ææ–‡æœ¬éšæ—¶é—´çš„æ¼”å˜è¶‹åŠ¿
    
    Attributes:
        texts: åˆ†è¯åçš„æ–‡æœ¬åˆ—è¡¨ï¼ˆæ¯ä¸ªæ–‡æœ¬æ˜¯è¯è¯­åˆ—è¡¨ï¼‰
        file_names: æ–‡æ¡£åç§°åˆ—è¡¨
        time_labels: æ–‡æ¡£æ—¶é—´æ ‡ç­¾æ˜ å°„
    
    Requirements: 4.1, 4.2, 4.3, 4.5
    """
    
    def __init__(self, texts: List[List[str]], file_names: List[str]):
        """
        åˆå§‹åŒ–æ—¶åºåˆ†æå™¨
        
        Args:
            texts: åˆ†è¯åçš„æ–‡æœ¬åˆ—è¡¨
            file_names: æ–‡æ¡£åç§°åˆ—è¡¨
        """
        self.texts = texts if texts else []
        self.file_names = file_names if file_names else []
        self.time_labels: Dict[str, TimeLabel] = {}
        self._sorted_periods: Optional[List[str]] = None
    
    def set_time_label(self, doc_name: str, time_label: str) -> bool:
        """
        ä¸ºæ–‡æ¡£è®¾ç½®æ—¶é—´æ ‡ç­¾
        
        Args:
            doc_name: æ–‡æ¡£åç§°
            time_label: æ—¶é—´æ ‡ç­¾ï¼ˆå¹´ä»½æˆ–æ—¥æœŸï¼‰
        
        Returns:
            bool: æ˜¯å¦è®¾ç½®æˆåŠŸ
        
        Requirements: 4.1
        """
        if doc_name not in self.file_names:
            return False
        
        if not time_label or not time_label.strip():
            # ç§»é™¤ç©ºæ ‡ç­¾
            if doc_name in self.time_labels:
                del self.time_labels[doc_name]
            return True
        
        self.time_labels[doc_name] = TimeLabel(doc_name, time_label.strip())
        self._sorted_periods = None  # æ¸…é™¤ç¼“å­˜
        return True
    
    def set_time_labels_batch(self, labels: Dict[str, str]) -> int:
        """
        æ‰¹é‡è®¾ç½®æ—¶é—´æ ‡ç­¾
        
        Args:
            labels: æ–‡æ¡£ååˆ°æ—¶é—´æ ‡ç­¾çš„æ˜ å°„
        
        Returns:
            int: æˆåŠŸè®¾ç½®çš„æ•°é‡
        """
        count = 0
        for doc_name, time_label in labels.items():
            if self.set_time_label(doc_name, time_label):
                count += 1
        return count
    
    def get_time_label(self, doc_name: str) -> Optional[str]:
        """
        è·å–æ–‡æ¡£çš„æ—¶é—´æ ‡ç­¾
        
        Args:
            doc_name: æ–‡æ¡£åç§°
        
        Returns:
            Optional[str]: æ—¶é—´æ ‡ç­¾ï¼Œå¦‚æœæœªè®¾ç½®åˆ™è¿”å›None
        """
        label = self.time_labels.get(doc_name)
        return label.time_label if label else None
    
    def get_all_time_labels(self) -> Dict[str, str]:
        """
        è·å–æ‰€æœ‰æ–‡æ¡£çš„æ—¶é—´æ ‡ç­¾
        
        Returns:
            Dict[str, str]: æ–‡æ¡£ååˆ°æ—¶é—´æ ‡ç­¾çš„æ˜ å°„
        """
        return {name: label.time_label for name, label in self.time_labels.items()}
    
    def get_labeled_documents_count(self) -> int:
        """è·å–å·²æ ‡æ³¨æ—¶é—´æ ‡ç­¾çš„æ–‡æ¡£æ•°é‡"""
        return len(self.time_labels)
    
    def get_unlabeled_documents(self) -> List[str]:
        """è·å–æœªæ ‡æ³¨æ—¶é—´æ ‡ç­¾çš„æ–‡æ¡£åˆ—è¡¨"""
        return [name for name in self.file_names if name not in self.time_labels]
    
    def get_sorted_periods(self) -> List[str]:
        """
        è·å–æŒ‰æ—¶é—´æ’åºçš„æ—¶é—´æ®µåˆ—è¡¨
        
        Returns:
            List[str]: æ’åºåçš„æ—¶é—´æ ‡ç­¾åˆ—è¡¨ï¼ˆå»é‡ï¼‰
        
        Requirements: 4.2
        """
        if self._sorted_periods is not None:
            return self._sorted_periods
        
        # æ”¶é›†æ‰€æœ‰å”¯ä¸€çš„æ—¶é—´æ ‡ç­¾
        unique_labels = {}
        for label in self.time_labels.values():
            if label.time_label not in unique_labels:
                unique_labels[label.time_label] = label.sort_key
        
        # æŒ‰æ’åºé”®æ’åº
        sorted_labels = sorted(unique_labels.items(), key=lambda x: x[1])
        self._sorted_periods = [label for label, _ in sorted_labels]
        
        return self._sorted_periods
    
    def get_documents_by_period(self, period: str) -> List[str]:
        """
        è·å–æŒ‡å®šæ—¶é—´æ®µçš„æ–‡æ¡£åˆ—è¡¨
        
        Args:
            period: æ—¶é—´æ ‡ç­¾
        
        Returns:
            List[str]: è¯¥æ—¶é—´æ®µçš„æ–‡æ¡£åç§°åˆ—è¡¨
        
        Requirements: 4.2
        """
        return [
            name for name, label in self.time_labels.items()
            if label.time_label == period
        ]
    
    def get_documents_sorted_by_time(self) -> List[Tuple[str, str]]:
        """
        è·å–æŒ‰æ—¶é—´æ’åºçš„æ–‡æ¡£åˆ—è¡¨
        
        Returns:
            List[Tuple[str, str]]: (æ–‡æ¡£å, æ—¶é—´æ ‡ç­¾) åˆ—è¡¨ï¼ŒæŒ‰æ—¶é—´å‡åºæ’åˆ—
        
        Requirements: 4.2
        """
        labeled_docs = [
            (name, label.time_label, label.sort_key)
            for name, label in self.time_labels.items()
        ]
        
        # æŒ‰æ’åºé”®æ’åº
        labeled_docs.sort(key=lambda x: x[2])
        
        return [(name, time_label) for name, time_label, _ in labeled_docs]
    
    def _get_doc_index(self, doc_name: str) -> int:
        """è·å–æ–‡æ¡£åœ¨åˆ—è¡¨ä¸­çš„ç´¢å¼•"""
        try:
            return self.file_names.index(doc_name)
        except ValueError:
            return -1
    
    def analyze_keyword_trend(self, keyword: str) -> Dict[str, int]:
        """
        åˆ†æå•ä¸ªå…³é”®è¯åœ¨ä¸åŒæ—¶é—´æ®µçš„é¢‘ç‡å˜åŒ–
        
        Args:
            keyword: è¦åˆ†æçš„å…³é”®è¯
        
        Returns:
            Dict[str, int]: æ—¶é—´æ®µ -> é¢‘ç‡ çš„æ˜ å°„ï¼ŒæŒ‰æ—¶é—´æ’åº
        
        Requirements: 4.3
        """
        if not self.time_labels:
            return {}
        
        # æŒ‰æ—¶é—´æ®µç»Ÿè®¡å…³é”®è¯é¢‘ç‡
        period_freq = defaultdict(int)
        
        for doc_name, label in self.time_labels.items():
            doc_idx = self._get_doc_index(doc_name)
            if doc_idx >= 0 and doc_idx < len(self.texts):
                # ç»Ÿè®¡è¯¥æ–‡æ¡£ä¸­å…³é”®è¯å‡ºç°æ¬¡æ•°
                count = self.texts[doc_idx].count(keyword)
                period_freq[label.time_label] += count
        
        # æŒ‰æ—¶é—´æ’åº
        sorted_periods = self.get_sorted_periods()
        result = {}
        for period in sorted_periods:
            result[period] = period_freq.get(period, 0)
        
        return result
    
    def analyze_keywords_trends(self, keywords: List[str]) -> Dict[str, Dict[str, int]]:
        """
        åˆ†æå¤šä¸ªå…³é”®è¯çš„æ—¶åºè¶‹åŠ¿
        
        Args:
            keywords: å…³é”®è¯åˆ—è¡¨
        
        Returns:
            Dict[str, Dict[str, int]]: å…³é”®è¯ -> (æ—¶é—´æ®µ -> é¢‘ç‡) çš„æ˜ å°„
        
        Requirements: 4.3
        """
        return {keyword: self.analyze_keyword_trend(keyword) for keyword in keywords}
    
    def get_period_word_frequency(self, period: str, top_n: int = 20) -> List[Tuple[str, int]]:
        """
        è·å–æŒ‡å®šæ—¶é—´æ®µçš„é«˜é¢‘è¯
        
        Args:
            period: æ—¶é—´æ ‡ç­¾
            top_n: è¿”å›çš„è¯è¯­æ•°é‡
        
        Returns:
            List[Tuple[str, int]]: (è¯è¯­, é¢‘ç‡) åˆ—è¡¨
        """
        docs = self.get_documents_by_period(period)
        
        word_counter = Counter()
        for doc_name in docs:
            doc_idx = self._get_doc_index(doc_name)
            if doc_idx >= 0 and doc_idx < len(self.texts):
                word_counter.update(self.texts[doc_idx])
        
        return word_counter.most_common(top_n)
    
    def analyze_topic_evolution(self, doc_topic_dist: np.ndarray) -> Dict[str, List[float]]:
        """
        åˆ†æä¸»é¢˜åœ¨ä¸åŒæ—¶é—´æ®µçš„åˆ†å¸ƒå˜åŒ–
        
        Args:
            doc_topic_dist: æ–‡æ¡£-ä¸»é¢˜åˆ†å¸ƒçŸ©é˜µï¼Œå½¢çŠ¶ä¸º (n_docs, n_topics)
        
        Returns:
            Dict[str, List[float]]: æ—¶é—´æ®µ -> å„ä¸»é¢˜å¹³å‡æ¦‚ç‡åˆ—è¡¨
        
        Requirements: 4.5
        """
        if doc_topic_dist is None or len(doc_topic_dist) == 0:
            return {}
        
        if not self.time_labels:
            return {}
        
        n_topics = doc_topic_dist.shape[1] if len(doc_topic_dist.shape) > 1 else 1
        sorted_periods = self.get_sorted_periods()
        
        result = {}
        for period in sorted_periods:
            docs = self.get_documents_by_period(period)
            
            if not docs:
                result[period] = [0.0] * n_topics
                continue
            
            # æ”¶é›†è¯¥æ—¶é—´æ®µæ‰€æœ‰æ–‡æ¡£çš„ä¸»é¢˜åˆ†å¸ƒ
            topic_sums = np.zeros(n_topics)
            doc_count = 0
            
            for doc_name in docs:
                doc_idx = self._get_doc_index(doc_name)
                if doc_idx >= 0 and doc_idx < len(doc_topic_dist):
                    topic_sums += doc_topic_dist[doc_idx]
                    doc_count += 1
            
            # è®¡ç®—å¹³å‡å€¼
            if doc_count > 0:
                result[period] = (topic_sums / doc_count).tolist()
            else:
                result[period] = [0.0] * n_topics
        
        return result
    
    def get_emerging_keywords(self, recent_periods: int = 2, top_n: int = 10) -> List[Tuple[str, float]]:
        """
        è¯†åˆ«æ–°å…´å…³é”®è¯ï¼ˆåœ¨æœ€è¿‘æ—¶é—´æ®µé¢‘ç‡æ˜¾è‘—å¢åŠ çš„è¯ï¼‰
        
        Args:
            recent_periods: æœ€è¿‘çš„æ—¶é—´æ®µæ•°é‡
            top_n: è¿”å›çš„å…³é”®è¯æ•°é‡
        
        Returns:
            List[Tuple[str, float]]: (å…³é”®è¯, å¢é•¿ç‡) åˆ—è¡¨
        """
        sorted_periods = self.get_sorted_periods()
        
        if len(sorted_periods) < 2:
            return []
        
        # åˆ†å‰²ä¸ºæ—©æœŸå’Œè¿‘æœŸ
        split_point = max(1, len(sorted_periods) - recent_periods)
        early_periods = sorted_periods[:split_point]
        recent_periods_list = sorted_periods[split_point:]
        
        # ç»Ÿè®¡æ—©æœŸè¯é¢‘
        early_freq = Counter()
        early_doc_count = 0
        for period in early_periods:
            docs = self.get_documents_by_period(period)
            early_doc_count += len(docs)
            for doc_name in docs:
                doc_idx = self._get_doc_index(doc_name)
                if doc_idx >= 0 and doc_idx < len(self.texts):
                    early_freq.update(self.texts[doc_idx])
        
        # ç»Ÿè®¡è¿‘æœŸè¯é¢‘
        recent_freq = Counter()
        recent_doc_count = 0
        for period in recent_periods_list:
            docs = self.get_documents_by_period(period)
            recent_doc_count += len(docs)
            for doc_name in docs:
                doc_idx = self._get_doc_index(doc_name)
                if doc_idx >= 0 and doc_idx < len(self.texts):
                    recent_freq.update(self.texts[doc_idx])
        
        if early_doc_count == 0 or recent_doc_count == 0:
            return []
        
        # è®¡ç®—å¢é•¿ç‡
        growth_rates = []
        for word, recent_count in recent_freq.items():
            early_count = early_freq.get(word, 0)
            
            # å½’ä¸€åŒ–é¢‘ç‡
            early_rate = early_count / early_doc_count if early_doc_count > 0 else 0
            recent_rate = recent_count / recent_doc_count if recent_doc_count > 0 else 0
            
            # è®¡ç®—å¢é•¿ç‡ï¼ˆé¿å…é™¤é›¶ï¼‰
            if early_rate > 0:
                growth = (recent_rate - early_rate) / early_rate
            elif recent_rate > 0:
                growth = float('inf')  # æ–°å‡ºç°çš„è¯
            else:
                growth = 0
            
            if growth > 0 and recent_count >= 2:  # åªä¿ç•™å¢é•¿çš„è¯
                growth_rates.append((word, growth))
        
        # æŒ‰å¢é•¿ç‡æ’åº
        growth_rates.sort(key=lambda x: -x[1])
        
        # å¤„ç†æ— ç©·å¤§å€¼
        result = []
        for word, rate in growth_rates[:top_n]:
            if rate == float('inf'):
                result.append((word, 999.99))  # ç”¨å¤§æ•°è¡¨ç¤ºæ–°è¯
            else:
                result.append((word, round(rate * 100, 2)))  # è½¬ä¸ºç™¾åˆ†æ¯”
        
        return result
    
    def get_declining_keywords(self, recent_periods: int = 2, top_n: int = 10) -> List[Tuple[str, float]]:
        """
        è¯†åˆ«è¡°é€€å…³é”®è¯ï¼ˆåœ¨æœ€è¿‘æ—¶é—´æ®µé¢‘ç‡æ˜¾è‘—ä¸‹é™çš„è¯ï¼‰
        
        Args:
            recent_periods: æœ€è¿‘çš„æ—¶é—´æ®µæ•°é‡
            top_n: è¿”å›çš„å…³é”®è¯æ•°é‡
        
        Returns:
            List[Tuple[str, float]]: (å…³é”®è¯, ä¸‹é™ç‡) åˆ—è¡¨
        """
        sorted_periods = self.get_sorted_periods()
        
        if len(sorted_periods) < 2:
            return []
        
        # åˆ†å‰²ä¸ºæ—©æœŸå’Œè¿‘æœŸ
        split_point = max(1, len(sorted_periods) - recent_periods)
        early_periods = sorted_periods[:split_point]
        recent_periods_list = sorted_periods[split_point:]
        
        # ç»Ÿè®¡æ—©æœŸè¯é¢‘
        early_freq = Counter()
        early_doc_count = 0
        for period in early_periods:
            docs = self.get_documents_by_period(period)
            early_doc_count += len(docs)
            for doc_name in docs:
                doc_idx = self._get_doc_index(doc_name)
                if doc_idx >= 0 and doc_idx < len(self.texts):
                    early_freq.update(self.texts[doc_idx])
        
        # ç»Ÿè®¡è¿‘æœŸè¯é¢‘
        recent_freq = Counter()
        recent_doc_count = 0
        for period in recent_periods_list:
            docs = self.get_documents_by_period(period)
            recent_doc_count += len(docs)
            for doc_name in docs:
                doc_idx = self._get_doc_index(doc_name)
                if doc_idx >= 0 and doc_idx < len(self.texts):
                    recent_freq.update(self.texts[doc_idx])
        
        if early_doc_count == 0 or recent_doc_count == 0:
            return []
        
        # è®¡ç®—ä¸‹é™ç‡
        decline_rates = []
        for word, early_count in early_freq.items():
            recent_count = recent_freq.get(word, 0)
            
            # å½’ä¸€åŒ–é¢‘ç‡
            early_rate = early_count / early_doc_count if early_doc_count > 0 else 0
            recent_rate = recent_count / recent_doc_count if recent_doc_count > 0 else 0
            
            # è®¡ç®—ä¸‹é™ç‡
            if early_rate > 0:
                decline = (early_rate - recent_rate) / early_rate
            else:
                decline = 0
            
            if decline > 0 and early_count >= 2:  # åªä¿ç•™ä¸‹é™çš„è¯
                decline_rates.append((word, decline))
        
        # æŒ‰ä¸‹é™ç‡æ’åº
        decline_rates.sort(key=lambda x: -x[1])
        
        return [(word, round(rate * 100, 2)) for word, rate in decline_rates[:top_n]]
    
    def export_trend_data(self, keywords: Optional[List[str]] = None) -> str:
        """
        å¯¼å‡ºå…³é”®è¯è¶‹åŠ¿æ•°æ®ä¸ºCSVæ ¼å¼
        
        Args:
            keywords: è¦å¯¼å‡ºçš„å…³é”®è¯åˆ—è¡¨ï¼Œå¦‚æœä¸ºNoneåˆ™å¯¼å‡ºæ‰€æœ‰é«˜é¢‘è¯
        
        Returns:
            str: CSVæ ¼å¼å­—ç¬¦ä¸²
        
        Requirements: 4.7
        """
        if not self.time_labels:
            return ""
        
        sorted_periods = self.get_sorted_periods()
        
        # å¦‚æœæ²¡æœ‰æŒ‡å®šå…³é”®è¯ï¼Œè·å–æ‰€æœ‰æ—¶é—´æ®µçš„é«˜é¢‘è¯
        if keywords is None:
            all_words = Counter()
            for doc_idx, text in enumerate(self.texts):
                if self.file_names[doc_idx] in self.time_labels:
                    all_words.update(text)
            keywords = [word for word, _ in all_words.most_common(50)]
        
        # æ„å»ºæ•°æ®
        data = []
        for keyword in keywords:
            trend = self.analyze_keyword_trend(keyword)
            row = {"å…³é”®è¯": keyword}
            for period in sorted_periods:
                row[period] = trend.get(period, 0)
            data.append(row)
        
        df = pd.DataFrame(data)
        return df.to_csv(index=False, encoding='utf-8-sig')
    
    def export_topic_evolution_data(self, doc_topic_dist: np.ndarray, topic_names: Optional[List[str]] = None) -> str:
        """
        å¯¼å‡ºä¸»é¢˜æ¼”å˜æ•°æ®ä¸ºCSVæ ¼å¼
        
        Args:
            doc_topic_dist: æ–‡æ¡£-ä¸»é¢˜åˆ†å¸ƒçŸ©é˜µ
            topic_names: ä¸»é¢˜åç§°åˆ—è¡¨
        
        Returns:
            str: CSVæ ¼å¼å­—ç¬¦ä¸²
        
        Requirements: 4.7
        """
        evolution = self.analyze_topic_evolution(doc_topic_dist)
        
        if not evolution:
            return ""
        
        n_topics = len(list(evolution.values())[0])
        
        if topic_names is None:
            topic_names = [f"ä¸»é¢˜{i+1}" for i in range(n_topics)]
        
        # æ„å»ºæ•°æ®
        data = []
        for period, topic_probs in evolution.items():
            row = {"æ—¶é—´æ®µ": period}
            for i, prob in enumerate(topic_probs):
                topic_name = topic_names[i] if i < len(topic_names) else f"ä¸»é¢˜{i+1}"
                row[topic_name] = round(prob, 4)
            data.append(row)
        
        df = pd.DataFrame(data)
        return df.to_csv(index=False, encoding='utf-8-sig')
    
    def export_time_labels(self) -> str:
        """
        å¯¼å‡ºæ—¶é—´æ ‡ç­¾æ•°æ®ä¸ºCSVæ ¼å¼
        
        Returns:
            str: CSVæ ¼å¼å­—ç¬¦ä¸²
        """
        data = []
        for doc_name in self.file_names:
            label = self.time_labels.get(doc_name)
            data.append({
                "æ–‡æ¡£å": doc_name,
                "æ—¶é—´æ ‡ç­¾": label.time_label if label else ""
            })
        
        df = pd.DataFrame(data)
        return df.to_csv(index=False, encoding='utf-8-sig')
    
    def import_time_labels(self, csv_content: str) -> int:
        """
        ä»CSVå¯¼å…¥æ—¶é—´æ ‡ç­¾
        
        Args:
            csv_content: CSVæ ¼å¼å­—ç¬¦ä¸²
        
        Returns:
            int: æˆåŠŸå¯¼å…¥çš„æ•°é‡
        """
        try:
            from io import StringIO
            df = pd.read_csv(StringIO(csv_content))
            
            count = 0
            for _, row in df.iterrows():
                doc_name = str(row.get("æ–‡æ¡£å", ""))
                time_label = str(row.get("æ—¶é—´æ ‡ç­¾", ""))
                
                if doc_name and time_label and self.set_time_label(doc_name, time_label):
                    count += 1
            
            return count
        except Exception:
            return 0
    
    def auto_extract_time_from_filename(self, pattern: str = r'(\d{4})') -> int:
        """
        ä»æ–‡ä»¶åè‡ªåŠ¨æå–æ—¶é—´æ ‡ç­¾
        
        Args:
            pattern: æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼ï¼Œé»˜è®¤æå–4ä½æ•°å­—ï¼ˆå¹´ä»½ï¼‰
        
        Returns:
            int: æˆåŠŸæå–çš„æ•°é‡
        """
        count = 0
        regex = re.compile(pattern)
        
        for doc_name in self.file_names:
            match = regex.search(doc_name)
            if match:
                time_label = match.group(1)
                if self.set_time_label(doc_name, time_label):
                    count += 1
        
        return count



# ============================================================================
# Streamlit UI æ¸²æŸ“å‡½æ•°
# ============================================================================

def render_temporal_analyzer():
    """
    æ¸²æŸ“æ—¶åºæ¼”å˜åˆ†ææ¨¡å—UI
    
    Requirements: 4.4, 4.6, 4.7
    """
    import streamlit as st
    from utils.session_state import log_message
    
    st.header("ğŸ“… æ—¶åºæ¼”å˜åˆ†æ")
    
    # åŠŸèƒ½ä»‹ç»ä¸æ“ä½œæ‰‹å†Œ
    with st.expander("ğŸ“– åŠŸèƒ½ä»‹ç»ä¸æ“ä½œæ‰‹å†Œ", expanded=False):
        st.markdown("""
        ## ğŸ“… æ—¶åºæ¼”å˜åˆ†ææ¨¡å—
        
        **åŠŸèƒ½æ¦‚è¿°**ï¼šåˆ†ææ”¿ç­–æ–‡æœ¬éšæ—¶é—´çš„æ¼”å˜è¶‹åŠ¿ï¼Œè¿½è¸ªå…³é”®è¯å’Œä¸»é¢˜çš„å˜åŒ–ã€‚
        
        ---
        
        ### ğŸ¯ ä½¿ç”¨åœºæ™¯
        
        | åœºæ™¯ | å…³æ³¨ç‚¹ | åº”ç”¨ |
        |------|--------|------|
        | æ”¿ç­–æ¼”å˜ç ”ç©¶ | å…³é”®è¯è¶‹åŠ¿ | è¿½è¸ªæ”¿ç­–è®®é¢˜çš„å‘å±•å˜åŒ– |
        | ä¸»é¢˜å˜è¿åˆ†æ | ä¸»é¢˜åˆ†å¸ƒå˜åŒ– | äº†è§£æ”¿ç­–é‡å¿ƒçš„è½¬ç§» |
        | æ–°å…´è®®é¢˜å‘ç° | æ–°å…´å…³é”®è¯ | è¯†åˆ«æ–°å‡ºç°çš„æ”¿ç­–çƒ­ç‚¹ |
        | è¡°é€€è®®é¢˜è¯†åˆ« | è¡°é€€å…³é”®è¯ | å‘ç°é€æ¸æ·¡å‡ºçš„æ”¿ç­–è®®é¢˜ |
        
        ---
        
        ### ğŸ“‹ æ“ä½œæ­¥éª¤
        
        **1. è®¾ç½®æ—¶é—´æ ‡ç­¾**ï¼š
        - æ‰‹åŠ¨ä¸ºæ¯ä¸ªæ–‡æ¡£è®¾ç½®æ—¶é—´æ ‡ç­¾ï¼ˆå¹´ä»½æˆ–æ—¥æœŸï¼‰
        - æˆ–ä½¿ç”¨"ä»æ–‡ä»¶åæå–"åŠŸèƒ½è‡ªåŠ¨æå–
        - æˆ–å¯¼å…¥å·²æœ‰çš„æ—¶é—´æ ‡ç­¾CSVæ–‡ä»¶
        
        **2. å…³é”®è¯è¶‹åŠ¿åˆ†æ**ï¼š
        - è¾“å…¥è¦è¿½è¸ªçš„å…³é”®è¯ï¼ˆé€—å·åˆ†éš”ï¼‰
        - æŸ¥çœ‹å…³é”®è¯åœ¨ä¸åŒæ—¶é—´æ®µçš„é¢‘ç‡å˜åŒ–
        - æŠ˜çº¿å›¾å±•ç¤ºè¶‹åŠ¿å˜åŒ–
        
        **3. ä¸»é¢˜æ¼”å˜åˆ†æ**ï¼ˆéœ€å…ˆå®Œæˆä¸»é¢˜å»ºæ¨¡ï¼‰ï¼š
        - æŸ¥çœ‹å„ä¸»é¢˜åœ¨ä¸åŒæ—¶é—´æ®µçš„åˆ†å¸ƒå˜åŒ–
        - å †å é¢ç§¯å›¾å±•ç¤ºä¸»é¢˜æ¼”å˜
        
        **4. å¯¼å‡ºåˆ†æç»“æœ**ï¼š
        - å¯¼å‡ºå…³é”®è¯è¶‹åŠ¿æ•°æ®
        - å¯¼å‡ºä¸»é¢˜æ¼”å˜æ•°æ®
        - å¯¼å‡ºæ—¶é—´æ ‡ç­¾è®¾ç½®
        
        ---
        
        ### âš™ï¸ æ—¶é—´æ ‡ç­¾æ ¼å¼
        
        | æ ¼å¼ | ç¤ºä¾‹ | è¯´æ˜ |
        |------|------|------|
        | å¹´ä»½ | 2020, 2021å¹´ | æŒ‰å¹´åº¦åˆ†æ |
        | å¹´æœˆ | 2020-01, 2020å¹´1æœˆ | æŒ‰æœˆåº¦åˆ†æ |
        | å®Œæ•´æ—¥æœŸ | 2020-01-15 | ç²¾ç¡®åˆ°æ—¥ |
        
        ---
        
        ### ğŸ’¡ ä½¿ç”¨å»ºè®®
        
        **å­¦æœ¯ç ”ç©¶å»ºè®®**ï¼š
        - ç¡®ä¿æ—¶é—´æ ‡ç­¾è¦†ç›–æ‰€æœ‰æ–‡æ¡£
        - é€‰æ‹©æœ‰ä»£è¡¨æ€§çš„å…³é”®è¯è¿›è¡Œè¶‹åŠ¿åˆ†æ
        - ç»“åˆä¸»é¢˜å»ºæ¨¡ç»“æœè¿›è¡Œç»¼åˆåˆ†æ
        
        **å¯è§†åŒ–å»ºè®®**ï¼š
        - å…³é”®è¯æ•°é‡å»ºè®®3-8ä¸ªï¼Œä¾¿äºå›¾è¡¨æ¸…æ™°å±•ç¤º
        - æ—¶é—´è·¨åº¦è¾ƒé•¿æ—¶ï¼Œå¯æŒ‰å¹´åº¦èšåˆåˆ†æ
        """)
    
    # æ£€æŸ¥æ•°æ®
    if not st.session_state.get("texts"):
        st.warning("âš ï¸ è¯·å…ˆåœ¨ã€Œæ–‡æœ¬é¢„å¤„ç†ã€æ ‡ç­¾é¡µä¸­å®Œæˆæ–‡æœ¬é¢„å¤„ç†")
        return
    
    texts = st.session_state["texts"]
    file_names = st.session_state.get("file_names", [])
    
    if not file_names:
        st.warning("âš ï¸ æœªæ‰¾åˆ°æ–‡æ¡£åç§°åˆ—è¡¨")
        return
    
    # è·å–æˆ–åˆ›å»ºåˆ†æå™¨
    if "temporal_analyzer" not in st.session_state or st.session_state["temporal_analyzer"] is None:
        st.session_state["temporal_analyzer"] = TemporalAnalyzer(texts, file_names)
        # æ¢å¤å·²ä¿å­˜çš„æ—¶é—´æ ‡ç­¾
        if st.session_state.get("time_labels"):
            st.session_state["temporal_analyzer"].set_time_labels_batch(st.session_state["time_labels"])
    
    analyzer = st.session_state["temporal_analyzer"]
    
    # åˆ›å»ºæ ‡ç­¾é¡µ
    tabs = st.tabs([
        "ğŸ·ï¸ æ—¶é—´æ ‡ç­¾è®¾ç½®",
        "ğŸ“ˆ å…³é”®è¯è¶‹åŠ¿",
        "ğŸ“Š ä¸»é¢˜æ¼”å˜",
        "ğŸ” æ–°å…´/è¡°é€€è¯",
        "ğŸ’¾ å¯¼å‡º"
    ])
    
    # ========== æ—¶é—´æ ‡ç­¾è®¾ç½® ==========
    with tabs[0]:
        st.subheader("æ—¶é—´æ ‡ç­¾è®¾ç½®")
        
        # ç»Ÿè®¡ä¿¡æ¯
        labeled_count = analyzer.get_labeled_documents_count()
        total_count = len(file_names)
        
        col1, col2, col3 = st.columns(3)
        col1.metric("æ€»æ–‡æ¡£æ•°", total_count)
        col2.metric("å·²æ ‡æ³¨", labeled_count)
        col3.metric("æœªæ ‡æ³¨", total_count - labeled_count)
        
        st.markdown("---")
        
        # è‡ªåŠ¨æå–é€‰é¡¹
        st.markdown("**ğŸ”§ è‡ªåŠ¨æå–æ—¶é—´æ ‡ç­¾**")
        auto_col1, auto_col2 = st.columns([3, 1])
        
        with auto_col1:
            extract_pattern = st.text_input(
                "æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼",
                value=r'(\d{4})',
                help="é»˜è®¤æå–4ä½æ•°å­—ä½œä¸ºå¹´ä»½ï¼Œå¯è‡ªå®šä¹‰æ¨¡å¼"
            )
        
        with auto_col2:
            st.write("")  # å ä½
            st.write("")
            if st.button("ä»æ–‡ä»¶åæå–", type="secondary"):
                count = analyzer.auto_extract_time_from_filename(extract_pattern)
                if count > 0:
                    st.success(f"æˆåŠŸæå– {count} ä¸ªæ—¶é—´æ ‡ç­¾")
                    # ä¿å­˜åˆ°ä¼šè¯çŠ¶æ€
                    st.session_state["time_labels"] = analyzer.get_all_time_labels()
                    log_message(f"ä»æ–‡ä»¶åè‡ªåŠ¨æå–äº† {count} ä¸ªæ—¶é—´æ ‡ç­¾")
                    st.rerun()
                else:
                    st.warning("æœªèƒ½ä»æ–‡ä»¶åä¸­æå–æ—¶é—´æ ‡ç­¾")
        
        st.markdown("---")
        
        # æ‰‹åŠ¨è®¾ç½®æ—¶é—´æ ‡ç­¾
        st.markdown("**âœï¸ æ‰‹åŠ¨è®¾ç½®æ—¶é—´æ ‡ç­¾**")
        
        # ä½¿ç”¨æ•°æ®ç¼–è¾‘å™¨
        current_labels = analyzer.get_all_time_labels()
        label_data = []
        for name in file_names:
            label_data.append({
                "æ–‡æ¡£å": name,
                "æ—¶é—´æ ‡ç­¾": current_labels.get(name, "")
            })
        
        df = pd.DataFrame(label_data)
        
        edited_df = st.data_editor(
            df,
            column_config={
                "æ–‡æ¡£å": st.column_config.TextColumn("æ–‡æ¡£å", disabled=True),
                "æ—¶é—´æ ‡ç­¾": st.column_config.TextColumn(
                    "æ—¶é—´æ ‡ç­¾",
                    help="è¾“å…¥å¹´ä»½ï¼ˆå¦‚2020ï¼‰æˆ–æ—¥æœŸï¼ˆå¦‚2020-01-15ï¼‰"
                )
            },
            use_container_width=True,
            hide_index=True,
            num_rows="fixed"
        )
        
        # ä¿å­˜æŒ‰é’®
        if st.button("ğŸ’¾ ä¿å­˜æ—¶é—´æ ‡ç­¾", type="primary"):
            # æ›´æ–°åˆ†æå™¨ä¸­çš„æ—¶é—´æ ‡ç­¾
            for _, row in edited_df.iterrows():
                doc_name = row["æ–‡æ¡£å"]
                time_label = row["æ—¶é—´æ ‡ç­¾"]
                analyzer.set_time_label(doc_name, time_label if pd.notna(time_label) else "")
            
            # ä¿å­˜åˆ°ä¼šè¯çŠ¶æ€
            st.session_state["time_labels"] = analyzer.get_all_time_labels()
            st.success("æ—¶é—´æ ‡ç­¾å·²ä¿å­˜")
            log_message(f"ä¿å­˜äº† {analyzer.get_labeled_documents_count()} ä¸ªæ—¶é—´æ ‡ç­¾")
        
        # å¯¼å…¥/å¯¼å‡ºæ—¶é—´æ ‡ç­¾
        st.markdown("---")
        st.markdown("**ğŸ“ å¯¼å…¥/å¯¼å‡ºæ—¶é—´æ ‡ç­¾**")
        
        imp_col, exp_col = st.columns(2)
        
        with imp_col:
            uploaded_file = st.file_uploader(
                "å¯¼å…¥æ—¶é—´æ ‡ç­¾CSV",
                type=["csv"],
                help="CSVæ–‡ä»¶éœ€åŒ…å«'æ–‡æ¡£å'å’Œ'æ—¶é—´æ ‡ç­¾'ä¸¤åˆ—"
            )
            if uploaded_file is not None:
                content = uploaded_file.read().decode('utf-8-sig')
                count = analyzer.import_time_labels(content)
                if count > 0:
                    st.session_state["time_labels"] = analyzer.get_all_time_labels()
                    st.success(f"æˆåŠŸå¯¼å…¥ {count} ä¸ªæ—¶é—´æ ‡ç­¾")
                    st.rerun()
                else:
                    st.error("å¯¼å…¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥CSVæ ¼å¼")
        
        with exp_col:
            csv_content = analyzer.export_time_labels()
            st.download_button(
                label="ğŸ“¥ å¯¼å‡ºæ—¶é—´æ ‡ç­¾CSV",
                data=csv_content,
                file_name="time_labels.csv",
                mime="text/csv"
            )
    
    # ========== å…³é”®è¯è¶‹åŠ¿åˆ†æ ==========
    with tabs[1]:
        st.subheader("å…³é”®è¯è¶‹åŠ¿åˆ†æ")
        
        if analyzer.get_labeled_documents_count() < 2:
            st.warning("âš ï¸ è¯·å…ˆåœ¨ã€Œæ—¶é—´æ ‡ç­¾è®¾ç½®ã€ä¸­ä¸ºè‡³å°‘2ä¸ªæ–‡æ¡£è®¾ç½®æ—¶é—´æ ‡ç­¾")
            return
        
        # æ˜¾ç¤ºæ—¶é—´æ®µä¿¡æ¯
        sorted_periods = analyzer.get_sorted_periods()
        st.info(f"ğŸ“… æ—¶é—´èŒƒå›´: {sorted_periods[0]} ~ {sorted_periods[-1]} ({len(sorted_periods)} ä¸ªæ—¶é—´æ®µ)")
        
        # å…³é”®è¯è¾“å…¥
        keywords_input = st.text_input(
            "è¾“å…¥è¦è¿½è¸ªçš„å…³é”®è¯ï¼ˆé€—å·åˆ†éš”ï¼‰",
            value=st.session_state.get("temporal_keywords", ""),
            placeholder="ä¾‹å¦‚: åˆ›æ–°, å‘å±•, æ”¹é©, æ•°å­—åŒ–"
        )
        
        if st.button("ğŸ“ˆ åˆ†æå…³é”®è¯è¶‹åŠ¿", type="primary"):
            if not keywords_input.strip():
                st.warning("è¯·è¾“å…¥è‡³å°‘ä¸€ä¸ªå…³é”®è¯")
            else:
                keywords = [k.strip() for k in keywords_input.split(",") if k.strip()]
                st.session_state["temporal_keywords"] = keywords_input
                
                with st.spinner("æ­£åœ¨åˆ†æå…³é”®è¯è¶‹åŠ¿..."):
                    trends = analyzer.analyze_keywords_trends(keywords)
                    st.session_state["keyword_trends"] = trends
                    log_message(f"åˆ†æäº† {len(keywords)} ä¸ªå…³é”®è¯çš„æ—¶åºè¶‹åŠ¿")
        
        # æ˜¾ç¤ºè¶‹åŠ¿å›¾
        if st.session_state.get("keyword_trends"):
            trends = st.session_state["keyword_trends"]
            
            # å‡†å¤‡æ•°æ®
            chart_data = []
            for keyword, trend in trends.items():
                for period, freq in trend.items():
                    chart_data.append({
                        "æ—¶é—´æ®µ": period,
                        "å…³é”®è¯": keyword,
                        "é¢‘ç‡": freq
                    })
            
            if chart_data:
                df = pd.DataFrame(chart_data)
                
                # ä½¿ç”¨Plotlyç»˜åˆ¶æŠ˜çº¿å›¾
                try:
                    import plotly.express as px
                    
                    fig = px.line(
                        df,
                        x="æ—¶é—´æ®µ",
                        y="é¢‘ç‡",
                        color="å…³é”®è¯",
                        markers=True,
                        title="å…³é”®è¯æ—¶åºè¶‹åŠ¿"
                    )
                    fig.update_layout(
                        xaxis_title="æ—¶é—´æ®µ",
                        yaxis_title="å‡ºç°é¢‘ç‡",
                        legend_title="å…³é”®è¯",
                        hovermode="x unified"
                    )
                    st.plotly_chart(fig, use_container_width=True)
                    
                except ImportError:
                    # ä½¿ç”¨StreamlitåŸç”Ÿå›¾è¡¨
                    pivot_df = df.pivot(index="æ—¶é—´æ®µ", columns="å…³é”®è¯", values="é¢‘ç‡")
                    st.line_chart(pivot_df)
                
                # æ˜¾ç¤ºæ•°æ®è¡¨æ ¼
                with st.expander("ğŸ“Š æŸ¥çœ‹è¯¦ç»†æ•°æ®"):
                    pivot_df = df.pivot(index="å…³é”®è¯", columns="æ—¶é—´æ®µ", values="é¢‘ç‡").fillna(0)
                    st.dataframe(pivot_df, use_container_width=True)
    
    # ========== ä¸»é¢˜æ¼”å˜åˆ†æ ==========
    with tabs[2]:
        st.subheader("ä¸»é¢˜æ¼”å˜åˆ†æ")
        
        if analyzer.get_labeled_documents_count() < 2:
            st.warning("âš ï¸ è¯·å…ˆåœ¨ã€Œæ—¶é—´æ ‡ç­¾è®¾ç½®ã€ä¸­ä¸ºè‡³å°‘2ä¸ªæ–‡æ¡£è®¾ç½®æ—¶é—´æ ‡ç­¾")
            return
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ä¸»é¢˜æ¨¡å‹ç»“æœ
        doc_topic_dist = st.session_state.get("doc_topic_dist")
        
        if doc_topic_dist is None:
            st.warning("âš ï¸ è¯·å…ˆåœ¨ã€Œä¸»é¢˜å»ºæ¨¡ã€æ ‡ç­¾é¡µä¸­å®ŒæˆLDAä¸»é¢˜å»ºæ¨¡")
            return
        
        # è·å–ä¸»é¢˜å…³é”®è¯ä½œä¸ºä¸»é¢˜åç§°
        topic_keywords = st.session_state.get("topic_keywords", {})
        n_topics = doc_topic_dist.shape[1] if len(doc_topic_dist.shape) > 1 else 1
        
        topic_names = []
        for i in range(n_topics):
            if i in topic_keywords and topic_keywords[i]:
                # å–å‰3ä¸ªå…³é”®è¯ä½œä¸ºä¸»é¢˜åç§°
                keywords = topic_keywords[i][:3] if isinstance(topic_keywords[i], list) else []
                name = f"ä¸»é¢˜{i+1}: {', '.join(keywords)}" if keywords else f"ä¸»é¢˜{i+1}"
            else:
                name = f"ä¸»é¢˜{i+1}"
            topic_names.append(name)
        
        if st.button("ğŸ“Š åˆ†æä¸»é¢˜æ¼”å˜", type="primary"):
            with st.spinner("æ­£åœ¨åˆ†æä¸»é¢˜æ¼”å˜..."):
                evolution = analyzer.analyze_topic_evolution(doc_topic_dist)
                st.session_state["topic_evolution"] = evolution
                log_message("å®Œæˆä¸»é¢˜æ¼”å˜åˆ†æ")
        
        # æ˜¾ç¤ºä¸»é¢˜æ¼”å˜å›¾
        if st.session_state.get("topic_evolution"):
            evolution = st.session_state["topic_evolution"]
            
            # å‡†å¤‡æ•°æ®
            chart_data = []
            for period, probs in evolution.items():
                for i, prob in enumerate(probs):
                    chart_data.append({
                        "æ—¶é—´æ®µ": period,
                        "ä¸»é¢˜": topic_names[i] if i < len(topic_names) else f"ä¸»é¢˜{i+1}",
                        "æ¦‚ç‡": prob
                    })
            
            if chart_data:
                df = pd.DataFrame(chart_data)
                
                try:
                    import plotly.express as px
                    
                    # å †å é¢ç§¯å›¾
                    fig = px.area(
                        df,
                        x="æ—¶é—´æ®µ",
                        y="æ¦‚ç‡",
                        color="ä¸»é¢˜",
                        title="ä¸»é¢˜æ—¶åºæ¼”å˜ï¼ˆå †å é¢ç§¯å›¾ï¼‰"
                    )
                    fig.update_layout(
                        xaxis_title="æ—¶é—´æ®µ",
                        yaxis_title="ä¸»é¢˜æ¦‚ç‡",
                        legend_title="ä¸»é¢˜",
                        hovermode="x unified"
                    )
                    st.plotly_chart(fig, use_container_width=True)
                    
                    # æŠ˜çº¿å›¾
                    fig2 = px.line(
                        df,
                        x="æ—¶é—´æ®µ",
                        y="æ¦‚ç‡",
                        color="ä¸»é¢˜",
                        markers=True,
                        title="ä¸»é¢˜æ—¶åºæ¼”å˜ï¼ˆæŠ˜çº¿å›¾ï¼‰"
                    )
                    fig2.update_layout(
                        xaxis_title="æ—¶é—´æ®µ",
                        yaxis_title="ä¸»é¢˜æ¦‚ç‡",
                        legend_title="ä¸»é¢˜"
                    )
                    st.plotly_chart(fig2, use_container_width=True)
                    
                except ImportError:
                    pivot_df = df.pivot(index="æ—¶é—´æ®µ", columns="ä¸»é¢˜", values="æ¦‚ç‡")
                    st.area_chart(pivot_df)
                
                # æ˜¾ç¤ºæ•°æ®è¡¨æ ¼
                with st.expander("ğŸ“Š æŸ¥çœ‹è¯¦ç»†æ•°æ®"):
                    pivot_df = df.pivot(index="æ—¶é—´æ®µ", columns="ä¸»é¢˜", values="æ¦‚ç‡").round(4)
                    st.dataframe(pivot_df, use_container_width=True)
    
    # ========== æ–°å…´/è¡°é€€å…³é”®è¯ ==========
    with tabs[3]:
        st.subheader("æ–°å…´ä¸è¡°é€€å…³é”®è¯")
        
        if analyzer.get_labeled_documents_count() < 2:
            st.warning("âš ï¸ è¯·å…ˆåœ¨ã€Œæ—¶é—´æ ‡ç­¾è®¾ç½®ã€ä¸­ä¸ºè‡³å°‘2ä¸ªæ–‡æ¡£è®¾ç½®æ—¶é—´æ ‡ç­¾")
            return
        
        sorted_periods = analyzer.get_sorted_periods()
        if len(sorted_periods) < 2:
            st.warning("âš ï¸ éœ€è¦è‡³å°‘2ä¸ªä¸åŒçš„æ—¶é—´æ®µæ‰èƒ½è¿›è¡Œè¶‹åŠ¿åˆ†æ")
            return
        
        # å‚æ•°è®¾ç½®
        col1, col2 = st.columns(2)
        with col1:
            recent_n = st.slider(
                "è¿‘æœŸæ—¶é—´æ®µæ•°é‡",
                min_value=1,
                max_value=max(1, len(sorted_periods) - 1),
                value=min(2, len(sorted_periods) - 1),
                help="å°†æœ€è¿‘Nä¸ªæ—¶é—´æ®µä¸ä¹‹å‰çš„æ—¶é—´æ®µè¿›è¡Œå¯¹æ¯”"
            )
        with col2:
            top_n = st.slider(
                "æ˜¾ç¤ºå…³é”®è¯æ•°é‡",
                min_value=5,
                max_value=30,
                value=10
            )
        
        if st.button("ğŸ” åˆ†ææ–°å…´/è¡°é€€å…³é”®è¯", type="primary"):
            with st.spinner("æ­£åœ¨åˆ†æ..."):
                emerging = analyzer.get_emerging_keywords(recent_n, top_n)
                declining = analyzer.get_declining_keywords(recent_n, top_n)
                
                st.session_state["emerging_keywords"] = emerging
                st.session_state["declining_keywords"] = declining
                log_message("å®Œæˆæ–°å…´/è¡°é€€å…³é”®è¯åˆ†æ")
        
        # æ˜¾ç¤ºç»“æœ
        if st.session_state.get("emerging_keywords") or st.session_state.get("declining_keywords"):
            col1, col2 = st.columns(2)
            
            with col1:
                st.markdown("### ğŸ“ˆ æ–°å…´å…³é”®è¯")
                st.caption("åœ¨è¿‘æœŸæ—¶é—´æ®µé¢‘ç‡æ˜¾è‘—å¢åŠ çš„è¯")
                
                emerging = st.session_state.get("emerging_keywords", [])
                if emerging:
                    df = pd.DataFrame(emerging, columns=["å…³é”®è¯", "å¢é•¿ç‡(%)"])
                    st.dataframe(df, use_container_width=True, hide_index=True)
                else:
                    st.info("æœªå‘ç°æ˜¾è‘—å¢é•¿çš„å…³é”®è¯")
            
            with col2:
                st.markdown("### ğŸ“‰ è¡°é€€å…³é”®è¯")
                st.caption("åœ¨è¿‘æœŸæ—¶é—´æ®µé¢‘ç‡æ˜¾è‘—ä¸‹é™çš„è¯")
                
                declining = st.session_state.get("declining_keywords", [])
                if declining:
                    df = pd.DataFrame(declining, columns=["å…³é”®è¯", "ä¸‹é™ç‡(%)"])
                    st.dataframe(df, use_container_width=True, hide_index=True)
                else:
                    st.info("æœªå‘ç°æ˜¾è‘—ä¸‹é™çš„å…³é”®è¯")
    
    # ========== å¯¼å‡º ==========
    with tabs[4]:
        st.subheader("å¯¼å‡ºåˆ†æç»“æœ")
        
        if analyzer.get_labeled_documents_count() == 0:
            st.warning("âš ï¸ è¯·å…ˆè®¾ç½®æ—¶é—´æ ‡ç­¾")
            return
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("**ğŸ“Š å…³é”®è¯è¶‹åŠ¿æ•°æ®**")
            if st.session_state.get("keyword_trends"):
                keywords = list(st.session_state["keyword_trends"].keys())
                csv_content = analyzer.export_trend_data(keywords)
                st.download_button(
                    label="ğŸ“¥ ä¸‹è½½å…³é”®è¯è¶‹åŠ¿CSV",
                    data=csv_content,
                    file_name="keyword_trends.csv",
                    mime="text/csv"
                )
            else:
                st.info("è¯·å…ˆè¿›è¡Œå…³é”®è¯è¶‹åŠ¿åˆ†æ")
        
        with col2:
            st.markdown("**ğŸ“ˆ ä¸»é¢˜æ¼”å˜æ•°æ®**")
            doc_topic_dist = st.session_state.get("doc_topic_dist")
            if doc_topic_dist is not None and st.session_state.get("topic_evolution"):
                topic_keywords = st.session_state.get("topic_keywords", {})
                n_topics = doc_topic_dist.shape[1]
                topic_names = [f"ä¸»é¢˜{i+1}" for i in range(n_topics)]
                
                csv_content = analyzer.export_topic_evolution_data(doc_topic_dist, topic_names)
                st.download_button(
                    label="ğŸ“¥ ä¸‹è½½ä¸»é¢˜æ¼”å˜CSV",
                    data=csv_content,
                    file_name="topic_evolution.csv",
                    mime="text/csv"
                )
            else:
                st.info("è¯·å…ˆè¿›è¡Œä¸»é¢˜æ¼”å˜åˆ†æ")
        
        st.markdown("---")
        
        # æ—¶é—´æ ‡ç­¾å¯¼å‡º
        st.markdown("**ğŸ·ï¸ æ—¶é—´æ ‡ç­¾æ•°æ®**")
        csv_content = analyzer.export_time_labels()
        st.download_button(
            label="ğŸ“¥ ä¸‹è½½æ—¶é—´æ ‡ç­¾CSV",
            data=csv_content,
            file_name="time_labels.csv",
            mime="text/csv"
        )
