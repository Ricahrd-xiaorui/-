# -*- coding: utf-8 -*-
"""
æ–‡æœ¬æ¯”è¾ƒåˆ†ææ¨¡å— (Comparative Analysis Module)
==============================================

æœ¬æ¨¡å—æä¾›æ–‡æœ¬æ¯”è¾ƒåˆ†æåŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š
- æ–‡æ¡£ç›¸ä¼¼åº¦è®¡ç®—
- å…±åŒå…³é”®è¯ä¸å·®å¼‚å…³é”®è¯è¯†åˆ«
- ç›¸ä¼¼æ®µè½æ£€æµ‹
- æ¯”è¾ƒç»“æœå¯è§†åŒ–
- ç»“æœå¯¼å‡º

Requirements: 5.1, 5.2, 5.3, 5.4, 5.5, 5.6, 5.7
"""

from typing import List, Dict, Tuple, Optional, Set
from collections import Counter
import numpy as np
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from dataclasses import dataclass


@dataclass
class ComparisonResult:
    """æ¯”è¾ƒç»“æœæ•°æ®ç±»"""
    doc1_name: str
    doc2_name: str
    similarity: float
    common_keywords: List[str]
    doc1_unique_keywords: List[str]
    doc2_unique_keywords: List[str]
    similar_segments: List[Tuple[str, str, float]]


class ComparativeAnalyzer:
    """
    æ¯”è¾ƒåˆ†æå™¨ - å¯¹æ¯”åˆ†æä¸åŒæ–‡æœ¬çš„å¼‚åŒ
    
    Attributes:
        texts: åˆ†è¯åçš„æ–‡æœ¬åˆ—è¡¨ï¼ˆæ¯ä¸ªæ–‡æœ¬æ˜¯è¯è¯­åˆ—è¡¨ï¼‰
        file_names: æ–‡æ¡£åç§°åˆ—è¡¨
        raw_texts: åŸå§‹æ–‡æœ¬åˆ—è¡¨ï¼ˆç”¨äºæ®µè½æ¯”è¾ƒï¼‰
    
    Requirements: 5.2, 5.3, 5.6
    """
    
    def __init__(self, texts: List[List[str]], file_names: List[str], 
                 raw_texts: Optional[List[str]] = None):
        """
        åˆå§‹åŒ–æ¯”è¾ƒåˆ†æå™¨
        
        Args:
            texts: åˆ†è¯åçš„æ–‡æœ¬åˆ—è¡¨
            file_names: æ–‡æ¡£åç§°åˆ—è¡¨
            raw_texts: åŸå§‹æ–‡æœ¬åˆ—è¡¨ï¼ˆå¯é€‰ï¼Œç”¨äºæ®µè½æ¯”è¾ƒï¼‰
        """
        self.texts = texts if texts else []
        self.file_names = file_names if file_names else []
        self.raw_texts = raw_texts if raw_texts else []
        self._tfidf_matrix: Optional[np.ndarray] = None
        self._vectorizer: Optional[TfidfVectorizer] = None
        self._similarity_matrix: Optional[np.ndarray] = None
    
    def _ensure_tfidf_matrix(self) -> None:
        """ç¡®ä¿TF-IDFçŸ©é˜µå·²è®¡ç®—"""
        if self._tfidf_matrix is None:
            if not self.texts:
                self._tfidf_matrix = np.array([])
                return
            
            # å°†è¯è¯­åˆ—è¡¨è½¬æ¢ä¸ºç©ºæ ¼åˆ†éš”çš„å­—ç¬¦ä¸²
            text_strings = [' '.join(words) for words in self.texts]
            
            self._vectorizer = TfidfVectorizer(max_features=1000)
            self._tfidf_matrix = self._vectorizer.fit_transform(text_strings).toarray()
    
    def calculate_similarity(self, doc1_idx: int, doc2_idx: int, 
                           method: str = 'cosine') -> float:
        """
        è®¡ç®—ä¸¤ä¸ªæ–‡æ¡£ä¹‹é—´çš„ç›¸ä¼¼åº¦
        
        Args:
            doc1_idx: ç¬¬ä¸€ä¸ªæ–‡æ¡£çš„ç´¢å¼•
            doc2_idx: ç¬¬äºŒä¸ªæ–‡æ¡£çš„ç´¢å¼•
            method: ç›¸ä¼¼åº¦è®¡ç®—æ–¹æ³•ï¼Œæ”¯æŒ 'cosine'ï¼ˆä½™å¼¦ç›¸ä¼¼åº¦ï¼‰å’Œ 'jaccard'ï¼ˆJaccardç›¸ä¼¼åº¦ï¼‰
        
        Returns:
            float: ç›¸ä¼¼åº¦å¾—åˆ†ï¼ŒèŒƒå›´ [0, 1]
        
        Requirements: 5.2
        """
        if not self.texts:
            return 0.0
        
        # è¾¹ç•Œæ£€æŸ¥
        if doc1_idx < 0 or doc1_idx >= len(self.texts):
            return 0.0
        if doc2_idx < 0 or doc2_idx >= len(self.texts):
            return 0.0
        
        # ç›¸åŒæ–‡æ¡£ç›¸ä¼¼åº¦ä¸º1
        if doc1_idx == doc2_idx:
            return 1.0
        
        if method == 'cosine':
            self._ensure_tfidf_matrix()
            if self._tfidf_matrix is None or len(self._tfidf_matrix) == 0:
                return 0.0
            
            vec1 = self._tfidf_matrix[doc1_idx].reshape(1, -1)
            vec2 = self._tfidf_matrix[doc2_idx].reshape(1, -1)
            
            similarity = cosine_similarity(vec1, vec2)[0][0]
            return float(similarity)
        
        elif method == 'jaccard':
            # Jaccardç›¸ä¼¼åº¦ï¼šäº¤é›†/å¹¶é›†
            set1 = set(self.texts[doc1_idx])
            set2 = set(self.texts[doc2_idx])
            
            if not set1 and not set2:
                return 1.0  # ä¸¤ä¸ªç©ºé›†è§†ä¸ºç›¸åŒ
            if not set1 or not set2:
                return 0.0
            
            intersection = len(set1 & set2)
            union = len(set1 | set2)
            
            return intersection / union if union > 0 else 0.0
        
        return 0.0
    
    def calculate_similarity_matrix(self, method: str = 'cosine') -> np.ndarray:
        """
        è®¡ç®—æ‰€æœ‰æ–‡æ¡£ä¹‹é—´çš„ç›¸ä¼¼åº¦çŸ©é˜µ
        
        Args:
            method: ç›¸ä¼¼åº¦è®¡ç®—æ–¹æ³•
        
        Returns:
            np.ndarray: ç›¸ä¼¼åº¦çŸ©é˜µï¼Œå½¢çŠ¶ä¸º (n_docs, n_docs)
        
        Requirements: 5.2
        """
        n_docs = len(self.texts)
        
        if n_docs == 0:
            return np.array([])
        
        if method == 'cosine':
            self._ensure_tfidf_matrix()
            if self._tfidf_matrix is None or len(self._tfidf_matrix) == 0:
                return np.array([])
            
            self._similarity_matrix = cosine_similarity(self._tfidf_matrix)
            return self._similarity_matrix
        
        elif method == 'jaccard':
            matrix = np.zeros((n_docs, n_docs))
            for i in range(n_docs):
                for j in range(n_docs):
                    matrix[i][j] = self.calculate_similarity(i, j, method='jaccard')
            self._similarity_matrix = matrix
            return matrix
        
        return np.array([])
    
    def find_common_keywords(self, doc_indices: List[int], top_n: int = 20) -> List[str]:
        """
        è¯†åˆ«å¤šä¸ªæ–‡æ¡£é—´çš„å…±åŒå…³é”®è¯
        
        Args:
            doc_indices: è¦æ¯”è¾ƒçš„æ–‡æ¡£ç´¢å¼•åˆ—è¡¨
            top_n: è¿”å›çš„å…³é”®è¯æ•°é‡
        
        Returns:
            List[str]: å…±åŒå…³é”®è¯åˆ—è¡¨
        
        Requirements: 5.3
        """
        if not doc_indices or not self.texts:
            return []
        
        # éªŒè¯ç´¢å¼•æœ‰æ•ˆæ€§
        valid_indices = [i for i in doc_indices if 0 <= i < len(self.texts)]
        if len(valid_indices) < 2:
            return []
        
        # è·å–æ¯ä¸ªæ–‡æ¡£çš„è¯é›†åˆ
        word_sets = [set(self.texts[i]) for i in valid_indices]
        
        # è®¡ç®—äº¤é›†
        common_words = word_sets[0]
        for word_set in word_sets[1:]:
            common_words = common_words & word_set
        
        if not common_words:
            return []
        
        # æŒ‰æ€»é¢‘ç‡æ’åº
        word_freq = Counter()
        for idx in valid_indices:
            for word in self.texts[idx]:
                if word in common_words:
                    word_freq[word] += 1
        
        return [word for word, _ in word_freq.most_common(top_n)]
    
    def find_unique_keywords(self, doc_idx: int, other_indices: List[int], 
                            top_n: int = 20) -> List[str]:
        """
        è¯†åˆ«æ–‡æ¡£ç›¸å¯¹äºå…¶ä»–æ–‡æ¡£çš„ç‹¬ç‰¹å…³é”®è¯
        
        Args:
            doc_idx: ç›®æ ‡æ–‡æ¡£ç´¢å¼•
            other_indices: å…¶ä»–æ–‡æ¡£ç´¢å¼•åˆ—è¡¨
            top_n: è¿”å›çš„å…³é”®è¯æ•°é‡
        
        Returns:
            List[str]: ç‹¬ç‰¹å…³é”®è¯åˆ—è¡¨
        
        Requirements: 5.3
        """
        if not self.texts:
            return []
        
        # éªŒè¯ç´¢å¼•æœ‰æ•ˆæ€§
        if doc_idx < 0 or doc_idx >= len(self.texts):
            return []
        
        valid_other_indices = [i for i in other_indices if 0 <= i < len(self.texts) and i != doc_idx]
        
        # è·å–ç›®æ ‡æ–‡æ¡£çš„è¯é›†åˆ
        target_words = set(self.texts[doc_idx])
        
        # è·å–å…¶ä»–æ–‡æ¡£çš„è¯é›†åˆå¹¶é›†
        other_words = set()
        for idx in valid_other_indices:
            other_words.update(self.texts[idx])
        
        # è®¡ç®—å·®é›†
        unique_words = target_words - other_words
        
        if not unique_words:
            return []
        
        # æŒ‰é¢‘ç‡æ’åº
        word_freq = Counter()
        for word in self.texts[doc_idx]:
            if word in unique_words:
                word_freq[word] += 1
        
        return [word for word, _ in word_freq.most_common(top_n)]
    
    def find_similar_segments(self, doc1_idx: int, doc2_idx: int, 
                             threshold: float = 0.5,
                             segment_size: int = 50) -> List[Tuple[str, str, float]]:
        """
        æŸ¥æ‰¾ä¸¤ä¸ªæ–‡æ¡£ä¹‹é—´çš„ç›¸ä¼¼æ®µè½
        
        Args:
            doc1_idx: ç¬¬ä¸€ä¸ªæ–‡æ¡£çš„ç´¢å¼•
            doc2_idx: ç¬¬äºŒä¸ªæ–‡æ¡£çš„ç´¢å¼•
            threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
            segment_size: æ®µè½å¤§å°ï¼ˆå­—ç¬¦æ•°ï¼‰
        
        Returns:
            List[Tuple[str, str, float]]: (æ–‡æ¡£1æ®µè½, æ–‡æ¡£2æ®µè½, ç›¸ä¼¼åº¦) åˆ—è¡¨
        
        Requirements: 5.6
        """
        if not self.raw_texts:
            return []
        
        # éªŒè¯ç´¢å¼•æœ‰æ•ˆæ€§
        if doc1_idx < 0 or doc1_idx >= len(self.raw_texts):
            return []
        if doc2_idx < 0 or doc2_idx >= len(self.raw_texts):
            return []
        
        text1 = self.raw_texts[doc1_idx]
        text2 = self.raw_texts[doc2_idx]
        
        # åˆ†å‰²æˆæ®µè½
        segments1 = self._split_into_segments(text1, segment_size)
        segments2 = self._split_into_segments(text2, segment_size)
        
        if not segments1 or not segments2:
            return []
        
        # è®¡ç®—æ®µè½é—´çš„ç›¸ä¼¼åº¦
        similar_pairs = []
        
        # ä½¿ç”¨TF-IDFè®¡ç®—æ®µè½ç›¸ä¼¼åº¦
        all_segments = segments1 + segments2
        if len(all_segments) < 2:
            return []
        
        try:
            vectorizer = TfidfVectorizer(max_features=500)
            tfidf_matrix = vectorizer.fit_transform(all_segments).toarray()
            
            n1 = len(segments1)
            
            for i, seg1 in enumerate(segments1):
                for j, seg2 in enumerate(segments2):
                    vec1 = tfidf_matrix[i].reshape(1, -1)
                    vec2 = tfidf_matrix[n1 + j].reshape(1, -1)
                    
                    sim = cosine_similarity(vec1, vec2)[0][0]
                    
                    if sim >= threshold:
                        similar_pairs.append((seg1, seg2, float(sim)))
        except Exception:
            return []
        
        # æŒ‰ç›¸ä¼¼åº¦é™åºæ’åº
        similar_pairs.sort(key=lambda x: -x[2])
        
        return similar_pairs[:20]  # æœ€å¤šè¿”å›20å¯¹
    
    def _split_into_segments(self, text: str, segment_size: int) -> List[str]:
        """
        å°†æ–‡æœ¬åˆ†å‰²æˆæ®µè½
        
        Args:
            text: åŸå§‹æ–‡æœ¬
            segment_size: æ®µè½å¤§å°
        
        Returns:
            List[str]: æ®µè½åˆ—è¡¨
        """
        if not text:
            return []
        
        # é¦–å…ˆæŒ‰æ¢è¡Œç¬¦åˆ†å‰²
        paragraphs = [p.strip() for p in text.split('\n') if p.strip()]
        
        segments = []
        for para in paragraphs:
            if len(para) <= segment_size:
                if para:
                    segments.append(para)
            else:
                # æŒ‰å¥å­åˆ†å‰²é•¿æ®µè½
                sentences = self._split_into_sentences(para)
                current_segment = ""
                
                for sentence in sentences:
                    if len(current_segment) + len(sentence) <= segment_size:
                        current_segment += sentence
                    else:
                        if current_segment:
                            segments.append(current_segment)
                        current_segment = sentence
                
                if current_segment:
                    segments.append(current_segment)
        
        return segments
    
    def _split_into_sentences(self, text: str) -> List[str]:
        """
        å°†æ–‡æœ¬åˆ†å‰²æˆå¥å­
        
        Args:
            text: æ–‡æœ¬
        
        Returns:
            List[str]: å¥å­åˆ—è¡¨
        """
        import re
        # ä¸­æ–‡å¥å­åˆ†å‰²
        sentences = re.split(r'([ã€‚ï¼ï¼Ÿï¼›\n])', text)
        
        result = []
        for i in range(0, len(sentences) - 1, 2):
            sentence = sentences[i] + (sentences[i + 1] if i + 1 < len(sentences) else '')
            if sentence.strip():
                result.append(sentence.strip())
        
        # å¤„ç†æœ€åä¸€ä¸ªå…ƒç´ 
        if len(sentences) % 2 == 1 and sentences[-1].strip():
            result.append(sentences[-1].strip())
        
        return result
    
    def compare_documents(self, doc1_idx: int, doc2_idx: int, 
                         top_n: int = 20) -> ComparisonResult:
        """
        å…¨é¢æ¯”è¾ƒä¸¤ä¸ªæ–‡æ¡£
        
        Args:
            doc1_idx: ç¬¬ä¸€ä¸ªæ–‡æ¡£çš„ç´¢å¼•
            doc2_idx: ç¬¬äºŒä¸ªæ–‡æ¡£çš„ç´¢å¼•
            top_n: è¿”å›çš„å…³é”®è¯æ•°é‡
        
        Returns:
            ComparisonResult: æ¯”è¾ƒç»“æœ
        """
        doc1_name = self.file_names[doc1_idx] if doc1_idx < len(self.file_names) else f"æ–‡æ¡£{doc1_idx}"
        doc2_name = self.file_names[doc2_idx] if doc2_idx < len(self.file_names) else f"æ–‡æ¡£{doc2_idx}"
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        similarity = self.calculate_similarity(doc1_idx, doc2_idx)
        
        # æŸ¥æ‰¾å…±åŒå…³é”®è¯
        common_keywords = self.find_common_keywords([doc1_idx, doc2_idx], top_n)
        
        # æŸ¥æ‰¾å„è‡ªç‹¬ç‰¹å…³é”®è¯
        doc1_unique = self.find_unique_keywords(doc1_idx, [doc2_idx], top_n)
        doc2_unique = self.find_unique_keywords(doc2_idx, [doc1_idx], top_n)
        
        # æŸ¥æ‰¾ç›¸ä¼¼æ®µè½
        similar_segments = self.find_similar_segments(doc1_idx, doc2_idx)
        
        return ComparisonResult(
            doc1_name=doc1_name,
            doc2_name=doc2_name,
            similarity=similarity,
            common_keywords=common_keywords,
            doc1_unique_keywords=doc1_unique,
            doc2_unique_keywords=doc2_unique,
            similar_segments=similar_segments
        )
    
    def get_most_similar_pairs(self, top_n: int = 10) -> List[Tuple[str, str, float]]:
        """
        è·å–æœ€ç›¸ä¼¼çš„æ–‡æ¡£å¯¹
        
        Args:
            top_n: è¿”å›çš„æ–‡æ¡£å¯¹æ•°é‡
        
        Returns:
            List[Tuple[str, str, float]]: (æ–‡æ¡£1å, æ–‡æ¡£2å, ç›¸ä¼¼åº¦) åˆ—è¡¨
        """
        if len(self.texts) < 2:
            return []
        
        # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
        sim_matrix = self.calculate_similarity_matrix()
        
        if sim_matrix is None or len(sim_matrix) == 0:
            return []
        
        # æ”¶é›†æ‰€æœ‰æ–‡æ¡£å¯¹çš„ç›¸ä¼¼åº¦
        pairs = []
        n_docs = len(self.texts)
        
        for i in range(n_docs):
            for j in range(i + 1, n_docs):
                pairs.append((
                    self.file_names[i] if i < len(self.file_names) else f"æ–‡æ¡£{i}",
                    self.file_names[j] if j < len(self.file_names) else f"æ–‡æ¡£{j}",
                    float(sim_matrix[i][j])
                ))
        
        # æŒ‰ç›¸ä¼¼åº¦é™åºæ’åº
        pairs.sort(key=lambda x: -x[2])
        
        return pairs[:top_n]
    
    def export_comparison(self, doc_indices: Optional[List[int]] = None) -> str:
        """
        å¯¼å‡ºæ¯”è¾ƒåˆ†æç»“æœä¸ºCSVæ ¼å¼
        
        Args:
            doc_indices: è¦å¯¼å‡ºçš„æ–‡æ¡£ç´¢å¼•åˆ—è¡¨ï¼Œå¦‚æœä¸ºNoneåˆ™å¯¼å‡ºæ‰€æœ‰
        
        Returns:
            str: CSVæ ¼å¼å­—ç¬¦ä¸²
        
        Requirements: 5.7
        """
        if not self.texts:
            return ""
        
        if doc_indices is None:
            doc_indices = list(range(len(self.texts)))
        
        # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
        sim_matrix = self.calculate_similarity_matrix()
        
        if sim_matrix is None or len(sim_matrix) == 0:
            return ""
        
        # æ„å»ºæ•°æ®
        data = []
        for i in doc_indices:
            for j in doc_indices:
                if i < j:
                    doc1_name = self.file_names[i] if i < len(self.file_names) else f"æ–‡æ¡£{i}"
                    doc2_name = self.file_names[j] if j < len(self.file_names) else f"æ–‡æ¡£{j}"
                    
                    common_kw = self.find_common_keywords([i, j], 10)
                    
                    data.append({
                        "æ–‡æ¡£1": doc1_name,
                        "æ–‡æ¡£2": doc2_name,
                        "ç›¸ä¼¼åº¦": round(float(sim_matrix[i][j]), 4),
                        "å…±åŒå…³é”®è¯": ", ".join(common_kw[:5])
                    })
        
        df = pd.DataFrame(data)
        return df.to_csv(index=False, encoding='utf-8-sig')
    
    def export_similarity_matrix(self) -> str:
        """
        å¯¼å‡ºç›¸ä¼¼åº¦çŸ©é˜µä¸ºCSVæ ¼å¼
        
        Returns:
            str: CSVæ ¼å¼å­—ç¬¦ä¸²
        """
        sim_matrix = self.calculate_similarity_matrix()
        
        if sim_matrix is None or len(sim_matrix) == 0:
            return ""
        
        # ä½¿ç”¨æ–‡æ¡£åä½œä¸ºè¡Œåˆ—æ ‡ç­¾
        labels = [self.file_names[i] if i < len(self.file_names) else f"æ–‡æ¡£{i}" 
                  for i in range(len(self.texts))]
        
        df = pd.DataFrame(sim_matrix, index=labels, columns=labels)
        return df.to_csv(encoding='utf-8-sig')



# ============================================================================
# Streamlit UI æ¸²æŸ“å‡½æ•°
# ============================================================================

def render_comparative_analyzer():
    """
    æ¸²æŸ“æ–‡æœ¬æ¯”è¾ƒåˆ†ææ¨¡å—UI
    
    Requirements: 5.1, 5.4, 5.5, 5.7
    """
    import streamlit as st
    from utils.session_state import log_message
    
    st.header("ğŸ” æ–‡æœ¬æ¯”è¾ƒåˆ†æ")
    
    # åŠŸèƒ½ä»‹ç»ä¸æ“ä½œæ‰‹å†Œ
    with st.expander("ğŸ“– åŠŸèƒ½ä»‹ç»ä¸æ“ä½œæ‰‹å†Œ", expanded=False):
        st.markdown("""
        ## ğŸ” æ–‡æœ¬æ¯”è¾ƒåˆ†ææ¨¡å—
        
        **åŠŸèƒ½æ¦‚è¿°**ï¼šå¯¹æ¯”åˆ†æä¸åŒæ”¿ç­–æ–‡æœ¬çš„å¼‚åŒï¼Œæ”¯æŒç›¸ä¼¼åº¦è®¡ç®—ã€å…³é”®è¯å¯¹æ¯”å’Œç›¸ä¼¼æ®µè½æ£€æµ‹ã€‚
        
        ---
        
        ### ğŸ¯ ä½¿ç”¨åœºæ™¯
        
        | åœºæ™¯ | å…³æ³¨ç‚¹ | åº”ç”¨ |
        |------|--------|------|
        | æ”¿ç­–æ¯”è¾ƒç ”ç©¶ | ç›¸ä¼¼åº¦çŸ©é˜µ | å‘ç°ç›¸ä¼¼æ”¿ç­–æ–‡ä»¶ |
        | å·®å¼‚åˆ†æ | ç‹¬ç‰¹å…³é”®è¯ | è¯†åˆ«æ”¿ç­–å·®å¼‚ç‚¹ |
        | å…±æ€§åˆ†æ | å…±åŒå…³é”®è¯ | å‘ç°æ”¿ç­–å…±åŒä¸»é¢˜ |
        | æ–‡æœ¬æº¯æº | ç›¸ä¼¼æ®µè½ | è¿½è¸ªæ”¿ç­–æ–‡æœ¬æ¥æº |
        
        ---
        
        ### ğŸ“‹ æ“ä½œæ­¥éª¤
        
        **1. æ–‡æ¡£é€‰æ‹©**ï¼š
        - é€‰æ‹©è¦æ¯”è¾ƒçš„ä¸¤ä¸ªæˆ–å¤šä¸ªæ–‡æ¡£
        - å¯ä»¥é€‰æ‹©å…¨éƒ¨æ–‡æ¡£è¿›è¡Œæ•´ä½“åˆ†æ
        
        **2. ç›¸ä¼¼åº¦åˆ†æ**ï¼š
        - æŸ¥çœ‹æ–‡æ¡£é—´çš„ç›¸ä¼¼åº¦å¾—åˆ†
        - ç›¸ä¼¼åº¦çƒ­å›¾å±•ç¤ºæ•´ä½“å…³ç³»
        
        **3. å…³é”®è¯å¯¹æ¯”**ï¼š
        - æŸ¥çœ‹å…±åŒå…³é”®è¯ï¼ˆéŸ¦æ©å›¾ï¼‰
        - æŸ¥çœ‹å„æ–‡æ¡£ç‹¬ç‰¹å…³é”®è¯
        
        **4. å¹¶æ’å¯¹æ¯”**ï¼š
        - é€‰æ‹©ä¸¤ä¸ªæ–‡æ¡£è¿›è¡Œè¯¦ç»†å¯¹æ¯”
        - æŸ¥çœ‹ç›¸ä¼¼æ®µè½é«˜äº®æ˜¾ç¤º
        
        **5. å¯¼å‡ºç»“æœ**ï¼š
        - å¯¼å‡ºç›¸ä¼¼åº¦çŸ©é˜µ
        - å¯¼å‡ºæ¯”è¾ƒåˆ†ææŠ¥å‘Š
        
        ---
        
        ### âš™ï¸ ç›¸ä¼¼åº¦è®¡ç®—æ–¹æ³•
        
        | æ–¹æ³• | åŸç† | é€‚ç”¨åœºæ™¯ |
        |------|------|----------|
        | ä½™å¼¦ç›¸ä¼¼åº¦ | åŸºäºTF-IDFå‘é‡å¤¹è§’ | é€šç”¨ï¼Œæ¨èä½¿ç”¨ |
        | Jaccardç›¸ä¼¼åº¦ | åŸºäºè¯é›†åˆäº¤å¹¶æ¯” | å…³æ³¨è¯æ±‡é‡å  |
        
        ---
        
        ### ğŸ’¡ ä½¿ç”¨å»ºè®®
        
        **å­¦æœ¯ç ”ç©¶å»ºè®®**ï¼š
        - ç›¸ä¼¼åº¦>0.8ï¼šé«˜åº¦ç›¸ä¼¼ï¼Œå¯èƒ½å­˜åœ¨å¼•ç”¨å…³ç³»
        - ç›¸ä¼¼åº¦0.5-0.8ï¼šä¸­åº¦ç›¸ä¼¼ï¼Œä¸»é¢˜ç›¸è¿‘
        - ç›¸ä¼¼åº¦<0.5ï¼šå·®å¼‚è¾ƒå¤§ï¼Œä¸»é¢˜ä¸åŒ
        
        **å¯è§†åŒ–å»ºè®®**ï¼š
        - çƒ­å›¾é€‚åˆå±•ç¤ºå¤šæ–‡æ¡£æ•´ä½“å…³ç³»
        - éŸ¦æ©å›¾é€‚åˆå±•ç¤º2-3ä¸ªæ–‡æ¡£çš„å…³é”®è¯é‡å 
        """)
    
    # æ£€æŸ¥æ•°æ®
    if not st.session_state.get("texts"):
        st.warning("âš ï¸ è¯·å…ˆåœ¨ã€Œæ–‡æœ¬é¢„å¤„ç†ã€æ ‡ç­¾é¡µä¸­å®Œæˆæ–‡æœ¬é¢„å¤„ç†")
        return
    
    texts = st.session_state["texts"]
    file_names = st.session_state.get("file_names", [])
    raw_texts = st.session_state.get("raw_texts", [])
    
    if len(texts) < 2:
        st.warning("âš ï¸ è‡³å°‘éœ€è¦2ä¸ªæ–‡æ¡£æ‰èƒ½è¿›è¡Œæ¯”è¾ƒåˆ†æ")
        return
    
    # è·å–æˆ–åˆ›å»ºåˆ†æå™¨
    if "comparative_analyzer" not in st.session_state or st.session_state["comparative_analyzer"] is None:
        st.session_state["comparative_analyzer"] = ComparativeAnalyzer(texts, file_names, raw_texts)
    
    analyzer = st.session_state["comparative_analyzer"]
    
    # åˆ›å»ºæ ‡ç­¾é¡µ
    tabs = st.tabs([
        "ğŸ“Š ç›¸ä¼¼åº¦çŸ©é˜µ",
        "ğŸ”„ æ–‡æ¡£å¯¹æ¯”",
        "ğŸ”‘ å…³é”®è¯åˆ†æ",
        "ğŸ“ ç›¸ä¼¼æ®µè½",
        "ğŸ’¾ å¯¼å‡º"
    ])
    
    # ========== ç›¸ä¼¼åº¦çŸ©é˜µ ==========
    with tabs[0]:
        st.subheader("æ–‡æ¡£ç›¸ä¼¼åº¦çŸ©é˜µ")
        
        # å‚æ•°è®¾ç½®
        col1, col2 = st.columns(2)
        with col1:
            similarity_method = st.selectbox(
                "ç›¸ä¼¼åº¦è®¡ç®—æ–¹æ³•",
                ["ä½™å¼¦ç›¸ä¼¼åº¦ (cosine)", "Jaccardç›¸ä¼¼åº¦ (jaccard)"],
                index=0,
                help="ä½™å¼¦ç›¸ä¼¼åº¦åŸºäºTF-IDFå‘é‡ï¼ŒJaccardç›¸ä¼¼åº¦åŸºäºè¯é›†åˆ"
            )
        
        method = "cosine" if "cosine" in similarity_method else "jaccard"
        
        if st.button("ğŸ“Š è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ", type="primary", key="calc_sim_matrix"):
            with st.spinner("æ­£åœ¨è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ..."):
                sim_matrix = analyzer.calculate_similarity_matrix(method=method)
                st.session_state["similarity_matrix"] = sim_matrix
                st.session_state["similarity_method"] = method
                log_message(f"è®¡ç®—äº† {len(texts)} ä¸ªæ–‡æ¡£çš„ç›¸ä¼¼åº¦çŸ©é˜µ")
        
        # æ˜¾ç¤ºç›¸ä¼¼åº¦çŸ©é˜µ
        if st.session_state.get("similarity_matrix") is not None:
            sim_matrix = st.session_state["similarity_matrix"]
            
            if len(sim_matrix) > 0:
                # åˆ›å»ºDataFrameç”¨äºæ˜¾ç¤º
                labels = [file_names[i] if i < len(file_names) else f"æ–‡æ¡£{i}" 
                         for i in range(len(sim_matrix))]
                
                # æˆªæ–­é•¿æ–‡ä»¶å
                short_labels = [name[:15] + "..." if len(name) > 15 else name for name in labels]
                
                df = pd.DataFrame(sim_matrix, index=short_labels, columns=short_labels)
                
                # ä½¿ç”¨Plotlyç»˜åˆ¶çƒ­å›¾
                try:
                    import plotly.express as px
                    import plotly.graph_objects as go
                    
                    fig = px.imshow(
                        sim_matrix,
                        x=short_labels,
                        y=short_labels,
                        color_continuous_scale="RdYlBu_r",
                        aspect="auto",
                        title="æ–‡æ¡£ç›¸ä¼¼åº¦çƒ­å›¾"
                    )
                    fig.update_layout(
                        xaxis_title="æ–‡æ¡£",
                        yaxis_title="æ–‡æ¡£",
                        xaxis_tickangle=-45
                    )
                    st.plotly_chart(fig, use_container_width=True)
                    
                except ImportError:
                    st.dataframe(df.style.background_gradient(cmap='RdYlBu_r'), 
                               use_container_width=True)
                
                # æ˜¾ç¤ºæœ€ç›¸ä¼¼çš„æ–‡æ¡£å¯¹
                st.markdown("---")
                st.subheader("æœ€ç›¸ä¼¼çš„æ–‡æ¡£å¯¹")
                
                similar_pairs = analyzer.get_most_similar_pairs(top_n=10)
                
                if similar_pairs:
                    pairs_df = pd.DataFrame(similar_pairs, columns=["æ–‡æ¡£1", "æ–‡æ¡£2", "ç›¸ä¼¼åº¦"])
                    pairs_df["ç›¸ä¼¼åº¦"] = pairs_df["ç›¸ä¼¼åº¦"].apply(lambda x: f"{x:.4f}")
                    st.dataframe(pairs_df, use_container_width=True, hide_index=True)
    
    # ========== æ–‡æ¡£å¯¹æ¯” ==========
    with tabs[1]:
        st.subheader("æ–‡æ¡£å¯¹æ¯”åˆ†æ")
        
        # é€‰æ‹©è¦æ¯”è¾ƒçš„æ–‡æ¡£
        col1, col2 = st.columns(2)
        
        with col1:
            doc1_name = st.selectbox(
                "é€‰æ‹©ç¬¬ä¸€ä¸ªæ–‡æ¡£",
                file_names,
                index=0,
                key="compare_doc1"
            )
        
        with col2:
            # é»˜è®¤é€‰æ‹©ç¬¬äºŒä¸ªæ–‡æ¡£
            default_idx = 1 if len(file_names) > 1 else 0
            doc2_name = st.selectbox(
                "é€‰æ‹©ç¬¬äºŒä¸ªæ–‡æ¡£",
                file_names,
                index=default_idx,
                key="compare_doc2"
            )
        
        if doc1_name == doc2_name:
            st.warning("è¯·é€‰æ‹©ä¸¤ä¸ªä¸åŒçš„æ–‡æ¡£è¿›è¡Œæ¯”è¾ƒ")
        else:
            doc1_idx = file_names.index(doc1_name)
            doc2_idx = file_names.index(doc2_name)
            
            if st.button("ğŸ”„ å¼€å§‹å¯¹æ¯”", type="primary", key="start_compare"):
                with st.spinner("æ­£åœ¨åˆ†ææ–‡æ¡£å·®å¼‚..."):
                    result = analyzer.compare_documents(doc1_idx, doc2_idx)
                    st.session_state["comparison_result"] = result
                    log_message(f"å¯¹æ¯”äº†æ–‡æ¡£: {doc1_name} vs {doc2_name}")
            
            # æ˜¾ç¤ºå¯¹æ¯”ç»“æœ
            if st.session_state.get("comparison_result"):
                result = st.session_state["comparison_result"]
                
                # ç›¸ä¼¼åº¦æŒ‡æ ‡
                st.markdown("### ğŸ“Š ç›¸ä¼¼åº¦")
                
                sim_color = "green" if result.similarity > 0.7 else ("orange" if result.similarity > 0.4 else "red")
                st.markdown(f"""
                <div style="text-align: center; padding: 20px; background-color: #f0f2f6; border-radius: 10px;">
                    <h2 style="color: {sim_color}; margin: 0;">{result.similarity:.2%}</h2>
                    <p style="margin: 5px 0 0 0;">æ–‡æ¡£ç›¸ä¼¼åº¦</p>
                </div>
                """, unsafe_allow_html=True)
                
                st.markdown("---")
                
                # å…³é”®è¯å¯¹æ¯”
                st.markdown("### ğŸ”‘ å…³é”®è¯å¯¹æ¯”")
                
                kw_col1, kw_col2, kw_col3 = st.columns(3)
                
                with kw_col1:
                    st.markdown(f"**{result.doc1_name} ç‹¬ç‰¹å…³é”®è¯**")
                    if result.doc1_unique_keywords:
                        for kw in result.doc1_unique_keywords[:10]:
                            st.markdown(f"- {kw}")
                    else:
                        st.info("æ— ç‹¬ç‰¹å…³é”®è¯")
                
                with kw_col2:
                    st.markdown("**å…±åŒå…³é”®è¯**")
                    if result.common_keywords:
                        for kw in result.common_keywords[:10]:
                            st.markdown(f"- {kw}")
                    else:
                        st.info("æ— å…±åŒå…³é”®è¯")
                
                with kw_col3:
                    st.markdown(f"**{result.doc2_name} ç‹¬ç‰¹å…³é”®è¯**")
                    if result.doc2_unique_keywords:
                        for kw in result.doc2_unique_keywords[:10]:
                            st.markdown(f"- {kw}")
                    else:
                        st.info("æ— ç‹¬ç‰¹å…³é”®è¯")
    
    # ========== å…³é”®è¯åˆ†æ ==========
    with tabs[2]:
        st.subheader("å¤šæ–‡æ¡£å…³é”®è¯åˆ†æ")
        
        # é€‰æ‹©å¤šä¸ªæ–‡æ¡£
        selected_docs = st.multiselect(
            "é€‰æ‹©è¦åˆ†æçš„æ–‡æ¡£ï¼ˆ2-5ä¸ªï¼‰",
            file_names,
            default=file_names[:min(3, len(file_names))],
            key="keyword_analysis_docs"
        )
        
        if len(selected_docs) < 2:
            st.warning("è¯·è‡³å°‘é€‰æ‹©2ä¸ªæ–‡æ¡£")
        elif len(selected_docs) > 5:
            st.warning("å»ºè®®é€‰æ‹©ä¸è¶…è¿‡5ä¸ªæ–‡æ¡£ä»¥è·å¾—æ›´æ¸…æ™°çš„åˆ†æç»“æœ")
        else:
            top_n = st.slider("æ˜¾ç¤ºå…³é”®è¯æ•°é‡", 5, 30, 15, key="kw_top_n")
            
            if st.button("ğŸ”‘ åˆ†æå…³é”®è¯", type="primary", key="analyze_keywords"):
                doc_indices = [file_names.index(name) for name in selected_docs]
                
                with st.spinner("æ­£åœ¨åˆ†æå…³é”®è¯..."):
                    # å…±åŒå…³é”®è¯
                    common_kw = analyzer.find_common_keywords(doc_indices, top_n)
                    
                    # å„æ–‡æ¡£ç‹¬ç‰¹å…³é”®è¯
                    unique_kw = {}
                    for i, name in enumerate(selected_docs):
                        idx = file_names.index(name)
                        other_indices = [file_names.index(n) for n in selected_docs if n != name]
                        unique_kw[name] = analyzer.find_unique_keywords(idx, other_indices, top_n)
                    
                    st.session_state["keyword_analysis"] = {
                        "common": common_kw,
                        "unique": unique_kw,
                        "docs": selected_docs
                    }
                    log_message(f"åˆ†æäº† {len(selected_docs)} ä¸ªæ–‡æ¡£çš„å…³é”®è¯")
            
            # æ˜¾ç¤ºç»“æœ
            if st.session_state.get("keyword_analysis"):
                analysis = st.session_state["keyword_analysis"]
                
                # å…±åŒå…³é”®è¯
                st.markdown("### ğŸ“Œ å…±åŒå…³é”®è¯")
                if analysis["common"]:
                    # æ˜¾ç¤ºä¸ºæ ‡ç­¾äº‘æ ·å¼
                    kw_html = " ".join([
                        f'<span style="background-color: #e1f5fe; padding: 5px 10px; margin: 3px; border-radius: 15px; display: inline-block;">{kw}</span>'
                        for kw in analysis["common"]
                    ])
                    st.markdown(kw_html, unsafe_allow_html=True)
                else:
                    st.info("è¿™äº›æ–‡æ¡£æ²¡æœ‰å…±åŒå…³é”®è¯")
                
                st.markdown("---")
                
                # å„æ–‡æ¡£ç‹¬ç‰¹å…³é”®è¯
                st.markdown("### ğŸ¯ å„æ–‡æ¡£ç‹¬ç‰¹å…³é”®è¯")
                
                cols = st.columns(len(analysis["docs"]))
                for i, doc_name in enumerate(analysis["docs"]):
                    with cols[i]:
                        st.markdown(f"**{doc_name[:20]}...**" if len(doc_name) > 20 else f"**{doc_name}**")
                        unique = analysis["unique"].get(doc_name, [])
                        if unique:
                            for kw in unique[:10]:
                                st.markdown(f"- {kw}")
                        else:
                            st.info("æ— ç‹¬ç‰¹å…³é”®è¯")
                
                # å°è¯•ç»˜åˆ¶éŸ¦æ©å›¾ï¼ˆå¦‚æœæ˜¯2-3ä¸ªæ–‡æ¡£ï¼‰
                if 2 <= len(analysis["docs"]) <= 3:
                    st.markdown("---")
                    st.markdown("### ğŸ“Š å…³é”®è¯é‡å éŸ¦æ©å›¾")
                    
                    try:
                        from matplotlib_venn import venn2, venn3
                        import matplotlib.pyplot as plt
                        
                        fig, ax = plt.subplots(figsize=(10, 8))
                        
                        # è·å–æ¯ä¸ªæ–‡æ¡£çš„è¯é›†åˆ
                        doc_indices = [file_names.index(name) for name in analysis["docs"]]
                        word_sets = [set(texts[idx]) for idx in doc_indices]
                        
                        if len(analysis["docs"]) == 2:
                            venn2(word_sets, set_labels=analysis["docs"], ax=ax)
                        else:
                            venn3(word_sets, set_labels=analysis["docs"], ax=ax)
                        
                        ax.set_title("æ–‡æ¡£å…³é”®è¯é‡å æƒ…å†µ")
                        st.pyplot(fig)
                        plt.close()
                        
                    except ImportError:
                        st.info("ğŸ’¡ å®‰è£… matplotlib-venn åº“å¯æ˜¾ç¤ºéŸ¦æ©å›¾: pip install matplotlib-venn")
    
    # ========== ç›¸ä¼¼æ®µè½ ==========
    with tabs[3]:
        st.subheader("ç›¸ä¼¼æ®µè½æ£€æµ‹")
        
        if not raw_texts:
            st.warning("âš ï¸ éœ€è¦åŸå§‹æ–‡æœ¬æ•°æ®æ‰èƒ½è¿›è¡Œæ®µè½æ¯”è¾ƒ")
            return
        
        # é€‰æ‹©æ–‡æ¡£
        col1, col2 = st.columns(2)
        
        with col1:
            seg_doc1 = st.selectbox(
                "é€‰æ‹©ç¬¬ä¸€ä¸ªæ–‡æ¡£",
                file_names,
                index=0,
                key="seg_doc1"
            )
        
        with col2:
            default_idx = 1 if len(file_names) > 1 else 0
            seg_doc2 = st.selectbox(
                "é€‰æ‹©ç¬¬äºŒä¸ªæ–‡æ¡£",
                file_names,
                index=default_idx,
                key="seg_doc2"
            )
        
        # å‚æ•°è®¾ç½®
        col3, col4 = st.columns(2)
        with col3:
            threshold = st.slider(
                "ç›¸ä¼¼åº¦é˜ˆå€¼",
                0.3, 0.9, 0.5, 0.05,
                help="åªæ˜¾ç¤ºç›¸ä¼¼åº¦é«˜äºæ­¤é˜ˆå€¼çš„æ®µè½å¯¹",
                key="seg_threshold"
            )
        with col4:
            segment_size = st.slider(
                "æ®µè½å¤§å°ï¼ˆå­—ç¬¦æ•°ï¼‰",
                30, 200, 80, 10,
                help="ç”¨äºåˆ†å‰²æ–‡æœ¬çš„æ®µè½å¤§å°",
                key="seg_size"
            )
        
        if seg_doc1 == seg_doc2:
            st.warning("è¯·é€‰æ‹©ä¸¤ä¸ªä¸åŒçš„æ–‡æ¡£")
        else:
            if st.button("ğŸ” æ£€æµ‹ç›¸ä¼¼æ®µè½", type="primary", key="detect_segments"):
                doc1_idx = file_names.index(seg_doc1)
                doc2_idx = file_names.index(seg_doc2)
                
                with st.spinner("æ­£åœ¨æ£€æµ‹ç›¸ä¼¼æ®µè½..."):
                    similar_segments = analyzer.find_similar_segments(
                        doc1_idx, doc2_idx, 
                        threshold=threshold,
                        segment_size=segment_size
                    )
                    st.session_state["similar_segments"] = {
                        "segments": similar_segments,
                        "doc1": seg_doc1,
                        "doc2": seg_doc2
                    }
                    log_message(f"æ£€æµ‹åˆ° {len(similar_segments)} å¯¹ç›¸ä¼¼æ®µè½")
            
            # æ˜¾ç¤ºç»“æœ
            if st.session_state.get("similar_segments"):
                seg_data = st.session_state["similar_segments"]
                segments = seg_data["segments"]
                
                if segments:
                    st.success(f"æ‰¾åˆ° {len(segments)} å¯¹ç›¸ä¼¼æ®µè½")
                    
                    for i, (seg1, seg2, sim) in enumerate(segments):
                        with st.expander(f"ç›¸ä¼¼æ®µè½ {i+1} (ç›¸ä¼¼åº¦: {sim:.2%})", expanded=(i < 3)):
                            col1, col2 = st.columns(2)
                            
                            with col1:
                                st.markdown(f"**{seg_data['doc1']}**")
                                st.markdown(f'<div style="background-color: #fff3e0; padding: 10px; border-radius: 5px;">{seg1}</div>', 
                                          unsafe_allow_html=True)
                            
                            with col2:
                                st.markdown(f"**{seg_data['doc2']}**")
                                st.markdown(f'<div style="background-color: #e3f2fd; padding: 10px; border-radius: 5px;">{seg2}</div>', 
                                          unsafe_allow_html=True)
                else:
                    st.info("æœªæ‰¾åˆ°ç›¸ä¼¼åº¦é«˜äºé˜ˆå€¼çš„æ®µè½å¯¹ï¼Œå¯ä»¥å°è¯•é™ä½ç›¸ä¼¼åº¦é˜ˆå€¼")
    
    # ========== å¯¼å‡º ==========
    with tabs[4]:
        st.subheader("å¯¼å‡ºæ¯”è¾ƒåˆ†æç»“æœ")
        
        export_col1, export_col2 = st.columns(2)
        
        with export_col1:
            st.markdown("**ğŸ“Š ç›¸ä¼¼åº¦çŸ©é˜µ**")
            if st.button("ç”Ÿæˆç›¸ä¼¼åº¦çŸ©é˜µ", key="gen_sim_matrix"):
                with st.spinner("æ­£åœ¨ç”Ÿæˆ..."):
                    analyzer.calculate_similarity_matrix()
                    csv_content = analyzer.export_similarity_matrix()
                    if csv_content:
                        st.session_state["sim_matrix_csv"] = csv_content
                        st.success("ç›¸ä¼¼åº¦çŸ©é˜µå·²ç”Ÿæˆ")
            
            if st.session_state.get("sim_matrix_csv"):
                st.download_button(
                    label="ğŸ“¥ ä¸‹è½½ç›¸ä¼¼åº¦çŸ©é˜µCSV",
                    data=st.session_state["sim_matrix_csv"],
                    file_name="similarity_matrix.csv",
                    mime="text/csv",
                    key="download_sim_matrix"
                )
        
        with export_col2:
            st.markdown("**ğŸ“‹ æ¯”è¾ƒåˆ†ææŠ¥å‘Š**")
            if st.button("ç”Ÿæˆæ¯”è¾ƒæŠ¥å‘Š", key="gen_comparison_report"):
                with st.spinner("æ­£åœ¨ç”Ÿæˆ..."):
                    csv_content = analyzer.export_comparison()
                    if csv_content:
                        st.session_state["comparison_csv"] = csv_content
                        st.success("æ¯”è¾ƒæŠ¥å‘Šå·²ç”Ÿæˆ")
            
            if st.session_state.get("comparison_csv"):
                st.download_button(
                    label="ğŸ“¥ ä¸‹è½½æ¯”è¾ƒæŠ¥å‘ŠCSV",
                    data=st.session_state["comparison_csv"],
                    file_name="comparison_report.csv",
                    mime="text/csv",
                    key="download_comparison"
                )
